<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="miu"><meta name="copyright" content="miu"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>dl &amp; ai - andrew ng | 临时个人主页</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/png" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"acbgzm.github.io","root":"/","title":"slowmotion","version":"1.10.9","mode":"time","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"/data/sentences.json"},"local_search":{"path":"/search.xml"},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="整理一下深度学习基础的笔记。">
<meta property="og:type" content="article">
<meta property="og:title" content="dl &amp; ai - andrew ng">
<meta property="og:url" content="http://acbgzm.github.io/2021/05/27/dlang/index.html">
<meta property="og:site_name" content="临时个人主页">
<meta property="og:description" content="整理一下深度学习基础的笔记。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3b1b1.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3b1b2.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3b1b3.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/1.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/2.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/4.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/6.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/5.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/7.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/8.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/9.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/10.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/11.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/12.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/13.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/14.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/15.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/16.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/17.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/18.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/19.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/20.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/21.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/22.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/23.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/24.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/25.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/26.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/27.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/28.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/29.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/30.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/31.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/30.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/32.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/33.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/34.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/35.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/36.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/37.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/38.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/39.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/40.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/41.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/42.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/43.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/44.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/45.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/46.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/47.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/48.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/49.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/50.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/51.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/52.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/53.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/54.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/55.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/56.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/57.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/58.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/59.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/60.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/61.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/62.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/63.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/64.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/65.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/65-add.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/66.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/67.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/68.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/69.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/70.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/71.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/72.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/73.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/74.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/75.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/76.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/77.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/78.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/79.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/80.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/81.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/82.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/83.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/84.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/85.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/86.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/87.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/88.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/89.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/90.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/91.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/92.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/93.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/94.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/95.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/96.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/97.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/98.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/99.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/100.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/101.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/102.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/103.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/104.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/105.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/106.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/107.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/108.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/109.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/110.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/111.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/112.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/113.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/114.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/115.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/116.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/117.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/118.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/119.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/120.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/121.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/122.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/123.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/124.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/125.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/126.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/127.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/128.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/129.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/130.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/131.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/132.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/133.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/134.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/135.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/136.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/137.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/138.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/139.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/140.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/141.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/142.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/143.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/144.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/145.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/146.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/147.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/148.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/149.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/150.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/151.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/152.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/153.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/154.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/155.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/156.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/157.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/158.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/159.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/160.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/161.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/162.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/163.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/164.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/165.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/166.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/167.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/168.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/169.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/170.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/171.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/172.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/173.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/174.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/175.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/176.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/177.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/178.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/179.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/180.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/181.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/182.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/183.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/184.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/185.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/186.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/187.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/188.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/189.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/190.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/191.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/192.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/193.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/194.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/195.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/196.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/197.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf1.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf2.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf3.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf4.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf5.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf6.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf7.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf8.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf9.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf10.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf11.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf12.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf13.png">
<meta property="article:published_time" content="2021-05-27T06:49:55.000Z">
<meta property="article:modified_time" content="2022-03-05T07:01:23.268Z">
<meta property="article:author" content="miu">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="课程笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3b1b1.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="miu"><img width="96" loading="lazy" src="/images/avatar.png" alt="miu"><span class="site-author-status" title="永远相信美好的事情即将发生">🎸</span></a><div class="site-author-name"><a href="/about/">miu</a></div><span class="site-name">临时个人主页</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">19</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">6</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">13</span></a></div><a class="site-state-item hty-icon-button" href="/lab" title="lab"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/ACBGZM" title="GitHub" target="_blank" style="color:#181717"><span class="icon iconify" data-icon="ri:github-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/10388034" title="哔哩哔哩动画" target="_blank" style="color:#FF8EB3"><span class="icon iconify" data-icon="ri:bilibili-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:ACBGZM@126.com" title="E-Mail" target="_blank" style="color:#8E71C1"><span class="icon iconify" data-icon="ri:mail-line"></span></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="友链" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:open-arm-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9A3B1B%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%86%E9%A2%91"><span class="toc-text">第一部分：3B1B的神经网络视频</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9A%E8%83%A1%E6%B5%A9%E5%9F%BA%E8%80%81%E5%B8%88NN%E3%80%81CNN%E9%83%A8%E5%88%86"><span class="toc-text">第二部分：胡浩基老师NN、CNN部分</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%9A%E5%90%B4%E6%81%A9%E8%BE%BEdeeplearning-ai%E7%BD%91%E8%AF%BE"><span class="toc-text">第三部分：吴恩达deeplearning.ai网课</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E8%AF%BE-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88Neural-Networks-and-Deep-Learning%EF%BC%89"><span class="toc-text">第一课 神经网络和深度学习（Neural Networks and Deep Learning）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E5%91%A8%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BC%95%E8%A8%80%EF%BC%88Introduction-to-Deep-Learning%EF%BC%89"><span class="toc-text">第一周：深度学习引言（Introduction to Deep Learning）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E6%AC%A2%E8%BF%8E"><span class="toc-text">1.1 欢迎</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9F"><span class="toc-text">1.2 什么是神经网络？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Supervised-Learning-with-Neural-Networks"><span class="toc-text">1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%9A%E6%B5%81%E8%A1%8C%EF%BC%9F"><span class="toc-text">1.4 为什么神经网络会流行？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-1-6-%E8%AF%BE%E7%A8%8B%E5%AE%89%E6%8E%92"><span class="toc-text">1.5~1.6 课程安排</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%EF%BC%88Basics-of-Neural-Network-Programming%EF%BC%89"><span class="toc-text">第二周：神经网络的编程基础（Basics of Neural Network Programming）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E4%BA%8C%E5%88%86%E7%B1%BB-Binary-Classification"><span class="toc-text">2.1 二分类(Binary Classification)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-Logistic-Regression"><span class="toc-text">2.2 逻辑回归(Logistic Regression)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-text">2.3 逻辑回归的代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Gradient-Descent%EF%BC%89"><span class="toc-text">2.4 梯度下降（Gradient Descent）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-2-6-%E5%AF%BC%E6%95%B0"><span class="toc-text">2.5~2.6 导数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-7-%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%88Computation-Graph%EF%BC%89"><span class="toc-text">2.7 计算图（Computation Graph）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-8-%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%AF%BC%E6%95%B0"><span class="toc-text">2.8 计算图导数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">2.9 逻辑回归的梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-10-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-text">2.10 梯度下降的例子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-11-%E5%90%91%E9%87%8F%E5%8C%96-Vectorization"><span class="toc-text">2.11 向量化(Vectorization)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-12-%E6%9B%B4%E5%A4%9A%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E4%BE%8B%E5%AD%90"><span class="toc-text">2.12 更多的向量化例子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-13-%E5%90%91%E9%87%8F%E5%8C%96%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-text">2.13 向量化逻辑回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-14-%E5%90%91%E9%87%8F%E5%8C%96%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-text">2.14 向量化逻辑回归的梯度计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-15-Python%E4%B8%AD%E7%9A%84%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6%EF%BC%88Broadcasting-in-Python%EF%BC%89"><span class="toc-text">2.15 Python中的广播机制（Broadcasting in Python）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-16-%E5%85%B3%E4%BA%8E-Python%E4%B8%8Enumpy%E5%90%91%E9%87%8F%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">2.16 关于 Python与numpy向量的使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-17-Jupyter-x2F-iPython-Notebooks%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8"><span class="toc-text">2.17 Jupyter&#x2F;iPython Notebooks快速入门</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-18-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3"><span class="toc-text">2.18* 逻辑回归损失函数详解</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E5%91%A8%EF%BC%9A%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Shallow-Neural-Networks%EF%BC%89"><span class="toc-text">第三周：浅层神经网络（Shallow Neural Networks）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0"><span class="toc-text">3.1 神经网络概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E7%A4%BA"><span class="toc-text">3.2 神经网络的表示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E8%AE%A1%E7%AE%97%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%BE%93%E5%87%BA"><span class="toc-text">3.3 计算一个神经网络的输出</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-%E5%A4%9A%E6%A0%B7%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96"><span class="toc-text">3.4 多样本向量化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-%E5%90%91%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="toc-text">3.5 向量化实现的解释</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-6-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-functions%EF%BC%89"><span class="toc-text">3.6 激活函数（Activation functions）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-7-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-text">3.7 为什么需要非线性激活函数？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-8-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="toc-text">3.8 激活函数的导数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-9-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">3.9 神经网络的梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-10%EF%BC%88%E9%80%89%E4%BF%AE%EF%BC%89%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88Backpropagation-intuition%EF%BC%89"><span class="toc-text">3.10（选修）直观理解反向传播（Backpropagation intuition）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-11-%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%88Random-Initialization%EF%BC%89"><span class="toc-text">3.11 随机初始化（Random Initialization）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E5%91%A8%EF%BC%9A%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">第四周：深层神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">4.1 深层神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">4.2 前向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-%E6%A3%80%E6%9F%A5%E7%9F%A9%E9%98%B5%E7%9A%84%E7%BB%B4%E6%95%B0"><span class="toc-text">4.3 检查矩阵的维数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%B1%82%E8%A1%A8%E7%A4%BA%EF%BC%9F"><span class="toc-text">4.4 为什么使用深层表示？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9D%97"><span class="toc-text">4.5 搭建深层神经网络块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-%E5%89%8D%E5%90%91%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">4.6 前向和反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-7-%E5%8F%82%E6%95%B0VS%E8%B6%85%E5%8F%82%E6%95%B0%EF%BC%88Parameters-vs-Hyperparameters%EF%BC%89"><span class="toc-text">4.7 参数VS超参数（Parameters vs Hyperparameters）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-8-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BA%BA%E7%B1%BB%E5%A4%A7%E8%84%91%E7%9A%84%E5%85%B3%E8%81%94%E6%80%A7"><span class="toc-text">4.8 深度学习和人类大脑的关联性</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E8%AF%BE-%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization"><span class="toc-text">第二课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks : Hyperparameter tuning, Regularization and Optimization)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E5%91%A8%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2"><span class="toc-text">第一周：深度学习的实用层面</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E8%AE%AD%E7%BB%83-x2F-%E9%AA%8C%E8%AF%81-x2F-%E6%B5%8B%E8%AF%95%E9%9B%86%EF%BC%88Train-x2F-Dev-x2F-Test-sets%EF%BC%89"><span class="toc-text">1.1 训练&#x2F;验证&#x2F;测试集（Train &#x2F; Dev &#x2F; Test sets）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E5%81%8F%E5%B7%AE-x2F-%E6%96%B9%E5%B7%AE%EF%BC%88Bias-x2F-Variance%EF%BC%89"><span class="toc-text">1.2 偏差&#x2F;方差（Bias &#x2F;Variance）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E5%85%88%E5%90%8E%E9%A1%BA%E5%BA%8F"><span class="toc-text">1.3 先后顺序</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="toc-text">1.4 正则化（Regularization）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AD%A3%E5%88%99%E5%8C%96%E6%9C%89%E5%88%A9%E4%BA%8E%E9%A2%84%E9%98%B2%E8%BF%87%E6%8B%9F%E5%90%88%E5%91%A2%EF%BC%9F"><span class="toc-text">1.5 为什么正则化有利于预防过拟合呢？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-dropout-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">1.6 dropout 正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-7-%E7%90%86%E8%A7%A3-dropout"><span class="toc-text">1.7 理解 dropout</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-8-%E5%85%B6%E4%BB%96%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-text">1.8 其他正则化方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-9-%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5%EF%BC%88Normalizing-inputs%EF%BC%89"><span class="toc-text">1.9 归一化输入（Normalizing inputs）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-10-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-x2F-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%EF%BC%88Vanishing-x2F-Exploding-gradients%EF%BC%89"><span class="toc-text">1.10 梯度消失&#x2F;梯度爆炸（Vanishing &#x2F; Exploding gradients）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-11-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">1.11 神经网络的权重初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-12-%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%95%B0%E5%80%BC%E8%BF%91%E4%BC%BC"><span class="toc-text">1.12 梯度的数值近似</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-13-%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%EF%BC%88Gradient-checking%EF%BC%89"><span class="toc-text">1.13 梯度检验（Gradient checking）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-14-%E5%BA%94%E7%94%A8%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-text">1.14 应用梯度检验的注意事项</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%9A%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-Optimization-algorithms"><span class="toc-text">第二周：优化算法 (Optimization algorithms)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Mini-batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">2.1 Mini-batch 梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E7%90%86%E8%A7%A3Mini-batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">2.2 理解Mini-batch 梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%EF%BC%88Exponentially-weighted-averages%EF%BC%89"><span class="toc-text">2.3 指数加权平均（Exponentially weighted averages）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E7%90%86%E8%A7%A3%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87"><span class="toc-text">2.4 理解指数加权平均</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E7%9A%84%E5%81%8F%E5%B7%AE%E4%BF%AE%E6%AD%A3%EF%BC%88Bias-correction-in-exponentially-weighted-average%EF%BC%89"><span class="toc-text">2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted average）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6-momentum%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">2.6 momentum梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-7-RMSprop-root-mean-square-prop"><span class="toc-text">2.7 RMSprop-root mean square prop</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-8-Adam%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-text">2.8 Adam优化算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%EF%BC%88Learning-rate-decay%EF%BC%89"><span class="toc-text">2.9 学习率衰减（Learning rate decay）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-10-%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E9%97%AE%E9%A2%98"><span class="toc-text">2.10 局部最优问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E5%91%A8%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%EF%BC%8C%E6%89%B9%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E7%A8%8B%E5%BA%8F%E6%A1%86%E6%9E%B6"><span class="toc-text">第三周：超参数调试，批正则化和程序框架</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E8%B0%83%E5%8F%82%E8%A7%84%E5%88%99"><span class="toc-text">3.1 调参规则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E5%90%88%E9%80%82%E7%9A%84%E5%8F%82%E6%95%B0%E5%8F%96%E5%80%BC%E8%8C%83%E5%9B%B4"><span class="toc-text">3.2 合适的参数取值范围</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E8%B6%85%E5%8F%82%E6%95%B0%E8%AE%AD%E7%BB%83%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%9APandas-vs-Caviar"><span class="toc-text">3.3 超参数训练的实践：Pandas vs. Caviar</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%BD%92%E4%B8%80%E5%8C%96-x2F-%E5%8D%95%E4%B8%80%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%8A%E7%9A%84%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88Batch-normalization%EF%BC%89"><span class="toc-text">3.4 激活函数的归一化&#x2F;单一隐藏层上的批归一化（Batch normalization）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">3.5 深度神经网络的批归一化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-6-%E4%B8%BA%E4%BB%80%E4%B9%88Batch-Norm%E6%9C%89%E7%94%A8%EF%BC%9F"><span class="toc-text">3.6 为什么Batch Norm有用？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-7-%E6%B5%8B%E8%AF%95%E6%97%B6%E7%9A%84Batch-Norm"><span class="toc-text">3.7 测试时的Batch Norm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-8-Softmax-%E5%9B%9E%E5%BD%92"><span class="toc-text">3.8 Softmax 回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-9-%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA-Softmax-%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-text">3.9 训练一个 Softmax 分类器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-10-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="toc-text">3.10 深度学习框架</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-11-TensorFlow"><span class="toc-text">3.11 TensorFlow</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E8%AF%BE-%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE-Structuring-Machine-Learning-Projects"><span class="toc-text">第三课 结构化机器学习项目 (Structuring Machine Learning Projects)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E8%AF%BE-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Convolutional-Neural-Networks%EF%BC%89"><span class="toc-text">第四课 卷积神经网络（Convolutional Neural Networks）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E5%91%A8-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Foundations-of-Convolutional-Neural-Networks"><span class="toc-text">第一周 卷积神经网络(Foundations of Convolutional Neural Networks)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%EF%BC%88Computer-vision%EF%BC%89"><span class="toc-text">1.1 计算机视觉（Computer vision）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97-%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B%E4%B8%BA%E4%BE%8B%EF%BC%88Edge-detection%EF%BC%89"><span class="toc-text">1.2 卷积运算-边缘检测为例（Edge detection）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E6%9B%B4%E5%A4%9A%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B%E5%86%85%E5%AE%B9"><span class="toc-text">1.3 更多边缘检测内容</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-Padding"><span class="toc-text">1.4 Padding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-%E5%8D%B7%E7%A7%AF%E6%AD%A5%E9%95%BF%EF%BC%88Strided-convolutions%EF%BC%89"><span class="toc-text">1.5 卷积步长（Strided convolutions）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-%E4%B8%89%E7%BB%B4%E5%8D%B7%E7%A7%AF%EF%BC%88Convolutions-over-volumes%EF%BC%89"><span class="toc-text">1.6 三维卷积（Convolutions over volumes）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-7-%E5%8D%95%E5%B1%82%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="toc-text">1.7 单层卷积网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-8-%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A4%BA%E4%BE%8B"><span class="toc-text">1.8 简单的卷积神经网络示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-9-%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%88Pooling-layers%EF%BC%89"><span class="toc-text">1.9 池化层（Pooling layers）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-10-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A4%BA%E4%BE%8B%EF%BC%88Convolutional-neural-network-example%EF%BC%89"><span class="toc-text">1.10 卷积神经网络示例（Convolutional neural network example）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-11-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%EF%BC%9F%EF%BC%88Why-convolutions-%EF%BC%89"><span class="toc-text">1.11 为什么使用卷积？（Why convolutions?）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E5%91%A8-%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%9A%E5%AE%9E%E4%BE%8B%E6%8E%A2%E7%A9%B6-Deep-convolutional-models-case-studies"><span class="toc-text">第二周 深度卷积网络：实例探究(Deep convolutional models: case studies)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E5%AE%9E%E4%BE%8B%E6%8E%A2%E7%A9%B6%EF%BC%9F"><span class="toc-text">2.1 为什么要进行实例探究？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C"><span class="toc-text">2.2 经典网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%EF%BC%88Residual-Networks-ResNets-%EF%BC%89"><span class="toc-text">2.3 残差网络（Residual Networks (ResNets)）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E7%94%A8%EF%BC%9F"><span class="toc-text">2.4 残差网络为什么有用？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C-x2F-1%C3%971%E5%8D%B7%E7%A7%AF"><span class="toc-text">2.5 网络中的网络 &#x2F; 1×1卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6-Inception-%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B%E3%80%81"><span class="toc-text">2.6 Inception 模块简介、</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-7-Inception-%E7%BD%91%E7%BB%9C-x2F-GoogLeNet%EF%BC%88Inception-network%EF%BC%89"><span class="toc-text">2.7 Inception 网络 &#x2F; GoogLeNet（Inception network）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-8-%E4%BD%BF%E7%94%A8%E5%BC%80%E6%BA%90%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88"><span class="toc-text">2.8 使用开源的实现方案</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%EF%BC%88Transfer-Learning%EF%BC%89"><span class="toc-text">2.9 迁移学习（Transfer Learning）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-10-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%EF%BC%88Data-augmentation%EF%BC%89"><span class="toc-text">2.10 数据增强（Data augmentation）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%8E%B0%E7%8A%B6"><span class="toc-text">2.11 计算机视觉现状</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E5%91%A8-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88Object-detection%EF%BC%89"><span class="toc-text">第三周 目标检测（Object detection）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E7%9B%AE%E6%A0%87%E5%AE%9A%E4%BD%8D%EF%BC%88Object-localization%EF%BC%89"><span class="toc-text">3.1 目标定位（Object localization）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E7%89%B9%E5%BE%81%E7%82%B9%E6%A3%80%E6%B5%8B%EF%BC%88Landmark-detection%EF%BC%89"><span class="toc-text">3.2 特征点检测（Landmark detection）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88Object-detection%EF%BC%89"><span class="toc-text">3.3 目标检测（Object detection）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%AE%9E%E7%8E%B0%EF%BC%88Convolutional-implementation-of-sliding-windows%EF%BC%89"><span class="toc-text">3.4 滑动窗口的卷积实现（Convolutional implementation of sliding windows）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-Bounding-Box-%E9%A2%84%E6%B5%8B-YOLO-%E7%AE%97%E6%B3%95"><span class="toc-text">3.5 Bounding Box 预测 - YOLO 算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-6-%E4%BA%A4%E5%B9%B6%E6%AF%94%EF%BC%88Intersection-over-union%EF%BC%89"><span class="toc-text">3.6 交并比（Intersection over union）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-7-%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6%EF%BC%88Non-max-suppression%EF%BC%89"><span class="toc-text">3.7 非极大值抑制（Non-max suppression）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-8-Anchor-Boxes"><span class="toc-text">3.8 Anchor Boxes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-9-%E6%95%B4%E5%90%88%E8%B5%B7%E6%9D%A5%EF%BC%9AYOLO-%E7%AE%97%E6%B3%95"><span class="toc-text">3.9 整合起来：YOLO 算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-10-%E5%80%99%E9%80%89%E5%8C%BA%E5%9F%9F%EF%BC%88Region-proposals-%EF%BC%89"><span class="toc-text">3.10 *候选区域（Region proposals ）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E5%91%A8-%E7%89%B9%E6%AE%8A%E5%BA%94%E7%94%A8%EF%BC%9A%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E5%92%8C%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB"><span class="toc-text">第四周 特殊应用：人脸识别和风格迁移</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%9F"><span class="toc-text">4.1 什么是人脸识别？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-One-Shot%E5%AD%A6%E4%B9%A0%EF%BC%88One-shot-learning%EF%BC%89"><span class="toc-text">4.2 One-Shot学习（One-shot learning）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-Siamese-%E7%BD%91%E7%BB%9C%EF%BC%88Siamese-network%EF%BC%89"><span class="toc-text">4.3 Siamese 网络（Siamese network）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-Triplet-%E6%8D%9F%E5%A4%B1"><span class="toc-text">4.4 Triplet 损失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-%E9%9D%A2%E9%83%A8%E9%AA%8C%E8%AF%81%E4%B8%8E%E4%BA%8C%E5%88%86%E7%B1%BB%EF%BC%88Face-verification-and-binary-classification%EF%BC%89"><span class="toc-text">4.5 面部验证与二分类（Face verification and binary classification）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%EF%BC%9F%EF%BC%88neural-style-transfer%EF%BC%89"><span class="toc-text">4.6 什么是神经风格迁移？（neural style transfer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-7-%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E5%9C%A8%E5%AD%A6%E4%B9%A0%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">4.7 深度卷积网络在学习什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-8-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-text">4.8 代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-9-%E5%86%85%E5%AE%B9%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-text">4.9 内容代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-10-%E9%A3%8E%E6%A0%BC%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-text">4.10 风格代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-11-%E5%9B%BE%E5%83%8F%E7%9A%84%E4%B8%80%E7%BB%B4%E5%92%8C%E4%B8%89%E7%BB%B4%E6%89%A9%E5%B1%95"><span class="toc-text">4.11 图像的一维和三维扩展</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E8%AF%BE-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-Sequence-Models"><span class="toc-text">第五课 序列模型(Sequence Models)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%EF%BC%9ATensorflow-2-0-%E7%AC%94%E8%AE%B0"><span class="toc-text">第四部分：Tensorflow 2.0 笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-text">1  基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E5%BC%A0%E9%87%8F-Tensor"><span class="toc-text">1.1 张量 Tensor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-text">1.2 常用函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-text">2 优化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E4%B8%80%E4%BA%9B%E5%87%BD%E6%95%B0"><span class="toc-text">2.1 一些函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E6%8C%87%E6%95%B0%E8%A1%B0%E5%87%8F%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-text">2. 2 指数衰减学习率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">2.3 激活函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">2.4 损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-%E7%BC%93%E8%A7%A3%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">2.5 缓解过拟合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">2.6 优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%A0%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">几种优化器</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%85%AB%E8%82%A1"><span class="toc-text">3 神经网络搭建八股</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E4%BD%BF%E7%94%A8Sequential%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">3.1 使用Sequential搭建神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Sequential"><span class="toc-text">Sequential</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#compile"><span class="toc-text">compile</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#fit"><span class="toc-text">fit</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#summary"><span class="toc-text">summary</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BE%E4%BE%8B%EF%BC%9A%E9%B8%A2%E5%B0%BE%E8%8A%B1%E8%AF%86%E5%88%AB"><span class="toc-text">举例：鸢尾花识别</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E4%BD%BF%E7%94%A8class%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">3.2  使用class搭建神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-MNIST-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">3.3 MNIST 数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Sequential-1"><span class="toc-text">Sequential</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#class"><span class="toc-text">class</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%85%AB%E8%82%A1%E6%89%A9%E5%B1%95"><span class="toc-text">4 八股扩展</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E8%87%AA%E5%88%B6%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">4.1 自制数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-text">4.2 数据增强</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-%E6%96%AD%E7%82%B9%E7%BB%AD%E8%AE%AD"><span class="toc-text">4.3 断点续训</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%A8%A1%E5%9E%8B"><span class="toc-text">读取模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="toc-text">保存模型</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-%E5%8F%82%E6%95%B0%E6%8F%90%E5%8F%96"><span class="toc-text">4.4 参数提取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-acc-x2F-loss-%E6%9B%B2%E7%BA%BF"><span class="toc-text">4.5 acc&#x2F;loss 曲线</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F"><span class="toc-text">4.6 模型应用程序</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">5 卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-%E5%8D%B7%E7%A7%AF%E6%A6%82%E5%BF%B5"><span class="toc-text">5.1 卷积概念</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97"><span class="toc-text">卷积计算</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc-text">感受野</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-tensorflow%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-text">5.2 tensorflow卷积层的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#A-padading-%E5%85%A8%E9%9B%B6%E5%A1%AB%E5%85%85"><span class="toc-text">A. padading-全零填充</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#B-Conv2D-%E6%8F%8F%E8%BF%B0%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-text">B. Conv2D-描述卷积层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#C-BN-%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-text">C. BN-批标准化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#D-Pooling-%E6%B1%A0%E5%8C%96"><span class="toc-text">D. Pooling-池化</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#E-Dropout-%E8%88%8D%E5%BC%83"><span class="toc-text">E. Dropout-舍弃</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E7%A4%BA%E4%BE%8B"><span class="toc-text">5.3 卷积神经网络搭建示例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Cifar-10-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">Cifar 10 数据集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%90%AD%E5%BB%BA%E7%A4%BA%E4%BE%8B"><span class="toc-text">搭建示例</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-LeNet"><span class="toc-text">5.4 LeNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-AlexNet"><span class="toc-text">5.5 AlexNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-6-VGGNet"><span class="toc-text">5.6 VGGNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-7-Inception-Net"><span class="toc-text">5.7 Inception Net</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-8-ResNet"><span class="toc-text">5.8 ResNet</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://acbgzm.github.io/2021/05/27/dlang/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="miu"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="临时个人主页"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">dl &amp; ai - andrew ng</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2021-05-27 14:49:55" itemprop="dateCreated datePublished" datetime="2021-05-27T14:49:55+08:00">2021-05-27</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><span class="icon iconify" data-icon="ri:file-word-line"></span></span> <span title="本文字数">26.2k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><span class="icon iconify" data-icon="ri:timer-line"></span></span> <span title="阅读时长">107m</span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E7%AC%94%E8%AE%B0/" style="--text-color:dimgray" itemprop="url" rel="index"><span itemprop="text">笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">机器学习</span></a><a class="tag-item" href="/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">课程笔记</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>其实最近一直在看一些具体的任务了，之前基础的深度学习笔记没有整理成文章，现在归纳一下。主要看了这些：</p>
<ul>
<li>3B1B的神经网络视频</li>
<li>胡浩基老师网课的一部分</li>
<li>吴恩达deeplearning.ai的一部分（主要看的这个）</li>
<li>曹建老师的tensorflow2视频</li>
</ul>
<h1 id="第一部分：3B1B的神经网络视频"><a href="#第一部分：3B1B的神经网络视频" class="headerlink" title="第一部分：3B1B的神经网络视频"></a>第一部分：3B1B的神经网络视频</h1><p>（主要是一些概念的直观理解。）</p>
<p>学习的含义：找到特定的 $\omega$ 和 $b$ ，使代价函数最小化。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3b1b1.png'  width="80%" height="80%"/ loading="lazy">



<p>反向传播：计算单个训练样本想怎样修改 $\omega$ 和 $b$ 。不仅是每个参数应该变大还是变小，还包括这些变化的比例是多大，才能最快下降梯度函数。一个真正的梯度下降过程要对所有的训练样本求平均，但计算太慢，就先把所有的样本分到minibatch中，，计算一个minibatch来作为梯度下降的一步，最终会收敛到局部最优点。</p>
<p>为了使 $a_{i+1}$ 的某个输出增大，可以</p>
<ul>
<li>增大 $b$ </li>
<li>增大 $\omega$ ：增加上层活跃的神经元的权重更好。依据对应权重大小，对激活值做出成比例的改变。</li>
<li>改变 $a_i$</li>
</ul>
<p>反向传播的链式法则</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3b1b2.png'  width="80%" height="80%"/ loading="lazy">



<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3b1b3.png'  width="80%" height="80%"/ loading="lazy">





<h1 id="第二部分：胡浩基老师NN、CNN部分"><a href="#第二部分：胡浩基老师NN、CNN部分" class="headerlink" title="第二部分：胡浩基老师NN、CNN部分"></a>第二部分：胡浩基老师NN、CNN部分</h1><p>手写了笔记，其实重复的地方比较多，就不每张拍照上传了。后续把几个手推的公式拍一下。</p>
<h1 id="第三部分：吴恩达deeplearning-ai网课"><a href="#第三部分：吴恩达deeplearning-ai网课" class="headerlink" title="第三部分：吴恩达deeplearning.ai网课"></a>第三部分：吴恩达deeplearning.ai网课</h1><p>看的网易云课堂做的字幕版本，可惜右上方有水印，有些地方有遮挡。</p>
<h2 id="第一课-神经网络和深度学习（Neural-Networks-and-Deep-Learning）"><a href="#第一课-神经网络和深度学习（Neural-Networks-and-Deep-Learning）" class="headerlink" title="第一课 神经网络和深度学习（Neural Networks and Deep Learning）"></a>第一课 神经网络和深度学习（Neural Networks and Deep Learning）</h2><h3 id="第一周：深度学习引言（Introduction-to-Deep-Learning）"><a href="#第一周：深度学习引言（Introduction-to-Deep-Learning）" class="headerlink" title="第一周：深度学习引言（Introduction to Deep Learning）"></a>第一周：深度学习引言（Introduction to Deep Learning）</h3><h4 id="1-1-欢迎"><a href="#1-1-欢迎" class="headerlink" title="1.1 欢迎"></a>1.1 欢迎</h4><p>关于课程安排</p>
<h4 id="1-2-什么是神经网络？"><a href="#1-2-什么是神经网络？" class="headerlink" title="1.2 什么是神经网络？"></a>1.2 什么是神经网络？</h4><p>神经网络可以当作一个函数。通过数据集计算从x到y的精准映射函数，然后对于新的数据x，给出预测的结果y。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/1.png'  width="80%" height="80%"/ loading="lazy">



<h4 id="1-3-神经网络的监督学习-Supervised-Learning-with-Neural-Networks"><a href="#1-3-神经网络的监督学习-Supervised-Learning-with-Neural-Networks" class="headerlink" title="1.3 神经网络的监督学习(Supervised Learning with Neural Networks)"></a>1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</h4><p>监督学习：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/2.png' width="80%" height="80%"/ loading="lazy">

<p>对于图片，使用CNN。对于序列信息（音频、语言信息等）使用RNN。</p>
<p>结构化数据：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3.png' width="80%" height="80%"/ loading="lazy">

<p>神经网络能帮助计算机理解无结构化数据。</p>
<h4 id="1-4-为什么神经网络会流行？"><a href="#1-4-为什么神经网络会流行？" class="headerlink" title="1.4 为什么神经网络会流行？"></a>1.4 为什么神经网络会流行？</h4><p>数据和计算规模的进展。现在获得了很大的数据量、计算了很复杂的网络。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/4.png' width="80%" height="80%"/ loading="lazy">

<p>其他原因：算法的改进，比如从sigmoid函数到relu函数</p>
<h4 id="1-5-1-6-课程安排"><a href="#1-5-1-6-课程安排" class="headerlink" title="1.5~1.6 课程安排"></a>1.5~1.6 课程安排</h4><p>略</p>
<h3 id="第二周：神经网络的编程基础（Basics-of-Neural-Network-Programming）"><a href="#第二周：神经网络的编程基础（Basics-of-Neural-Network-Programming）" class="headerlink" title="第二周：神经网络的编程基础（Basics of Neural Network Programming）"></a>第二周：神经网络的编程基础（Basics of Neural Network Programming）</h3><h4 id="2-1-二分类-Binary-Classification"><a href="#2-1-二分类-Binary-Classification" class="headerlink" title="2.1 二分类(Binary Classification)"></a>2.1 二分类(Binary Classification)</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/6.png' width="80%" height="80%"/ loading="lazy">

<p>数据集按列组成矩阵：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/5.png' width="80%" height="80%"/ loading="lazy">

<p>X.shape &#x3D; (n<sub>x</sub>, m)</p>
<p>y.shape &#x3D; (1, m)</p>
<h4 id="2-2-逻辑回归-Logistic-Regression"><a href="#2-2-逻辑回归-Logistic-Regression" class="headerlink" title="2.2 逻辑回归(Logistic Regression)"></a>2.2 逻辑回归(Logistic Regression)</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/7.png' width="80%" height="80%"/ loading="lazy">

<p>在神经网络中，将 $b$ 和 $w$ 分开表示，不采用逻辑回归那样组合成 $\theta$ 的形式。</p>
<h4 id="2-3-逻辑回归的代价函数"><a href="#2-3-逻辑回归的代价函数" class="headerlink" title="2.3 逻辑回归的代价函数"></a>2.3 逻辑回归的代价函数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/8.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-4-梯度下降（Gradient-Descent）"><a href="#2-4-梯度下降（Gradient-Descent）" class="headerlink" title="2.4 梯度下降（Gradient Descent）"></a>2.4 梯度下降（Gradient Descent）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/9.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-5-2-6-导数"><a href="#2-5-2-6-导数" class="headerlink" title="2.5~2.6 导数"></a>2.5~2.6 导数</h4><p>略</p>
<h4 id="2-7-计算图（Computation-Graph）"><a href="#2-7-计算图（Computation-Graph）" class="headerlink" title="2.7 计算图（Computation Graph）"></a>2.7 计算图（Computation Graph）</h4><p>计算图表示从左向右的计算过程。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/10.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-8-计算图导数"><a href="#2-8-计算图导数" class="headerlink" title="2.8 计算图导数"></a>2.8 计算图导数</h4><p>根据计算图，从右到左计算函数 J 的导数。（链式求导）</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/11.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-9-逻辑回归的梯度下降"><a href="#2-9-逻辑回归的梯度下降" class="headerlink" title="2.9 逻辑回归的梯度下降"></a>2.9 逻辑回归的梯度下降</h4><p>用计算图理解逻辑回归的梯度下降。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/12.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-10-梯度下降的例子"><a href="#2-10-梯度下降的例子" class="headerlink" title="2.10 梯度下降的例子"></a>2.10 梯度下降的例子</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/13.png' width="80%" height="80%"/ loading="lazy">

<p>dw<sub>1</sub>、 dw<sub>2</sub>、db 作为累加器。数据集循环后，J、 dw<sub>1</sub>、 dw<sub>2</sub>、db 除以样本个数。</p>
<p>一次梯度下降有两层循环，外层循环遍历所有数据样本（m个），内层循环遍历所有特征 w（n个）。</p>
<p>在深度学习中，数据量很大，<strong>为了不用显式的for循环，使用向量化</strong>。</p>
<h4 id="2-11-向量化-Vectorization"><a href="#2-11-向量化-Vectorization" class="headerlink" title="2.11 向量化(Vectorization)"></a>2.11 向量化(Vectorization)</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/14.png' width="80%" height="80%"/ loading="lazy">

<p>向量化是很有必要的。在上图1000000维向量相乘运算中，使用向量化比使用for循环节省300倍的时间。</p>
<h4 id="2-12-更多的向量化例子"><a href="#2-12-更多的向量化例子" class="headerlink" title="2.12 更多的向量化例子"></a>2.12 更多的向量化例子</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/15.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-13-向量化逻辑回归"><a href="#2-13-向量化逻辑回归" class="headerlink" title="2.13 向量化逻辑回归"></a>2.13 向量化逻辑回归</h4><p>向量化逻辑回归的正向传播：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/16.png' width="80%" height="80%"/ loading="lazy">

<p>Z &#x3D; np.dot(w.T, X) + b</p>
<blockquote>
<p>w.shape  &#x3D;(n_x, 1)，每个特征对应一个w，列向量</p>
<p>X.shape &#x3D; (n_x, m)</p>
<p>z.shape &#x3D; (1, m)</p>
<p>b本来是一个实数，python的broadcasting机制在相加时，把b扩展为 （1， m) 维的行向量。</p>
</blockquote>
<h4 id="2-14-向量化逻辑回归的梯度计算"><a href="#2-14-向量化逻辑回归的梯度计算" class="headerlink" title="2.14 向量化逻辑回归的梯度计算"></a>2.14 向量化逻辑回归的梯度计算</h4><p>向量化逻辑回归的反向传播：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/17.png' width="80%" height="80%"/ loading="lazy">

<p>dZ &#x3D; A - Y</p>
<p>db &#x3D; np.sum(dZ) &#x2F; m</p>
<p>dw &#x3D;  np.dot(X, dZ.T) &#x2F; m</p>
<blockquote>
<p>X.shape &#x3D; (n_x, m)</p>
<p>dZ.shape &#x3D; (1, m)</p>
<p>dw.shape &#x3D; (n_x, 1)</p>
</blockquote>
<p>一次梯度下降：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/18.png' width="80%" height="80%"/ loading="lazy">





<h4 id="2-15-Python中的广播机制（Broadcasting-in-Python）"><a href="#2-15-Python中的广播机制（Broadcasting-in-Python）" class="headerlink" title="2.15 Python中的广播机制（Broadcasting in Python）"></a>2.15 Python中的广播机制（Broadcasting in Python）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/19.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-16-关于-Python与numpy向量的使用"><a href="#2-16-关于-Python与numpy向量的使用" class="headerlink" title="2.16 关于 Python与numpy向量的使用"></a>2.16 关于 Python与numpy向量的使用</h4><p><code> a = np.random.randn(5)</code></p>
<blockquote>
<p>a.shape &#x3D; (5, )</p>
</blockquote>
<p>这是numpy的特殊格式”rank 1 array”，<code>a.T</code> 操作仍然得到这种格式的数组。</p>
<p>在神经网络编程中，避免使用这种秩为1的数组。</p>
<p>用 <code> a = np.random.randn(5, 1)</code> 作为替代。此时就可以用 <code>np.dot(a, a.T)</code> 得到一个矩阵了。</p>
<p>也可以用 <code>assert</code> 或 <code>reshape</code> 修改为矩阵格式。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/20.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-17-Jupyter-x2F-iPython-Notebooks快速入门"><a href="#2-17-Jupyter-x2F-iPython-Notebooks快速入门" class="headerlink" title="2.17 Jupyter&#x2F;iPython Notebooks快速入门"></a>2.17 Jupyter&#x2F;iPython Notebooks快速入门</h4><p>略</p>
<h4 id="2-18-逻辑回归损失函数详解"><a href="#2-18-逻辑回归损失函数详解" class="headerlink" title="2.18* 逻辑回归损失函数详解"></a>2.18* 逻辑回归损失函数详解</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/21.png' width="80%" height="80%"/ loading="lazy">



<p>在整个数据集上的情况：假设样本是独立同分布，可以累乘，做最大似然估计使这个式子取最大值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/22.png' width="80%" height="80%"/ loading="lazy">





<h3 id="第三周：浅层神经网络（Shallow-Neural-Networks）"><a href="#第三周：浅层神经网络（Shallow-Neural-Networks）" class="headerlink" title="第三周：浅层神经网络（Shallow Neural Networks）"></a>第三周：浅层神经网络（Shallow Neural Networks）</h3><h4 id="3-1-神经网络概述"><a href="#3-1-神经网络概述" class="headerlink" title="3.1 神经网络概述"></a>3.1 神经网络概述</h4><p>正向传播，计算损失函数 $L$ ；反向传播，计算梯度下降需要的 $dW$、$db$ 。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/23.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-2-神经网络的表示"><a href="#3-2-神经网络的表示" class="headerlink" title="3.2 神经网络的表示"></a>3.2 神经网络的表示</h4><p>$ a^{[i]}$ 表示第 $i$ 层的激活值，$a^{[0]}&#x3D;X$</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/24.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-3-计算一个神经网络的输出"><a href="#3-3-计算一个神经网络的输出" class="headerlink" title="3.3 计算一个神经网络的输出"></a>3.3 计算一个神经网络的输出</h4><p>向量化的前向传播</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/25.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-4-多样本向量化"><a href="#3-4-多样本向量化" class="headerlink" title="3.4 多样本向量化"></a>3.4 多样本向量化</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/26.png' width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/27.png' width="80%" height="80%"/ loading="lazy">

<p>$X,Z,A$ 都是按列组合起来的矩阵。</p>
<h4 id="3-5-向量化实现的解释"><a href="#3-5-向量化实现的解释" class="headerlink" title="3.5 向量化实现的解释"></a>3.5 向量化实现的解释</h4><p>$Z^{[1]}$ 的每一列都是一个训练样本 $X_i$ 经过 $W^{[1]}$ 计算而来的。</p>
<p>当处理多个训练样本时，$X$ 是列向量拼起来的形式，则 $Z$ 也是 $X$ 的每一列的计算结果。</p>
<p>把 $b$ 加上也一样，可能用到广播机制。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/28.png' width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/29.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-6-激活函数（Activation-functions）"><a href="#3-6-激活函数（Activation-functions）" class="headerlink" title="3.6 激活函数（Activation functions）"></a>3.6 激活函数（Activation functions）</h4><p>不同层的激活函数可以不一样。在隐藏层中，tanh函数效果比sigmoid好；但在输出层，二分类任务用sigmoid比较好，因为输出是0~1.</p>
<p>tanh和sigmoid函数的问题是：当x很大或很小，函数的梯度约为0，会拖慢梯度下降。因此在隐藏层用ReLU函数更好。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/30.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-7-为什么需要非线性激活函数？"><a href="#3-7-为什么需要非线性激活函数？" class="headerlink" title="3.7 为什么需要非线性激活函数？"></a>3.7 为什么需要非线性激活函数？</h4><p>如果不用非线性激活函数，神经网络的输出就是 $X$ 的线性组合。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/31.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-8-激活函数的导数"><a href="#3-8-激活函数的导数" class="headerlink" title="3.8 激活函数的导数"></a>3.8 激活函数的导数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/30.png' width="80%" height="80%"/ loading="lazy">

<p>Sigmoid：</p>
<p>$$a &#x3D; g(z) &#x3D; \frac{1}{1+e^{-z}}$$</p>
<p>$$g’(z)&#x3D;\frac{d}{dz}g(z) &#x3D; g(z)(1-g(z))&#x3D;a(1-a)$$</p>
<p>tanh：</p>
<p>$$g(z)&#x3D;\frac{e^z-e^{-z}}{e^z+e^{-z}}$$</p>
<p>$$g’(z)&#x3D;\frac{d}{dz}g(z) &#x3D; 1-(g(z))^2$$</p>
<p>ReLU：在0处可以指定导数的值</p>
<p>$$g(z)&#x3D;max(0, z)$$</p>
<p>$$ g’(z)&#x3D; \begin{cases}  0,&amp;\text{if } z&lt;0 \ 1,&amp;\text{if } z≥0 \end{cases} $$</p>
<p>Leaky ReLU：</p>
<p>$$g(z)&#x3D;max(0.01z, z)$$</p>
<p>$$ g’(z)&#x3D; \begin{cases}  0.01,&amp;\text{if } z&lt;0 \ 1,&amp;\text{if } z≥0 \end{cases} $$</p>
<h4 id="3-9-神经网络的梯度下降"><a href="#3-9-神经网络的梯度下降" class="headerlink" title="3.9 神经网络的梯度下降"></a>3.9 神经网络的梯度下降</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/32.png' width="80%" height="80%"/ loading="lazy">

<p>前向传播过程：</p>
<blockquote>
<p>$Z^{[1]} &#x3D; W^{[1]}X + b^{[1]}$</p>
<p>$A^{[1]} &#x3D; g^{[1]}(Z^{[1]})$</p>
<p>$Z^{[2]} &#x3D; W^{[2]}A^{[1]} + b^{[2]}$</p>
<p>$A^{[2]} &#x3D; g^{[2]}(Z^{[2]})$</p>
</blockquote>
<p>反向传播过程：</p>
<blockquote>
<p>$dZ^{[2]}&#x3D;A^{[2]}-Y$</p>
<p>$dW^{[2]}&#x3D;\frac{1}{m} dZ^{[2]} (A^{[1]} )^T$</p>
<p>$db^{[2]} &#x3D; \frac{1}{m}np.sum(dZ^{[2]}, axis&#x3D;1, keepdims&#x3D;ture)$ </p>
<p>$dZ^{[1]} &#x3D;( W^{[2]})^T  dZ^{[2]} * g^{‘[1]}(Z^{[1]})$</p>
<p>$dW^{[1]}&#x3D;\frac{1}{m} dZ^{[1]} X^T$</p>
<p>$db^{[1]} &#x3D; \frac{1}{m}np.sum(dZ^{[1]}, axis&#x3D;1, keepdims&#x3D;ture)$</p>
</blockquote>
<h4 id="3-10（选修）直观理解反向传播（Backpropagation-intuition）"><a href="#3-10（选修）直观理解反向传播（Backpropagation-intuition）" class="headerlink" title="3.10（选修）直观理解反向传播（Backpropagation intuition）"></a>3.10（选修）直观理解反向传播（Backpropagation intuition）</h4><p>想一想矩阵的维度。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/33.png' width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/34.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-11-随机初始化（Random-Initialization）"><a href="#3-11-随机初始化（Random-Initialization）" class="headerlink" title="3.11 随机初始化（Random Initialization）"></a>3.11 随机初始化（Random Initialization）</h4><p>不能用全0初始化神经网络，这会导致神经元的对称性和反向传播失效。</p>
<p>随机初始化：用很小的随机数初始化 $W$ ，用0初始化 $b$ .</p>
<p>如果 $W$ 的值太大，$z$ 会落在tanh函数和sigmoid函数平缓的部分，梯度下降会很慢。如果完全没有用到这两个函数就没有影响。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/35.png' width="80%" height="80%"/ loading="lazy">

<p>训练浅层神经网络，0.01是可用的；当训练深层神经网络，要使用其他的常数，在之后讲。</p>
<h3 id="第四周：深层神经网络"><a href="#第四周：深层神经网络" class="headerlink" title="第四周：深层神经网络"></a>第四周：深层神经网络</h3><h4 id="4-1-深层神经网络"><a href="#4-1-深层神经网络" class="headerlink" title="4.1 深层神经网络"></a>4.1 深层神经网络</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/36.png' width="80%" height="80%"/ loading="lazy">

<p>$L$：层数，从0开始计数。</p>
<p>$n^{[l]}$：$l$ 层的神经元个数。</p>
<p>图中 $L&#x3D;4$ ，$n^{[L]}&#x3D;1$，$n^{[1]} &#x3D; n^{[2]} &#x3D;5$ 。</p>
<h4 id="4-2-前向传播"><a href="#4-2-前向传播" class="headerlink" title="4.2 前向传播"></a>4.2 前向传播</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/37.png' width="80%" height="80%"/ loading="lazy">

<p>基本过程：</p>
<ul>
<li>$z^{[l]} &#x3D; w^{[l]} a^{[l-1]} +b^{[l]}$ </li>
<li>$a^{[l]} &#x3D; g(z^{[l]})$</li>
</ul>
<p>向量化见图右下方。</p>
<p>在前向传播的实现过程中，需要使用显示的for循环，来遍历从输入层到输出层。</p>
<h4 id="4-3-检查矩阵的维数"><a href="#4-3-检查矩阵的维数" class="headerlink" title="4.3 检查矩阵的维数"></a>4.3 检查矩阵的维数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/38.png' width="80%" height="80%"/ loading="lazy">

<p>同样本排列成一列（如 $(x_1,x_2)^T$、$(z_1,z_2)^T$），不同的样本m纵向组合起来（如 $(A[0],A[1])$、$(Z[1],Z[2])$）。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/39.png' width="80%" height="80%"/ loading="lazy">

<p>在向量化的场合，python的broadcasting机制把 $b[1]$ 维度 $(n^{[1]},1)$ 扩展成 $(n^{[1]},m)$。</p>
<h4 id="4-4-为什么使用深层表示？"><a href="#4-4-为什么使用深层表示？" class="headerlink" title="4.4 为什么使用深层表示？"></a>4.4 为什么使用深层表示？</h4><p>神经网络可以不用很大，但深层有好处。</p>
<p>在直觉层面理解，深层神经网络能组合从简单到复杂的信息。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/40.png' width="80%" height="80%"/ loading="lazy">



<p>另一种直觉理解，从电路角度，用小规模但深层的电路结构，可以进行复杂的计算；但用浅层的电路模型，要用指数级增长的运算单元才能实现相同的功能。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/41.png' width="80%" height="80%"/ loading="lazy">



<h4 id="4-5-搭建深层神经网络块"><a href="#4-5-搭建深层神经网络块" class="headerlink" title="4.5 搭建深层神经网络块"></a>4.5 搭建深层神经网络块</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/42.png' width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/43.png' width="80%" height="80%"/ loading="lazy">



<h4 id="4-6-前向和反向传播"><a href="#4-6-前向和反向传播" class="headerlink" title="4.6 前向和反向传播"></a>4.6 前向和反向传播</h4><p>前向传播的实现：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/44.png' width="80%" height="80%"/ loading="lazy">

<p>反向传播的实现：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/45.png' width="80%" height="80%"/ loading="lazy">



<h4 id="4-7-参数VS超参数（Parameters-vs-Hyperparameters）"><a href="#4-7-参数VS超参数（Parameters-vs-Hyperparameters）" class="headerlink" title="4.7 参数VS超参数（Parameters vs Hyperparameters）"></a>4.7 参数VS超参数（Parameters vs Hyperparameters）</h4><p>Parameters: W, b</p>
<p>Hyperparameters:</p>
<ul>
<li>learning rate(α), #iterations, #hidden layers(L), #hidden units(n), choice of activation function.</li>
<li>momentum, mini-batch size, regularization parameters, …</li>
</ul>
<p>尝试不同的超参数值，找到合适的值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/46.png' width="80%" height="80%"/ loading="lazy">



<h4 id="4-8-深度学习和人类大脑的关联性"><a href="#4-8-深度学习和人类大脑的关联性" class="headerlink" title="4.8 深度学习和人类大脑的关联性"></a>4.8 深度学习和人类大脑的关联性</h4><p>目前对人脑的认识没有达到建立数学模型的程度。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/47.png' width="80%" height="80%"/ loading="lazy">







<h2 id="第二课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization"><a href="#第二课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization" class="headerlink" title="第二课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks : Hyperparameter tuning, Regularization and Optimization)"></a>第二课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks : Hyperparameter tuning, Regularization and Optimization)</h2><h3 id="第一周：深度学习的实用层面"><a href="#第一周：深度学习的实用层面" class="headerlink" title="第一周：深度学习的实用层面"></a>第一周：深度学习的实用层面</h3><blockquote>
<p>深度学习的应用方法。数据集的划分；偏差&#x2F;方差；通过正则化来防止过拟合（包括L1L2，dropout，其他方法如数据增强和提前停止）；输入归一化；通过合理的权重初始化来避免梯度消失和梯度爆炸；进行梯度检验确保梯度下降算法正确运行。最后一节讲了以上方法的实践经验。</p>
</blockquote>
<h4 id="1-1-训练-x2F-验证-x2F-测试集（Train-x2F-Dev-x2F-Test-sets）"><a href="#1-1-训练-x2F-验证-x2F-测试集（Train-x2F-Dev-x2F-Test-sets）" class="headerlink" title="1.1 训练&#x2F;验证&#x2F;测试集（Train &#x2F; Dev &#x2F; Test sets）"></a>1.1 训练&#x2F;验证&#x2F;测试集（Train &#x2F; Dev &#x2F; Test sets）</h4><p>在训练集进行训练，根据在验证集上的得分选择最好的模型，在测试集上进行评估。</p>
<p>在数据集很大的情况下，可以把验证集、测试集划分得少一点。在百万条数据的情况下，甚至可以划分99.5%&#x2F;0.25%&#x2F;0.25%。</p>
<ul>
<li><p>注意1：<strong>保证验证集和测试集的数据来自同一分布</strong>。</p>
<p>如：训练集是网站上比较精美、清晰的图片；验证集、训练集是用户随手拍的图片。</p>
</li>
<li><p>注意2：不做测试集也可以。如果不需要对最终的神经网络做无偏评估，也可以不设置测试集。</p>
</li>
</ul>
<h4 id="1-2-偏差-x2F-方差（Bias-x2F-Variance）"><a href="#1-2-偏差-x2F-方差（Bias-x2F-Variance）" class="headerlink" title="1.2 偏差&#x2F;方差（Bias &#x2F;Variance）"></a>1.2 偏差&#x2F;方差（Bias &#x2F;Variance）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/48.png' width="80%" height="80%"/ loading="lazy">

<p>前提：基本error很低；验证集和测试集来自同一分布。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/49.png' width="80%" height="80%"/ loading="lazy">

<p>训练集的error要跟基本error比，基本error通常是人工识别的error。</p>
<h4 id="1-3-先后顺序"><a href="#1-3-先后顺序" class="headerlink" title="1.3 先后顺序"></a>1.3 先后顺序</h4><p>按步骤确认：</p>
<ol>
<li><p>high bias？ 增大网络规模、训练更长时间、（修改网络结构）</p>
</li>
<li><p>high variance？ 获得更多数据、正则化、（修改网络结构）</p>
</li>
<li><p>完成，获得 low bias &amp; variance 的模型。</p>
</li>
</ol>
<p>在现在深度学习、大数据的环境中，可以做到在减小bias或variance的过程中，不对另一方产生过多不良影响。我们不用太过关注如何 tradeoff。</p>
<h4 id="1-4-正则化（Regularization）"><a href="#1-4-正则化（Regularization）" class="headerlink" title="1.4 正则化（Regularization）"></a>1.4 正则化（Regularization）</h4><p>逻辑回归的正则化：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/50.png' width="80%" height="80%"/ loading="lazy">

<p>如果用的是L1正则化，W最终会是稀疏的，也就是W向量中有很多0。</p>
<p>现在更倾向于L2正则化。</p>
<p>$\lambda$ 也是一个需要调整的超参数。为了防止与python的关键字重复，在代码中一般写作lambd。 </p>
<p>神经网络的正则化：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/51.png' width="80%" height="80%"/ loading="lazy">

<p>由于历史原因，不叫矩阵的L2正则化，而是叫 frobenius norm。</p>
<p>在反向传播过程中，正则化项求导后加在 $dW$ 的后面，让梯度下降的幅度大一些。也被称为 weight decay 。</p>
<h4 id="1-5-为什么正则化有利于预防过拟合呢？"><a href="#1-5-为什么正则化有利于预防过拟合呢？" class="headerlink" title="1.5 为什么正则化有利于预防过拟合呢？"></a>1.5 为什么正则化有利于预防过拟合呢？</h4><p>从直观上理解，正则化项降低了 $W$ 的值，也就是降低了一些神经元的作用，简化了模型，让模型从过拟合向欠拟合发展。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/52.png' width="80%" height="80%"/ loading="lazy">



<p>第二种直观理解方法：$W$ 值变小，$z$ 集中在激活函数的线性部分，则模型的每一层都相当于线性变换，模型不适用于复杂的决策，降低了过拟合程度。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/53.png' width="80%" height="80%"/ loading="lazy">

<p>如果实施了带正则化项的损失函数，当使用梯度下降法时，为了调试梯度下降，要使用这个新定义的损失函数，否则损失函数可能不会再所有的调幅范围内都单调递减。</p>
<h4 id="1-6-dropout-正则化"><a href="#1-6-dropout-正则化" class="headerlink" title="1.6 dropout 正则化"></a>1.6 dropout 正则化</h4><p>对每个训练样本，遍历神经网络的每一层，并设置消除神经网络中节点的概率，消除一些节点，得到一个更小规模的神经网络，训练这个精简后的网络。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/54.png' width="80%" height="80%"/ loading="lazy">

<p> 一种实现方法：inverted dropout（反向随机失活）</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/55.png' width="80%" height="80%"/ loading="lazy">

<p>用不等式给d赋值为true或false，跟a相乘让a的一部分值失效。</p>
<p>有一个 <code>a/=deep_prob</code> 操作， 修正或弥补丢掉的一部分数据，让a的期望值不变。</p>
<p>在测试阶段，不使用dropout。</p>
<h4 id="1-7-理解-dropout"><a href="#1-7-理解-dropout" class="headerlink" title="1.7 理解 dropout"></a>1.7 理解 dropout</h4><p>直观上理解，dropout让神经元不依赖于某一个特征，而让权重更加分散。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/56.png' width="80%" height="80%"/ loading="lazy">

<p>如果更担心在某些层有过拟合，就把某些层的keep-prob设置得低一些。缺点是在验证集上调参工作量增大。</p>
<p>dropout本质上是一种正则化方法，用来防止过拟合。在计算机视觉问题中，输入的像素很多，以至于没有足够的数据，经常一直处于过拟合情况。因此dropout在CV应用的比较频繁。在其他领域，如果没有过拟合问题就不必使用。</p>
<p>dropout一大缺点就是代价函数 $J$ 不再被明确定义。每次迭代都随机保留神经元，很难对每次的反向传播梯度下降进行复查。也就失去了绘制递减的代价函数图像的工具。通常先关闭dropout，运行代码确保代价函数单调递减，再开启dropout。</p>
<h4 id="1-8-其他正则化方法"><a href="#1-8-其他正则化方法" class="headerlink" title="1.8 其他正则化方法"></a>1.8 其他正则化方法</h4><p>data augment</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/57.png' width="80%" height="80%"/ loading="lazy">



<p>early stopping</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/58.png' width="80%" height="80%"/ loading="lazy">

<p>建立模型的两个过程：其一是让 $J(w, b)$ 取到最小值，手段包括梯度下降等；其二是防止过拟合，又称为orthogonalization，手段包括正则化等。early stopping 的缺点是破坏了这两个过程相互的独立性。提前结束训练过程，也就是打断了第一个过程。</p>
<p>如果使用L2正则化，就避免了这个缺点，随之而来的是 $\lambda$ 的调参工作量，而不是只进行一次梯度下降就可以找到early stopping的位置。</p>
<h4 id="1-9-归一化输入（Normalizing-inputs）"><a href="#1-9-归一化输入（Normalizing-inputs）" class="headerlink" title="1.9 归一化输入（Normalizing inputs）"></a>1.9 归一化输入（Normalizing inputs）</h4><p>第一步：零均值化；第二步：方差归一化。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/59.png' width="80%" height="80%"/ loading="lazy">

<p>注意：在训练集和测试集上要用相同的 $\mu,\sigma$ 。</p>
<p>这样做的原因：让优化变快。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/60.png' width="80%" height="80%"/ loading="lazy">



<h4 id="1-10-梯度消失-x2F-梯度爆炸（Vanishing-x2F-Exploding-gradients）"><a href="#1-10-梯度消失-x2F-梯度爆炸（Vanishing-x2F-Exploding-gradients）" class="headerlink" title="1.10 梯度消失&#x2F;梯度爆炸（Vanishing &#x2F; Exploding gradients）"></a>1.10 梯度消失&#x2F;梯度爆炸（Vanishing &#x2F; Exploding gradients）</h4><p>activations以指数级增长或下降，给梯度下降造成困难。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/61.png' width="80%" height="80%"/ loading="lazy">

<p>以图中简化 $b$ 、$w$ 全部是对角矩阵的神经网络为例：$w$ 比单位矩阵大一点，激活值以指数级增长；w 比单位矩阵小一点，激活值以指数级减小。</p>
<h4 id="1-11-神经网络的权重初始化"><a href="#1-11-神经网络的权重初始化" class="headerlink" title="1.11 神经网络的权重初始化"></a>1.11 神经网络的权重初始化</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/62.png' width="80%" height="80%"/ loading="lazy">

<p>通过给 $W$ 设置合理的初始值（不能比1大&#x2F;小太多），避免梯度消失和梯度爆炸。</p>
<p>以图中去掉 $b$ 的单个神经元为例，最合理的方式是设置 $w$ 接近 $\frac{1}{n}$ 。</p>
<p>因此进行这样的初始化：$W^{[l]} &#x3D; np.random.randn(shape)*np.sqrt(\frac{2}{n^{[l-1]]}})$</p>
<p>当用ReLU函数，是 $\sqrt{\frac{2}{n^{[l-1]]}}}$ ；当用tanh函数，是 $\sqrt{\frac{1}{n^{[l-1]]}}}$；也有人用 $\sqrt{\frac{2}{n^{[l-1]]}+n^{[l]}}}$ 。</p>
<h4 id="1-12-梯度的数值近似"><a href="#1-12-梯度的数值近似" class="headerlink" title="1.12 梯度的数值近似"></a>1.12 梯度的数值近似</h4><p>在实施反向传播时，进行gradient checking，可以确保反向传播正在正确进行。</p>
<p>用 $\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}\approx g(\theta)$ 近似计算 $\theta$ 的梯度 $g(\theta)$。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/63.png' width="80%" height="80%"/ loading="lazy">



<h4 id="1-13-梯度检验（Gradient-checking）"><a href="#1-13-梯度检验（Gradient-checking）" class="headerlink" title="1.13 梯度检验（Gradient checking）"></a>1.13 梯度检验（Gradient checking）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/64.png' width="80%" height="80%"/ loading="lazy">

<p>把所有层的$w,b$ 组合成矩阵 $\theta$，所有层的$dW,db$ 组合成矩阵 $d\theta$ 。我们需要验证：$d\theta$ 是 $\theta$ 的梯度。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/65.png' width="80%" height="80%"/ loading="lazy">

<p>计算近似梯度：</p>
<p>$$ d\theta_{approx}[i] &#x3D;\frac{J(\theta_1, \theta_2,…,\theta_i+\epsilon,…)-J(\theta_1, \theta_2,…,\theta_i-\epsilon,…)}{2\epsilon} \approx d\theta[i] &#x3D; \frac{\partial J}{\partial \theta_i}$$</p>
<p>$$check: \frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{approx}||_2+||d\theta||_2} \approx 10^{-7}$$</p>
<p>如果 $\approx10^{-5}$，检查向量，确保没有一项误差过大，确保没有bug；如果 $\approx10^{-3}$，需要小心有bug。可以检查哪一项的导数计算结果和估计值偏差很大，并反推求导过程，检查bug。</p>
<h4 id="1-14-应用梯度检验的注意事项"><a href="#1-14-应用梯度检验的注意事项" class="headerlink" title="1.14 应用梯度检验的注意事项"></a>1.14 应用梯度检验的注意事项</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/65-add.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li>不要在训练过程中使用梯度检验，只用于调试。</li>
<li>如果梯度检验失败，检查哪一项的导数计算结果和估计值偏差很大，确定bug位置，比如在某一层的求导结果跟估计值差很大。</li>
<li>梯度检验的过程中，如果使用了正则化，要记住计算中应包括正则化项。</li>
<li><strong>梯度检验不能与dropout一起使用</strong>。dropout让我们难以计算 $J$ 。可以先把 keep_prob 设置为1，验证梯度下降是正确的；再开启dropout.</li>
<li>几乎不会出现的情况：随机初始化 $w,b$ 接近0，梯度下降的实施是正确的，但在运行梯度下降时，$w,b$ 变大，可能只有在 $w,b$ 接近0时，梯度下降才是正确的，但 $w,b$ 变大时它变得越来越不准确。做法（基本不用）：在随机初始化过程中进行梯度检验，然后再训练网络，如果随机初始化值比较小，$w,b$ 会有一段时间远离0 ；反复训练网络之后再重新进行梯度检验。（开始做一下梯度检验，训练后再进行一次梯度检验，保证正确。）</li>
</ul>
<h3 id="第二周：优化算法-Optimization-algorithms"><a href="#第二周：优化算法-Optimization-algorithms" class="headerlink" title="第二周：优化算法 (Optimization algorithms)"></a>第二周：优化算法 (Optimization algorithms)</h3><p>让梯度下降加速的优化方法。包括mini-batch梯度下降、momentum&#x2F;RMSprop&#x2F;Adam算法（和需要了解的指数加权平均、偏差修正基础）、学习率衰减。最后一节讲了局部最优问题。</p>
<h4 id="2-1-Mini-batch-梯度下降"><a href="#2-1-Mini-batch-梯度下降" class="headerlink" title="2.1 Mini-batch 梯度下降"></a>2.1 Mini-batch 梯度下降</h4><p>batch梯度下降：在整个数据集上进行梯度下降。</p>
<p>mini-batch梯度下降：把整个数据集划分为若干个mini-batch，在每个mini-batch上进行一次梯度下降。</p>
<p>实现过程：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/66.png' width="80%" height="80%"/ loading="lazy">

<p>前向传播、求平均损失 –&gt; 反向传播、梯度下降。在每个mini-batch中进行这样的操作。</p>
<p>完整的遍历一次训练集称为 1 epoch。mini-batch可以在 1 epoch 中完成多次梯度下降。</p>
<h4 id="2-2-理解Mini-batch-梯度下降"><a href="#2-2-理解Mini-batch-梯度下降" class="headerlink" title="2.2 理解Mini-batch 梯度下降"></a>2.2 理解Mini-batch 梯度下降</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/67.png' width="80%" height="80%"/ loading="lazy">

<p>mini-batch会让梯度下降有噪声，但最终也会收敛到比较小的水平。</p>
<p>mini-batch梯度下降的优势：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/68.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>mini-batch size &#x3D; m，即 batch 梯度下降：步长大，噪声少。单次迭代耗时长。</p>
</li>
<li><p>mini-batch size &#x3D; 1，即 随机 梯度下降：步长小，噪声多，永远不收敛（在最小值附近波动。失去向量化方法带来的计算加速。</p>
</li>
<li><p>mini-batch梯度下降：既能对样本进行向量化，又能快速迭代。</p>
</li>
</ul>
<p>选择 mini-batch size 的注意事项：</p>
<ul>
<li><p>样本集小（&lt;2000），直接用batch梯度下降。</p>
</li>
<li><p>一般把 mini-batch size 设置为2的次方数。</p>
</li>
<li><p>确保mini-batch内的数据 $(X^,Y^)$ 符合 CPU&#x2F;GPU 的内存。</p>
</li>
<li><p>这也是一个 hyperparameter ，需要多次尝试，找到让梯度下降最高效的取值。</p>
</li>
</ul>
<p>还有比梯度下降和mini-batch梯度下降都要高效得多的算法，在后面讲。</p>
<h4 id="2-3-指数加权平均（Exponentially-weighted-averages）"><a href="#2-3-指数加权平均（Exponentially-weighted-averages）" class="headerlink" title="2.3 指数加权平均（Exponentially weighted averages）"></a><span id = "2.2.3">2.3 指数加权平均（Exponentially weighted averages）</span></h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/69.png' width="80%" height="80%"/ loading="lazy">

<p>在统计学中叫指数加权移动平均值。</p>
<p>$$V_t &#x3D; \beta V_{t-1} + (1-\beta) \theta_t$$ </p>
<p>$V_t$ 可以看作在 $\frac{1}{1-\beta}$ 天中，温度$\theta$ 的平均值。</p>
<p>通过调整参数 $\beta$ ，获得不同的效果。</p>
<ul>
<li>$\beta$ 大，平均的样本多，曲线平滑但有偏移。（图中绿色线是50天的平均值）</li>
<li>$\beta$ 小，平均的样本少，曲线更拟合，但噪声大。（图中黄色线是2天的平均值，红色线是10天的平均值）</li>
</ul>
<h4 id="2-4-理解指数加权平均"><a href="#2-4-理解指数加权平均" class="headerlink" title="2.4 理解指数加权平均"></a>2.4 理解指数加权平均</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/70.png' width="80%" height="80%"/ loading="lazy">

<p>把算式展开，是sum(每天的温度×指数衰减系数)的形式。（右上方两个图中对应值相乘）</p>
<p>$$V_{100} &#x3D; 0.1\times\theta_{100}+0.1\times 0.9\times\theta_{99}+0.1\times 0.9\times 0.9\times\theta_{98}+ …$$</p>
<p>所有系数加起来近似＝1。</p>
<p>有 $(1-\epsilon)^{\frac{1}{\epsilon}}\approx \frac{1}{e}$ ，在此时权重系数衰减的下降幅度很大。因此可以近似认为，今天的 $V$ 的值是取了前 $\frac{1}{\epsilon} &#x3D; \frac{1}{1-\beta}$ 天的平均值。如图中的 $V$ 是取了 $\frac{1}{0.1}&#x3D;10$ 天的温度平均值。</p>
<p>实现：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/71.png' width="80%" height="80%"/ loading="lazy">

<p>在实现上，只需要存储单个变量 $V$ 并且不断更新即可。$V$ 近似了平均值，省去了使用滑动窗口求和求精确平均值所需的存储空间。</p>
<p>在之后的章节中，需要计算多个变量的平均值，使用指数加权平均是一个好的近似计算方法。</p>
<h4 id="2-5-指数加权平均的偏差修正（Bias-correction-in-exponentially-weighted-average）"><a href="#2-5-指数加权平均的偏差修正（Bias-correction-in-exponentially-weighted-average）" class="headerlink" title="2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted average）"></a>2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted average）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/72.png' width="80%" height="80%"/ loading="lazy">

<p>以 $\beta&#x3D;0.98$ 为例，在实际实现上，会得到紫线而不是绿线。以为初始化 $V&#x3D;0$ ，前几项会很小。（见左下算式）</p>
<p>使用偏差修正，让平均值近似计算更加准确：用 $\frac{V_t}{1-\beta^t}$ 代替 $V_t$ 。在刚开始 $t$ 较小时，$\frac{V_t}{1-\beta^t}$ 求的是 $\theta$ 的加权平均数（右下表达式）；$t$ 变大时，$\beta^t$ 接近 0。</p>
<p>偏差修正能在早期获得更好的估计，但也可以选择熬过初始时期，不使用偏差修正。</p>
<h4 id="2-6-momentum梯度下降"><a href="#2-6-momentum梯度下降" class="headerlink" title="2.6 momentum梯度下降"></a>2.6 momentum梯度下降</h4><p>计算梯度的指数加权平均数，加速梯度下降。这个方法好于普通梯度下降。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/73.png' width="80%" height="80%"/ loading="lazy">

<p>梯度下降的波动，要求我们不能用很大的学习率。在纵轴上，我们希望学习慢一点；在横轴上，希望学习快一点。</p>
<p>平均了这些梯度之后，会发现纵轴上的摆动平均值接近 0（图中红箭头），可以采用大一些的学习率了。</p>
<p>一个直观上的理解：小球从碗状函数像底部滚动，微分项 $dw,db$ 是加速度，momentum项 $V_{dw},V_{db}$ 是速度，球加速向底部滚动，而 $\beta$ 相当于摩擦力，让小球不会无限加速。不像梯度下降法每一步都独立于之前的步骤，现在小球可以向下滚，获得动量（momentum）。</p>
<p>实现：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/74.png' width="80%" height="80%"/ loading="lazy">

<p>$\beta$ 的常用值是 0.9，即平均前十次迭代的梯度。同时也可以不使用偏差修正 $\frac{V_t}{1-\beta^t}$ ，因为10次后已经可以正常近似了。</p>
<p>也有资料将 $(1-\beta)$ 忽略，使用右边的式子，这两者在效果上是相同的，只是会影响到 $\alpha$ 的最佳值。老师认为左边的计算方法更符合直觉，因为如果要调整超参数 $\beta$，就会影响到 $V_{dw}$ 和 $V_{db}$ ，也许还要修改 $\alpha$。</p>
<h4 id="2-7-RMSprop-root-mean-square-prop"><a href="#2-7-RMSprop-root-mean-square-prop" class="headerlink" title="2.7 RMSprop-root mean square prop"></a>2.7 RMSprop-root mean square prop</h4><p>另一种消除梯度下降的摆动，加快梯度下降的方法。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/75.png' width="80%" height="80%"/ loading="lazy">

<p>当在某个方向波动大（如图中举例 $db$ ，在梯度下降减去一个分母较大的数 $b:&#x3D; b-\alpha \frac{db}{\sqrt{db}}$，让梯度下降的幅度减小。在某个方向梯度下降幅度小（如图中举例 $dw$ ，在梯度下降减去一个较小的数 $w:&#x3D; w-\alpha \frac{dw}{\sqrt{dw}}$，让梯度下降的幅度增大。</p>
<p>其他：为了跟momentum结合起来，将RMSprop的超参数命名为 $\beta_2$ ；防止除以0，在分母加上很小的数 $\epsilon &#x3D; 10^{-8}$ 。</p>
<h4 id="2-8-Adam优化算法"><a href="#2-8-Adam优化算法" class="headerlink" title="2.8 Adam优化算法"></a>2.8 Adam优化算法</h4><p>把momentum和RMSprop组合起来。在不同的模型上都有很好的效果，有很广泛的应用。</p>
<p>Adam算法需要进行偏差修正。</p>
<p>momentum：$w&#x3D;w-\alpha V_{dw}$ </p>
<p>RMSprop：$w &#x3D; w - \alpha \frac{dw}{\sqrt{S_{dw}+\epsilon}}$ </p>
<p>Adam：$w &#x3D; w - \alpha \frac{V_{dw}}{\sqrt{S_{dw}+\epsilon}}$ </p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/76.png' width="80%" height="80%"/ loading="lazy">



<p>几个超参数，当应用adam算法时，$\beta_1,\beta_2,\epsilon$ 常常都是用缺省值，$\alpha$ 需要实验确定。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/77.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-9-学习率衰减（Learning-rate-decay）"><a href="#2-9-学习率衰减（Learning-rate-decay）" class="headerlink" title="2.9 学习率衰减（Learning rate decay）"></a>2.9 学习率衰减（Learning rate decay）</h4><p>随时间慢慢减小学习率。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/78.png' width="80%" height="80%"/ loading="lazy">

<p>一种方法：</p>
<p>$$\alpha &#x3D; \frac{1}{1+decay_rate \times epoch_num}\alpha_{init}$$</p>
<p>decay_rate 是需要调整的超参数。 </p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/79.png' width="80%" height="80%"/ loading="lazy">



<p>其他几种方法：指数衰减、除以epoch_num的开方、离散衰减等。也有看着模型训练过程，然后手动进行衰减的方法。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/80.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-10-局部最优问题"><a href="#2-10-局部最优问题" class="headerlink" title="2.10 局部最优问题"></a>2.10 局部最优问题</h4><p>在维数很高的情况下，更多的情况是收敛到鞍形部位（鞍点，图右方），而不是局部最优点（图左方）。在鞍点，一些方向的曲线向下弯曲，一些方向的曲线向上弯曲。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/81.png' width="80%" height="80%"/ loading="lazy">

<p>在鞍上称为plateaus问题，这段时间训练得比较慢，使用momentum等算法可以加速此过程。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/82.png' width="80%" height="80%"/ loading="lazy">





<h3 id="第三周：超参数调试，批正则化和程序框架"><a href="#第三周：超参数调试，批正则化和程序框架" class="headerlink" title="第三周：超参数调试，批正则化和程序框架"></a>第三周：超参数调试，批正则化和程序框架</h3><p>调参的基本规则和方法；batch norm让学习算法运行速度更快；softmax回归；深度学习框架。</p>
<h4 id="3-1-调参规则"><a href="#3-1-调参规则" class="headerlink" title="3.1 调参规则"></a>3.1 调参规则</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/83.png' width="50%" height="50%"/ loading="lazy">

<p>调参重要性排序：红 &gt; 黄 &gt; 紫。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/84.png' width="80%" height="80%"/ loading="lazy">

<p>在深度学习中，不要用网格取值进行实验（图左）。</p>
<p>应该随机取超参数的值并进行实验（图右）。因为不知道哪个超参数是更重要的，需要探究重要的超参数的更多潜在值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/85.png' width="80%" height="80%"/ loading="lazy">

<p>使用从粗略到精细（coarse to fine）的策略。在表现好的区域上进行更密集的取值尝试</p>
<h4 id="3-2-合适的参数取值范围"><a href="#3-2-合适的参数取值范围" class="headerlink" title="3.2 合适的参数取值范围"></a>3.2 合适的参数取值范围</h4><p>有些超参数，可以在合理的范围内，在<strong>线性轴</strong>上，做随机均匀取值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/86.png' width="80%" height="80%"/ loading="lazy">



<p>学习率等超参数，更合适的方法是在<strong>对数轴</strong>上均匀随机取值。</p>
<pre class="language-python" data-language="python"><code class="language-python">r <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">4</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">)</span>
alpha <span class="token operator">=</span> exp<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> r<span class="token punctuation">)</span></code></pre>

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/87.png' width="80%" height="80%"/ loading="lazy">





<p>在 $1-\beta$ 取值，而不是在 $\beta$ 取值。因为在 $\beta$ 越接近 1，平均的样本个数有更大的变化，需要更密集的取值。所以在 $1-\beta$ 接近 0 时进行更密集的取值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/88.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-3-超参数训练的实践：Pandas-vs-Caviar"><a href="#3-3-超参数训练的实践：Pandas-vs-Caviar" class="headerlink" title="3.3 超参数训练的实践：Pandas vs. Caviar"></a>3.3 超参数训练的实践：Pandas vs. Caviar</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/89.png' width="80%" height="80%"/ loading="lazy">

<p>在不同领域的参数设置可能有相似的部分，多了解其他工作；多进行尝试。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/90.png' width="80%" height="80%"/ loading="lazy">

<p>一种方法：在训练中照看（babysitting）模型，比如进行学习率的调整。</p>
<p>另一种方法：同时训练超参数取值不同的多个模型。</p>
<h4 id="3-4-激活函数的归一化-x2F-单一隐藏层上的批归一化（Batch-normalization）"><a href="#3-4-激活函数的归一化-x2F-单一隐藏层上的批归一化（Batch-normalization）" class="headerlink" title="3.4 激活函数的归一化&#x2F;单一隐藏层上的批归一化（Batch normalization）"></a>3.4 激活函数的归一化&#x2F;单一隐藏层上的批归一化（Batch normalization）</h4><p>batch normalization 会使参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/91.png' width="80%" height="80%"/ loading="lazy">

<p>对逻辑回归、神经网络的输入归一化而言，进行输入特征值的归一化是有效的。如图的上半部分，对 $x_1,x_2,x_3$ 进行归一化对 $w,b$ 的训练有帮助。</p>
<p>同样的思想：对深层的模型，能否对 $a^{[i]}$ 进行归一化，改进 $w^{[i+1]},b^{[i+1]}$ 的训练？</p>
<p>在实践中，我们不对 $a^{[i]}$ 做归一化，而是对 $z^{[i]}$ 做归一化。这一点一直有争论。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/92.png' width="80%" height="80%"/ loading="lazy">

<p>实现：</p>
<p>$$z_{norm}^{(i)} &#x3D; \frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}$$ </p>
<p>$$\widetilde{z}^{(i)} &#x3D; \gamma z_{norm}^{(i)} + \beta$$ </p>
<ul>
<li><p>$\gamma,\beta$ 是可以学习的参数（不是超参数）。如果 $\gamma&#x3D;\sqrt{\sigma^2+\epsilon},\beta&#x3D;\mu$，则 $\widetilde{z}^{(i)} &#x3D; {z}^{(i)}$，batch normalization不起作用。 $\gamma$ 控制方差，$\beta$ 值控制均值。通过给它们赋值，可以构造含平均值和方差的隐藏单元值。</p>
</li>
<li><p>用  $\widetilde{z}^{(i)}$ 取代 ${z}^{(i)}$ ，参与神经网络的后续计算。</p>
</li>
<li><p>不一定非要归一化成均值为0的分布。可以归一化到均值不是0，方差大一点，符合sigmoid等激活函数的特性。</p>
</li>
<li><p>batch normalization本质上是让隐藏单元值的均值和方差标准化，即 $z^{[i]}$ 有固定的均值和方差，由 $\gamma,\beta$ 两个参数控制。</p>
</li>
</ul>
<h4 id="3-5-深度神经网络的批归一化"><a href="#3-5-深度神经网络的批归一化" class="headerlink" title="3.5 深度神经网络的批归一化"></a>3.5 深度神经网络的批归一化</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/93.png' width="80%" height="80%"/ loading="lazy">

<p>batch norm是发生在计算 $z$ 和 $a$ 之间的。给神经网络添加了新的参数 $\gamma,\beta$ 。（注意，跟momentum等优化算法的超参数 $\beta$ 区分。这两者的论文都使用 $\beta$ 作为参数的名称。）</p>
<p>使用优化算法（如梯度下降或Adam等），对这些参数 $w,b,\gamma,\beta$ 进行更新。</p>
<p>在深度学习框架中，可以用一行代码完成batch norm的操作，无需自己实现。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/94.png' width="80%" height="80%"/ loading="lazy">

<p>batch norm通常和训练集的mini-batch一起使用。在每一个mini-batch上，做一次梯度下降。</p>
<p>在上一段中，提到对参数 $w,b,\gamma,\beta$ 进行更新。但实际的计算步骤为：</p>
<ul>
<li>先计算 $z^{[l]}  &#x3D; w^{[l]}a^{[l-1]}+b^{[l]}$；</li>
<li>然后对 $z^{[l]}$ 进行归一化计算 $z_{norm}^{[l]}$，在此过程中会减去均值， <strong>$b^{[l]}$ 这个加上去的参数是无效的。</strong></li>
<li>用 $\widetilde{z}^{[l]} &#x3D; \gamma^{[l]}z_{norm}^{[l]} + \beta^{[l]}$ 进行后续计算。<strong>形式上， $\beta$ 代替了参数 $b$ 。</strong></li>
</ul>
<p>实际计算步骤：</p>
<ul>
<li>$z^{[l]}  &#x3D; w^{[l]}a^{[l-1]}$ </li>
<li>计算 $z_{norm}^{[l]}$ </li>
<li>$\widetilde{z}^{[l]} &#x3D; \gamma^{[l]}z_{norm}^{[l]} + \beta^{[l]}$</li>
</ul>
<p>此外，注意参数的维度：$z,b,\beta,\gamma$ 维度都是 $(n^{[l]}, 1)$</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/95.png' width="80%" height="80%"/ loading="lazy">

<p>实现：</p>
<p>对于每一个mini-batch：在前向传播的过程中，在每个隐藏层使用 batch norm；反向传播计算梯度；进行梯度下降或使用其他优化算法。</p>
<h4 id="3-6-为什么Batch-Norm有用？"><a href="#3-6-为什么Batch-Norm有用？" class="headerlink" title="3.6 为什么Batch Norm有用？"></a>3.6 为什么Batch Norm有用？</h4><p>第一个原因：跟逻辑回归类似，让所有特征归一到同一尺度，加速梯度下降的过程。 </p>
<p>第二个原因：让权重比网络更滞后或更深层，让数值更稳定。第10层的权重比第1层的权重更robust。在之前层的权重发生改变时，$z$ 会发生变化，但batch norm保证了 $z$  的均值和方差保持不变。因此限制了在前层的参数更新对数值分布的影响。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/96.png' width="80%" height="80%"/ loading="lazy">

<p>第三个原因：batch norm有一点正则化的效果。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/97.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-7-测试时的Batch-Norm"><a href="#3-7-测试时的Batch-Norm" class="headerlink" title="3.7 测试时的Batch Norm"></a>3.7 测试时的Batch Norm</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/98.png' width="80%" height="80%"/ loading="lazy">

<p>在训练过程中，batch norm一次作用在一个mini-batch上，求这个mini-batch上的均值和方差（图左）；在评估阶段，batch norm只作用在单个测试样本上，虽然可以在整个测试集上计算 $\mu,\sigma^2$，但在实际操作中，通常使用指数加权平均（图右）。</p>
<p>追踪训练过程中每个mini-batch的 $\mu,\sigma^2$ 的值，然后使用之前求温度 $\theta_1,\theta_2,\theta_3$ 的指数加权平均的方法（见<a href="#2.2.3">第二课第二周第三节</a>，求 $\mu,\sigma^2$ 的近似值，然后用于下一步计算 $z_{norm} &#x3D; \frac{z-\mu}{\sqrt{\sigma^2+\epsilon}}$。</p>
<p>实际上，不管用什么样的估计方法，整套过程都是比较robust的。</p>
<p>当使用深度学习框架时，通常会有默认的估算 $\mu,\sigma^2$ 的方法。</p>
<h4 id="3-8-Softmax-回归"><a href="#3-8-Softmax-回归" class="headerlink" title="3.8 Softmax 回归"></a>3.8 Softmax 回归</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/99.png' width="80%" height="80%"/ loading="lazy">

<p>多分类问题中，预测一组相加为1的概率值作为神经网络的输出层。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/100.png' width="80%" height="80%"/ loading="lazy">

<p>使用softmax激活函数进行从权值到概率的转换。$t^i&#x3D;e^{z^i}$，$a^i &#x3D; \frac{t^i}{\sum t}$ </p>
<p>ReLU和Sigmoid函数输入一个实数，输出一个实数；而softmax函数因为要对所有的输出进行归一化（计算概率），需要输入一个向量，输出一个向量。</p>
<p>直观的softmax分类的例子：神经网络只有一层softmax层。神经元有三个，就分成3类，每类之间都是线性决策边界。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/101.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-9-训练一个-Softmax-分类器"><a href="#3-9-训练一个-Softmax-分类器" class="headerlink" title="3.9 训练一个 Softmax 分类器"></a>3.9 训练一个 Softmax 分类器</h4><p>softmax跟hardmax相对，把最大值更温和地转换成一个概率，而不是全部改为0和1.</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/102.png' width="80%" height="80%"/ loading="lazy">



<p><strong>训练-损失函数</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/103.png' width="80%" height="80%"/ loading="lazy">

<p>单个训练样本的损失函数：$$L(\hat{y}, y) &#x3D; -\sum^C_{j&#x3D;1}y_jlog\hat{y_j}$$</p>
<p>在真实情况中，$y_j$ 只有一个为1，其余都为0，因此损失函数是 $-log\hat{y_i}$ ，损失函数试图让 $y&#x3D;1$ 对应的 $\hat{y}$ 尽量地大。这也是最大似然估计的一种形式。</p>
<p>整个训练集的损失函数：$$J(w,b) &#x3D; \frac{1}{m}\sum^m_{i&#x3D;1} L(\hat{y}, y)$$ </p>
<p>$\hat{y},y$ 的维度都是 $(4, m) $。</p>
<p><strong>训练-梯度下降</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/104.png' width="80%" height="80%"/ loading="lazy">

<p>梯度：$$dz^{[l]} &#x3D; \hat{y} - y$$ </p>
<p>在深度学习框架中，主要精力放在将前向传播做好。通常框架自己会弄明白怎样反向传播。</p>
<h4 id="3-10-深度学习框架"><a href="#3-10-深度学习框架" class="headerlink" title="3.10 深度学习框架"></a>3.10 深度学习框架</h4><p>现存的框架；选择框架的标准。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/105.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-11-TensorFlow"><a href="#3-11-TensorFlow" class="headerlink" title="3.11 TensorFlow"></a>3.11 TensorFlow</h4><p>在tensorflow中定义损失函数cost，可以理解为tensorflow会建立起一个计算图，来自动完成后续的反向传播。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/106.png' width="80%" height="80%"/ loading="lazy">

<p>在框架中，可以只用一行代码修改很多工作，比如训练的方法是梯度下降还是Adam。这支持我们快速实现复杂的神经网络模型。</p>
<h2 id="第三课-结构化机器学习项目-Structuring-Machine-Learning-Projects"><a href="#第三课-结构化机器学习项目-Structuring-Machine-Learning-Projects" class="headerlink" title="第三课 结构化机器学习项目 (Structuring Machine Learning Projects)"></a>第三课 结构化机器学习项目 (Structuring Machine Learning Projects)</h2><p>暂时空缺</p>
<h2 id="第四课-卷积神经网络（Convolutional-Neural-Networks）"><a href="#第四课-卷积神经网络（Convolutional-Neural-Networks）" class="headerlink" title="第四课 卷积神经网络（Convolutional Neural Networks）"></a>第四课 卷积神经网络（Convolutional Neural Networks）</h2><h3 id="第一周-卷积神经网络-Foundations-of-Convolutional-Neural-Networks"><a href="#第一周-卷积神经网络-Foundations-of-Convolutional-Neural-Networks" class="headerlink" title="第一周 卷积神经网络(Foundations of Convolutional Neural Networks)"></a>第一周 卷积神经网络(Foundations of Convolutional Neural Networks)</h3><p>卷积运算（padding、stride）和不同的卷积核；将卷积核叠加的三维卷积和单层卷积网络；卷积神经网络（CONV、POOL、FN）。</p>
<h4 id="1-1-计算机视觉（Computer-vision）"><a href="#1-1-计算机视觉（Computer-vision）" class="headerlink" title="1.1 计算机视觉（Computer vision）"></a>1.1 计算机视觉（Computer vision）</h4><p>计算机视觉的应用：图片分类，目标检测，风格迁移等。</p>
<p>计算机视觉的一个问题是数据量非常大。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/107.png' width="80%" height="80%"/ loading="lazy">



<h4 id="1-2-卷积运算-边缘检测为例（Edge-detection）"><a href="#1-2-卷积运算-边缘检测为例（Edge-detection）" class="headerlink" title="1.2 卷积运算-边缘检测为例（Edge detection）"></a>1.2 卷积运算-边缘检测为例（Edge detection）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/108.png' width="80%" height="80%"/ loading="lazy">

<p>在神经网络隐藏层中，不同层识别不同的信息。比如，浅层识别物体的边缘，深层识别人脸的部位，更深层识别整个人脸。以边缘检测为例，展示卷积计算的过程。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/109.png' width="80%" height="80%"/ loading="lazy">

<p>以中间矩阵的区域，在左边矩阵的每个对应区域，进行对应元素相乘，然后加起来，作为右边矩阵的一个值。</p>
<p>左边的矩阵理解为图片；中间的矩阵是过滤器（filter）或卷积核（kernel）；右边的矩阵可以理解为另一张图片。* 是数学上的卷积运算符，但在python中， * 也被重载做很多场合的乘法运算，所以在编程中使用其他函数，比如tensorflow中是 <code>tf.nn.conv2d</code>。</p>
<p>这也是纵向边缘的计算过程：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/110.png' width="80%" height="80%"/ loading="lazy">

<p>越大的值理解为颜色越浅。本例计算出的边界比较宽，是因为原图片相对来说非常小。</p>
<h4 id="1-3-更多边缘检测内容"><a href="#1-3-更多边缘检测内容" class="headerlink" title="1.3 更多边缘检测内容"></a>1.3 更多边缘检测内容</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/111.png' width="80%" height="80%"/ loading="lazy">

<p>使用相同的filter，可以在输出图像中区分源图像从亮到暗&amp;从暗到亮这两种变化。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/112.png' width="80%" height="80%"/ loading="lazy">

<p>不同的filter可以帮助我们找到垂直或水平的边缘。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/113.png' width="80%" height="80%"/ loading="lazy">

<p>也有相关工作提出更robust的filter取值，同时也可以不手动定义filter，而把这些数字当成参数，通过反向传播学习更好的filter（之后的内容）。</p>
<p>通过合理设置filter，不仅能检查水平、垂直的边缘，也可以检测任何角度的边缘。</p>
<p>通过把filter的所有数字设置成参数，并让计算机自动学习它们，我们发现：神经网络可以学习一些低级的特征，比如图片的边缘特征。构成这些运算的基础依然是卷积运算（convolution），使得反向传播算法可以学习任何所需的3×3 filter，并在整张图片上应用它。</p>
<h4 id="1-4-Padding"><a href="#1-4-Padding" class="headerlink" title="1.4 Padding"></a>1.4 Padding</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/114.png' width="80%" height="80%"/ loading="lazy">

<p>使用 $f\times f$ 的卷积核，卷积 $n\times n$ 的源图像，得到 $(n-f+1)\times(n-f+1)$ 的新图象。</p>
<p>卷积的两个缺点：</p>
<ul>
<li>卷积会让图片尺寸缩小。可能做几次之后图像就变得很小了。</li>
<li>边缘的像素参与的卷积运算很少，中间的像素用得很多。意味着卷积丢失了图像边缘的信息。</li>
</ul>
<p>通过padding解决这两个问题：在图像周围再添加 $p$ 圈像素。</p>
<p>使用 $f\times f$ 的卷积核，卷积 $(n+p)\times (n+p)$ 的源图像，得到 $(n+2p-f+1)\times(n+2p-f+1)$ 的新图象。</p>
<p>如图 $p&#x3D;1$：</p>
<ul>
<li>$8\times 8$ 的新图象经过卷积，得到 $6\times 6$ 的图像，尺寸没有变小。</li>
<li>边缘的像素参与的卷积运算更多了。</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/115.png' width="80%" height="80%"/ loading="lazy">

<p> 关于padding多少：</p>
<ul>
<li>Valid convolution：不padding。 $(n\times n) * (f\times f) \longrightarrow (n-f+1)\times(n-f+1)$</li>
<li>Same convolution：padding后得到的输出图像尺寸是源图像尺寸。 $(n\times n) * (f\times f) \longrightarrow n\times n$，$p&#x3D;\frac{f-1}{2}$</li>
</ul>
<p>在计算机视觉问题中，$f$ 一般是奇数。</p>
<h4 id="1-5-卷积步长（Strided-convolutions）"><a href="#1-5-卷积步长（Strided-convolutions）" class="headerlink" title="1.5 卷积步长（Strided convolutions）"></a>1.5 卷积步长（Strided convolutions）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/116.png' width="80%" height="80%"/ loading="lazy">

<p>padding p，stride s：</p>
<p>$$(n\times n) * (f\times f) \longrightarrow (\lfloor \frac{n+2p-f}{2} +1\rfloor)\times(\lfloor \frac{n+2p-f}{2} +1\rfloor)$$</p>
<p>惯例：不是整数就向下取整，超出边缘的卷积不进行计算。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/117.png' width="80%" height="80%"/ loading="lazy">

<p>数学中的convolution还要进行反转filter的操作，在机器学习中则不进行。机器学习的运算在数学中被称为cross-correlation，但在论文中我们延续convolution这一说法，要注意与数学环境中的convolution做区分。</p>
<h4 id="1-6-三维卷积（Convolutions-over-volumes）"><a href="#1-6-三维卷积（Convolutions-over-volumes）" class="headerlink" title="1.6 三维卷积（Convolutions over volumes）"></a>1.6 三维卷积（Convolutions over volumes）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/118.png' width="80%" height="80%"/ loading="lazy">

<p>源图像和filter的channel数量必须相同。最终得到一个二维输出。</p>
<p>将27个数对应相乘再求和，得到输出图像上的一个数。</p>
<p>通过不同的filter的参数选择，获得不同的特征检测器。如图，可以构建只关心红色通道的纵向边缘的filter；也可以构建不关心任何颜色，只关心纵向边缘的filter。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/119.png' width="80%" height="80%"/ loading="lazy">

<p>也可以使用多个filter。如图，将纵向边缘filter、横向边缘filter卷积而来的两张图片结合起来，得到 $4\times 4\times 2$ 的新图像。这种思想使我们可以检测很多个不同的特征，并且输出的通道数等于要检测的特征数，即filter的个数。</p>
<h4 id="1-7-单层卷积网络"><a href="#1-7-单层卷积网络" class="headerlink" title="1.7 单层卷积网络"></a>1.7 单层卷积网络</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/120.png' width="80%" height="80%"/ loading="lazy">

<p>单层卷积网络的前向传播：</p>
<ul>
<li><p>卷积运算。对应 $w^{[1]}a^{[0]}$。$w^{[1]}$是filter，$a^{[0]}$是源图像。</p>
</li>
<li><p>对得到的 $4\times 4$ 矩阵加一个权值（使用 broadcasting）。对应 $z^{[1]} &#x3D; w^{[1]}a^{[0]} + b^{[1]}$。</p>
</li>
<li><p>进行非线性函数处理，如 ReLU，得到新的  $4\times 4$ 矩阵。对应 $a^{[1]} &#x3D; g(z^{[1]})$ 。</p>
</li>
<li><p>多个filter，计算结果叠加起来，得到  $$4\times 4 \times num_filters$$ 矩阵</p>
</li>
</ul>
<p>参数数量：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/121.png' width="80%" height="80%"/ loading="lazy">

<p>不管输入图片的尺寸有多大，参数的个数只跟filter有关。这是卷积神经网络的一个特性，可以避免过拟合。</p>
<p>符号总结：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/122.png' width="80%" height="80%"/ loading="lazy">

<p>每层输出图像的尺寸：</p>
<ul>
<li><p>$n_H^{[l]} &#x3D; \lfloor \frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}} +1 \rfloor  $ </p>
</li>
<li><p>$n_W^{[l]} &#x3D; \lfloor \frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}} +1 \rfloor $</p>
</li>
</ul>
<p>每个filter的尺寸需要匹配上层输出图像的channel数量：</p>
<ul>
<li>$f^{[l]} \times f^{[l]} \times n_c^{[l-1]}$</li>
</ul>
<p>所有的filter：</p>
<ul>
<li>$f^{[l]} \times f^{[l]} \times n_c^{[l-1]} \times n_c^{[l]}$</li>
</ul>
<p>本层图像经过bias和非线性函数得到的activation尺寸：</p>
<ul>
<li>$a^{[l]}:n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$</li>
</ul>
<p>一个mini-batch的所有activations：</p>
<ul>
<li>$A^{[l]}:m \times n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$</li>
</ul>
<p>也不是所有人都用这一套标记法，有些人把channel的数量写在前面。</p>
<h4 id="1-8-简单的卷积神经网络示例"><a href="#1-8-简单的卷积神经网络示例" class="headerlink" title="1.8 简单的卷积神经网络示例"></a>1.8 简单的卷积神经网络示例</h4><p>预测 $39\times 39 \times 3$ 的图像上是否有一只猫，设计以下卷积神经网络：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/123.png' width="80%" height="80%"/ loading="lazy">

<p>经过几步卷积后，获得 $7\times 7 \times 40$ 的特征图，将它们展开成 1960 长度的列向量，进行logistic或softmax回归，预测图片中是否有猫。</p>
<p>在卷积的过程中，有这样的趋势：图像的大小在减少，通道数量在增多。</p>
<p>选择超参数是一个问题，$f,s,p,num_filters$ 等。在之后的课程中会提供一些建议和指导。</p>
<p>卷积神经网络通常由三种layer组成：</p>
<ul>
<li>Convolution，卷积层，CONV</li>
<li>Pooling，池化层，POOL</li>
<li>Fully connectied，全连接层，FC</li>
</ul>
<h4 id="1-9-池化层（Pooling-layers）"><a href="#1-9-池化层（Pooling-layers）" class="headerlink" title="1.9 池化层（Pooling layers）"></a>1.9 池化层（Pooling layers）</h4><p>使用池化层，来缩减模型的大小，提高计算速度，同时让所提取的特征robust。</p>
<p>max pooling：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/124.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>max pooling对每一个通道独立处理，不改变通道个数。</p>
</li>
<li><p>有两个超参数 $f,s$ ，不需要网络学习，手动设置后就不再改变。</p>
</li>
<li><p>可以直觉理解为：数字大意味着可能提取了某些特定特征。</p>
</li>
</ul>
<p>average pooling：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/125.png' width="80%" height="80%"/ loading="lazy">

<p>跟max pooling差不多。</p>
<p>通常，max pooling更加常用；但有时，在很深的神经网络也会用到average pooling。（有时用，在下周讲）</p>
<p>池化层的超参数：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/126.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>有常用的设置 $f&#x3D;2,s&#x3D;2$ ，意味着把图片长宽都缩小一半。</p>
</li>
<li><p>可以自己增加padding参数 $p$，但极少这样做。（有意外，在下周讲）</p>
</li>
<li><p>$n_H \times n_W \times n_c \longrightarrow \lfloor \frac{n_H-f}{s}+1 \rfloor \times \lfloor \frac{n_H-f}{s}+1 \rfloor \times n_c$ ，<strong>池化层不改变通道的个数</strong>。</p>
</li>
<li><p><strong>池化层没有需要训练的参数，只有超参数</strong>。</p>
</li>
</ul>
<h4 id="1-10-卷积神经网络示例（Convolutional-neural-network-example）"><a href="#1-10-卷积神经网络示例（Convolutional-neural-network-example）" class="headerlink" title="1.10 卷积神经网络示例（Convolutional neural network example）"></a>1.10 卷积神经网络示例（Convolutional neural network example）</h4><p>手写数字识别（跟 LeNet-5 相似）：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/127.png' width="80%" height="80%"/ loading="lazy">

<p>有一种叫法是把 CONV+POOL 作为一层卷积，因为 POOL 层没有权重，只有超参数。在本例中同样将 CONV1+POOL1 作为 layer 1。</p>
<p>全连接层相当于单层普通神经网络，神经元全部相连，每条边有一个权值。</p>
<ul>
<li><p>第一层：卷积+最大池化。参数是6个filters。</p>
</li>
<li><p>第二层：卷积+最大池化。参数是16个filters。</p>
</li>
<li><p>第三层：flatten后，400到120的全连接。参数是 $w,b$。</p>
</li>
<li><p>第四层：120到84的全连接。参数是 $w,b$。</p>
</li>
<li><p>输出：对84个神经元进行 softmax ，预测手写数字。</p>
</li>
</ul>
<p>常见的模式：</p>
<ul>
<li>图像尺寸逐渐变小，通道数量逐渐增多。</li>
<li>一个或多个卷积层后接一个池化层，重复几次，最后是几个全连接层，最终进行softmax等函数输出。</li>
</ul>
<p>常规做法：尽量不要自己设置超参数，而是查看文献，使用别人在任务中效果很好的架构。（下周细讲）</p>
<p>卷积神经网络的一些细节：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/128.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li>参数<ul>
<li>池化层没有任何参数。</li>
<li>卷积层参数数量较小，这点在之前提过。只跟filter有关，跟图片的尺寸无关。<ul>
<li>416 &#x3D; 16channel * (5*5filter + 1bias)，每个filter有一个偏置。</li>
</ul>
</li>
<li>大多数参数存在于全连接层。<ul>
<li>48001 &#x3D; 120 * 400 + 1bias，每层一个偏置，可以类比普通的神经网络。</li>
</ul>
</li>
</ul>
</li>
<li>激活值<ul>
<li>随着神经网络加深，激活值会逐渐变小。如果激活值下降太快，也会影响神经网络的表现。</li>
</ul>
</li>
</ul>
<p>卷积神经网络的重点是如何更好地组织卷积层、池化层、全连接层。这要求我们多阅读论文，了解别人的模型，得到自己的insight&#x2F;intuation。下周将介绍一些表现良好的模型。</p>
<h4 id="1-11-为什么使用卷积？（Why-convolutions-）"><a href="#1-11-为什么使用卷积？（Why-convolutions-）" class="headerlink" title="1.11 为什么使用卷积？（Why convolutions?）"></a>1.11 为什么使用卷积？（Why convolutions?）</h4><p>卷积神经网络为何有效？如何整合这些卷积？如何通过标注过的训练集进行卷积神经网络的训练？</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/129.png' width="80%" height="80%"/ loading="lazy">

<p>卷积神经网络相比只有全连接的普通神经网络的优势：参数共享和稀疏连接。</p>
<ul>
<li><p>参数共享：filter的参数可以用于图片的任何区域，来提取特征。</p>
</li>
<li><p>稀疏链接：输出图像的每个像素仅与几个源图像的像素有关（不是全连接）。</p>
</li>
</ul>
<p>这两点保证了<strong>卷积神经网络可以用比较小的数据集进行训练，并且不容易过拟合</strong>。</p>
<p>卷积神经网络善于捕捉平移不变（translation invariance），因为神经网络的卷积结构保证了，即使移动几个像素，图片依然具有非常相似的特征。</p>
<p>训练卷积神经网络的过程：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/130.png' width="80%" height="80%"/ loading="lazy">

<p>通过梯度下降或其他优化算法，优化参数，让损失函数 $J$ 降到最低。</p>
<h3 id="第二周-深度卷积网络：实例探究-Deep-convolutional-models-case-studies"><a href="#第二周-深度卷积网络：实例探究-Deep-convolutional-models-case-studies" class="headerlink" title="第二周 深度卷积网络：实例探究(Deep convolutional models: case studies)"></a>第二周 深度卷积网络：实例探究(Deep convolutional models: case studies)</h3><p>一些卷积神经网络的实例分析：Classic networks（LeNet-5，AlexNet，VGG），ResNet，Inception（1×1卷积）；计算机视觉问题建立在小数据系统上，需要进行：数据增强，迁移训练，使用开源。</p>
<h4 id="2-1-为什么要进行实例探究？"><a href="#2-1-为什么要进行实例探究？" class="headerlink" title="2.1 为什么要进行实例探究？"></a>2.1 为什么要进行实例探究？</h4><p>好的网络架构可能在其他任务中也好用。</p>
<h4 id="2-2-经典网络"><a href="#2-2-经典网络" class="headerlink" title="2.2 经典网络"></a>2.2 经典网络</h4><p>红笔写的是现在基本不用的技术。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/131.png' width="80%" height="80%"/ loading="lazy">

<p>在LeNet提出时，使用的这些技术现在已经基本被取代了：sigmoid和tanh激活函数；平均池化；valid 卷积；受限于计算能力，卷积的计算方法也很复杂。现在用的是：ReLU；最大池化；same卷积。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/132.png' width="80%" height="80%"/ loading="lazy">

<p>在AlexNet中，使用了ReLU、same卷积、max-pool、设置stride、softmax等新技术。</p>
<p>LeNet-5大约有60,000个参数；AlexNet有大约60,000,000个参数。</p>
<p>在AlexNet提出时，GPU的处理速度还比较慢，所以AlexNet采用了很复杂的方法在两个GPU上训练。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/133.png' width="80%" height="80%"/ loading="lazy">

<p>VGG-16网络没有很多超参数，专注于构建卷积层。16的意思是网络中有16层有权值的地方（2+2+3+3+3&#x3D;13卷积层，3全连接层）。</p>
<ul>
<li>CONV &#x3D; 3×3 filter, s&#x3D;1, same</li>
<li>MAX-POOL &#x3D; 2×2, s&#x3D;2</li>
</ul>
<p>VGG-16 有约 138,000,000 个参数，但结构很规整，图像缩小的比例和channel增加的比例是有规律的。后面的VGG-19比这个模型更大，但这两个模型表现差不多。</p>
<h4 id="2-3-残差网络（Residual-Networks-ResNets-）"><a href="#2-3-残差网络（Residual-Networks-ResNets-）" class="headerlink" title="2.3 残差网络（Residual Networks (ResNets)）"></a>2.3 残差网络（Residual Networks (ResNets)）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/134.png' width="80%" height="80%"/ loading="lazy">

<p>很深的神经网络难以训练，因为存在梯度消失和梯度爆炸的问题。使用ResNet，可以训练北京深层的神经网络。</p>
<p>每两层组成一个残差块：浅层的激活值通过short cut，直接输入到深层的非线性函数（如ReLU）中。</p>
<p>$$a^{[l+2]} &#x3D; g(z^{[l+2]}+a^{[l]})$$ </p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/135.png' width="80%" height="80%"/ loading="lazy">

<p>论中将没有使用残差块的神经网络叫做Plain网络，在理论上层数越多，损失越小；但实际情况是，网络越深，在训练集上的误差会反弹。ResNet就会解决这一问题。</p>
<h4 id="2-4-残差网络为什么有用？"><a href="#2-4-残差网络为什么有用？" class="headerlink" title="2.4 残差网络为什么有用？"></a>2.4 残差网络为什么有用？</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/136.png' width="80%" height="80%"/ loading="lazy">

<p>如果让 $w,b$ 都为0，那么 $a^{[l+1]} &#x3D; a{^{[l]}}$ ，学习恒等函数对残差块来说很简单。也就是说，虽然加上一个残差块（两层神经网络），效率也不逊色于更简单的神经网络。并且残差块添加的位置也不影响网络的表现。</p>
<p>在不伤害性能的基础上，如果残差块的隐藏单元学习到一些有用信息，那么就能比恒等函数表现得更好。</p>
<p>而对于plain神经网络来说，就算是学习恒等函数的参数都很困难，因此很多层最后的表现变差了。</p>
<p>另外，ResNet使用same卷积，保证 $z^{[l+2]}$ 和 $a^{[l]}$ 有相同的维度，可以相加。如果输入和输出维度不一样，就再增加一个矩阵 $w_s$ 。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/137.png' width="80%" height="80%"/ loading="lazy">

<p>几个之前提到的细节：</p>
<ul>
<li>使用3×3 same卷积，保证 $z^{[l+2]}$ 和 $a^{[l]}$ 有相同的维度，可以相加。</li>
<li>pool-like 层，进行 &#x2F;2 降维操作。</li>
<li>CONV-CONV-CONV-POOL 交替进行的结构。</li>
</ul>
<h4 id="2-5-网络中的网络-x2F-1×1卷积"><a href="#2-5-网络中的网络-x2F-1×1卷积" class="headerlink" title="2.5 网络中的网络 &#x2F; 1×1卷积"></a>2.5 网络中的网络 &#x2F; 1×1卷积</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/138.png' width="80%" height="80%"/ loading="lazy">

<p>1×1卷积添加了非线性函数，可以让网络学习更复杂的函数。</p>
<p>1×1卷积对单通道作用不大，但对于多通道，可以把所有通道相同位置的数输出成一个数（对应位置相乘 -&gt; 相加 -&gt; ReLU）。如果filter数量不止一个，可以输出多个通道。</p>
<p>论文名字叫 network in network ，这种方法也可以称为1×1卷积。论文中的架构没有得到广泛使用，但这种方法利用到了之后的Inception等模型上。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/139.png' width="80%" height="80%"/ loading="lazy">

<p>作用如上图。POOL的作用是压缩 $n_H,n_W$，而 1×1 卷积可以压缩 $n_C$，减少信道数量来简化计算。当然让信道数量保持不变或者增加也可以。</p>
<h4 id="2-6-Inception-模块简介、"><a href="#2-6-Inception-模块简介、" class="headerlink" title="2.6 Inception 模块简介、"></a>2.6 Inception 模块简介、</h4><p>Inception模块：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/140.png' width="80%" height="80%"/ loading="lazy">

<p>Inception：不需要人来决定使用什么规格的filter、是否使用POOL。网络结构更复杂但表现更好。</p>
<p>如图，Inception模块输入某个量，经过不同的处理，输出将这些结果叠加起来。</p>
<p>Inception网络不需要人为决定使用哪个fitler，或是否需要池化，而是由网络自行决定这些参数。我们可以给网络添加这些参数的所有可能的值，然后把这些输出连接起来，让网络学习他需要什么样的参数。</p>
<p>为了维持所有的维度相同，对卷积要使用filter卷积，对池化要使用padding（比较特殊的POOL）。</p>
<p>巨大的运算量：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/141.png' width="80%" height="80%"/ loading="lazy">

<p>Inception模块的问题是参数多，计算成本高。以 5×5卷积的一部分为例：需要 5×5×192 filter，对于输出的每个数都要做 filter 规格次数的乘法，也就是一共要做 $(28×28×32) * (5×5×192) ≈ 120 M$ 次乘法。</p>
<p>改进方法：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/142.png' width="80%" height="80%"/ loading="lazy">

<p>使用 1×1 卷积得到相同规格的输出，通过压缩成较小的中间形态（瓶颈层bottleneck layer）。</p>
<p>一共要做 $(28×28×16) * (1×1×192)  + (28×28×32) * (5×5×16) ≈ 2.4 M + 10 M &#x3D; 12.4 M$ 次运算，乘法计算的成本大约变为原来的十分之一。</p>
<p>通过合理构建瓶颈层，既可以显著缩小表示层的规模，又不会降低网络性能，从而大量节省计算成本。</p>
<h4 id="2-7-Inception-网络-x2F-GoogLeNet（Inception-network）"><a href="#2-7-Inception-网络-x2F-GoogLeNet（Inception-network）" class="headerlink" title="2.7 Inception 网络 &#x2F; GoogLeNet（Inception network）"></a>2.7 Inception 网络 &#x2F; GoogLeNet（Inception network）</h4><p>Inception模块：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/143.png' width="80%" height="80%"/ loading="lazy">



<p>Inception网络：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/144.png' width="80%" height="80%"/ loading="lazy">

<p>红圈：由Inception模块重复堆叠而来，有些max-pooling层，来改变长和宽。</p>
<p>绿圈：一些分支，通过一些隐藏层，做一个softmax分类。它确保了即使是隐藏单元和中间层，也参与了特征运算，也能进行预测图片的分类。并且防止网络过拟合。</p>
<p>其他：也有变体把 Inception 和 ResNet 结合起来。可以看Inception的后续论文。</p>
<h4 id="2-8-使用开源的实现方案"><a href="#2-8-使用开源的实现方案" class="headerlink" title="2.8 使用开源的实现方案"></a>2.8 使用开源的实现方案</h4><p>很多神经网络难以复现，因为一些超参数的细节调整会影响性能。</p>
<p>先从使用开源的实现开始。</p>
<h4 id="2-9-迁移学习（Transfer-Learning）"><a href="#2-9-迁移学习（Transfer-Learning）" class="headerlink" title="2.9 迁移学习（Transfer Learning）"></a>2.9 迁移学习（Transfer Learning）</h4><p>用迁移学习把公共数据集的知识迁移到我们自己的问题上。</p>
<p>在做一个计算机视觉的应用时，相比于从头训练权重、随机初始化，可以<strong>下载开源的、别人已经训练好的网络结构的权重，作为我们模型的初始化</strong>。</p>
<p>以猫咪分类问题为例，我们使用预训练的 ImageNet 模型，将最后分类改为 softmax 分类这是猫是Tigger，还是Misty，还是都不是。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/145.png' width="80%" height="80%"/ loading="lazy">

<p><strong>图上</strong>：把前面预训练的模型当作冻结的，<strong>只训练跟我们的 softmax 层有关的参数</strong>。</p>
<ul>
<li><p>或许可以设置<code>trainableParameter = 0​</code>或<code>freeze=1</code>这样的参数，指定不训练特定层的权重。</p>
</li>
<li><p>可以把前面冻结的模型看作一个函数，输入一张图片，输出一个特征向量。只训练后面的softmax层，用这个特征向量来做预测。因此可以<strong>提前计算训练集中所有样本的这一层的激活值</strong>，然后存到硬盘里，在此之上训练softmax层。这样就不用每次遍历数据集重新计算这一层的激活值了。</p>
</li>
</ul>
<p><strong>图中</strong>：如果模型特别大，可以freeze一部分模型，然后<strong>训练后面的模型</strong>。也可以freeze一部分模型，把后面的模型进行修改。</p>
<ul>
<li>规律：<strong>数据集越大，需要冻结的层数越少，需要进行训练的层数越多</strong>。</li>
</ul>
<p><strong>图下</strong>：如果有特别多的数据，就用开源的网络和它的权重当作参数的初始化，然后<strong>训练整个网络</strong>。</p>
<p>其他：计算机视觉问题中，迁移学习特别常用。除非有一个极其大的数据集，才从头开始训练所有东西。</p>
<h4 id="2-10-数据增强（Data-augmentation）"><a href="#2-10-数据增强（Data-augmentation）" class="headerlink" title="2.10 数据增强（Data augmentation）"></a>2.10 数据增强（Data augmentation）</h4><p>计算机视觉方面的主要问题是没有办法得到充足的数据。当训练模型时，不管是从别人预训练的模型进行迁移学习，还是从源代码开始训练模型，数据增强会经常有所帮助。</p>
<p>数据增强的方法：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/146.png' width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/147.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li>镜像翻转、随即裁剪（保留主体）、旋转、剪切、局部弯曲 等。也可以组合起来用，但因为太复杂，实际上用的很少。</li>
<li>色彩转换，进行RGB的调整，一般是根据某种概率分布来决定改变的值。<ul>
<li>*对RGB不同的采样方式：使用PCA（见机器学习网课笔记）。在AlexNet的论文中，成为“PCA color augmentation”，比如我们的图片呈紫色（红蓝多，绿少），那么PCA颜色增强算法会对红蓝有大的增减幅度，对绿的变化相对少，以此使总体的颜色保持一致。</li>
</ul>
</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/148.png' width="80%" height="80%"/ loading="lazy">

<p>如果数据集比较大，常用的方法是设置单个thread，串行读取数据、进行数据增强。</p>
<ul>
<li><p>thread A：<strong>从硬盘读数据并数据增强</strong>。CPU有一个thread不停地从硬盘中读取数据，同时进行变形或颜色转换形成新的图像，从而构成一个batch或者mini-batch的数据；</p>
</li>
<li><p>thread B：<strong>训练</strong>。这些数据被传递给其他thread（可能是CPU或GPU），进行模型的训练。</p>
</li>
</ul>
<p><strong>以上两个thread可以并行实现。</strong></p>
<p>其他：数据增强也有一些超参数，比如如何进行颜色变化等。方法依然是学习开源的实现。</p>
<h4 id="2-11-计算机视觉现状"><a href="#2-11-计算机视觉现状" class="headerlink" title="2.11 计算机视觉现状"></a>2.11 计算机视觉现状</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/149.png' width="80%" height="80%"/ loading="lazy">

<p>从数据少到数据多的应用场景：目标检测，图像识别，语音识别</p>
<p>数据很多时：不需要精心设计模型，可以使用简单的算法、较少的人工。</p>
<p>数据很少：进行更多手工工程，进行迁移学习。</p>
<p>知识的两种来源：标签；手工工程。如果没有很多标签，就用更多的手工。</p>
<p><strong>计算机视觉问题通常数据相对较少，更多依赖于手工工程，并且设计比较复杂的网络架构，有比较复杂的超参数选择问题</strong>。可以说：<strong>计算机视觉问题建立在小数据系统</strong>。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/150.png' width="80%" height="80%"/ loading="lazy">

<p>在benchmarks（基准测试）&#x2F;竞赛上取得好的表现的方法：</p>
<ul>
<li>集成<ul>
<li>当构思好神经网络之后，独立训练几个网络，并对它们的输出求平均。</li>
<li>集成需要保留多个网络，对内存有比较大的占用。</li>
</ul>
</li>
<li>multi-crop<ul>
<li>这是一种将数据增强扩展到测试集的方法。以10-crop为例，把测试集的图片分为（中心裁剪、四角裁剪、镜像中心裁剪、镜像四角裁剪）十张图片，分别对它们进行分类，并对输出求平均。</li>
<li>multi-crop只需要保留一个网络，不会占用太多内存，但会让运行时间变慢。</li>
</ul>
</li>
</ul>
<p>需要注意，<strong>这是在基准测试和竞赛中使用的方法，不要在真实生产场景下使用这些方法，因为会让运行时间变慢。</strong></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/151.png' width="80%" height="80%"/ loading="lazy">

<p>由于计算机视觉问题建立在小数据系统，其他人已经完成了大量的网络架构的手工工程；并且一个神经网络在一个视觉问题上很有效，通常也会解决其他视觉问题。</p>
<p>所以想建立一个使用的系统，最好先从其他人的神经网络架构入手。尤其是开源系统，会包含超参数设置等细节问题。<strong>最好使用预训练的模型，在我们自己的数据集上进行调优</strong>。</p>
<h3 id="第三周-目标检测（Object-detection）"><a href="#第三周-目标检测（Object-detection）" class="headerlink" title="第三周 目标检测（Object detection）"></a>第三周 目标检测（Object detection）</h3><p>(1) 能完成 classification 的卷积神经网络。我们希望找到 location</p>
<p>(2) 结合滑动窗口进行 detection（滑动窗口+CNN），可以找到 location 了。但计算量大</p>
<p>(3) 滑动窗口的卷积实现（overfeat），计算简单了。但滑动窗口精度不高</p>
<p>(4) YOLO算法，划分单元格，每个格子预测中点在格子内的物体和物体的边框。可以预测物体的精确边框了。但单个物体有多个边框</p>
<p>(5) mon-max supression，单个物体只有一个边框了。但一个格子只能检测一个对象</p>
<p>(6) anchor boxes，一个格子可以检测多个对象了。</p>
<h4 id="3-1-目标定位（Object-localization）"><a href="#3-1-目标定位（Object-localization）" class="headerlink" title="3.1 目标定位（Object localization）"></a>3.1 目标定位（Object localization）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/152.png' width="80%" height="80%"/ loading="lazy">

<p>detection问题中，图片可以包含多个对象，甚至是多个不同分类的对象。</p>
<p>classification 的思路可以帮助学习 classification with localization 问题；classification with localization 的思路又有助于学习 detection 问题。</p>
<p>我们从 classification with localization 问题开始。</p>
<p>classification with localization pipeline：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/153.png' width="80%" height="80%"/ loading="lazy">

<p>图片输入ConvNet，输出一个4分类的softmax + 一个定位坐标。</p>
<p>标签 y 定义如下：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/154.png' width="80%" height="80%"/ loading="lazy">

<p>假定图片中最多出现一个对象。输出 y ：</p>
<ul>
<li>P_c：是否有物体（没有就是第四类background）</li>
<li>bx, by, bh, bw：物体框</li>
<li>c_1, c_2, c_3：是哪一类物体</li>
</ul>
<p>图右侧有两个例子，”?” 是不需要关心的数。</p>
<p>损失函数定义见图左。当 $y_1&#x3D;1$，使用 squared erorr；当 $y_1&#x3D;0$，不许考虑其他元素，只看 P_c 的准确度。</p>
<p>也可以对不同部分使用不同的损失函数，比如对分类部分 c_1, c_2, c_3 使用对数，对 bx, by, bh, bw 使用平方误差，对 P_c 使用逻辑回归损失函数。全用 squared error 也是可以的。</p>
<h4 id="3-2-特征点检测（Landmark-detection）"><a href="#3-2-特征点检测（Landmark-detection）" class="headerlink" title="3.2 特征点检测（Landmark detection）"></a>3.2 特征点检测（Landmark detection）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/155.png' width="80%" height="80%"/ loading="lazy">

<p>以面部识别为例，选定脸部特征点的个数，并生成包含这些特征点的标签训练集，就可以利用神经网络输出脸部特征点的位置。</p>
<p>检测关键点也是数据集图形效果的一个关键构造模块，有了关键点，我们可以进行各种处理，比如扭曲、头戴皇冠的AR等等。</p>
<p>为了得到这样的效果，我们需要一个带有关键点的数据集，这个数据集是人工标注的。举个例子：如果有64个关键点，并且采用图中上方的标签结构，就需要一个129维的标签y。</p>
<p>对人的姿态检测问题，也可以建立关键点。</p>
<h4 id="3-3-目标检测（Object-detection）"><a href="#3-3-目标检测（Object-detection）" class="headerlink" title="3.3 目标检测（Object detection）"></a>3.3 目标检测（Object detection）</h4><p>有了上两节的目标定位和特征点检测，可以通过滑动窗口构建目标检测系统了。</p>
<p>step 1：训练一个ConvNet，输入切割好的图片，输出是否是一辆汽车，y&#x3D;0或1.</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/156.png' width="80%" height="80%"/ loading="lazy">

<p>step 2：选定一个特定大小的窗口，将窗口在图片中滑动，把每个切片输入ConvNet进行识别。一次滑动结束后，使用更大的窗口重复上述操作。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/157.png' width="80%" height="80%"/ loading="lazy">

<p>不管汽车在图片中的哪里，总有一个窗口能让汽车被识别出来。</p>
<p>滑动窗口的一个大问题是计算开销。如果步幅很大，会让输入ConvNet的窗口切片减少，但粗粒度可能会影响性能。如果采用小粒度或小步幅，传递给ConvNet的窗口切片会特别多，计算成本很高。</p>
<p>这个问题已经有了比较好的解决方法，在下节讲。</p>
<h4 id="3-4-滑动窗口的卷积实现（Convolutional-implementation-of-sliding-windows）"><a href="#3-4-滑动窗口的卷积实现（Convolutional-implementation-of-sliding-windows）" class="headerlink" title="3.4 滑动窗口的卷积实现（Convolutional implementation of sliding windows）"></a>3.4 滑动窗口的卷积实现（Convolutional implementation of sliding windows）</h4><p>首先知道怎样把全连接层转换成卷积层：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/158.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>为了代替将 5×5×16 展开成 400 维的全连接层：进行 400 个 5×5×16 卷积核的卷积，得到 1×1×400 的输出层。 1×1×400 的输出是上一层  5×5×16 激活值经过某个线性函数的输出结果。</p>
</li>
<li><p>为了代替 400 到 400 的全连接层：进行 400 个 1×1 卷积核的卷积。</p>
</li>
<li><p>为了代替 400 到 softmax分类的全连接层：进行 4 个 1×1 卷积核的卷积接一个softmax函数，最终得到 1×1×4 的输出层。</p>
</li>
</ul>
<p>滑动窗口的卷积实现：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/159.png' width="80%" height="80%"/ loading="lazy">

<p>我们有一个 14×14×3 作为输入的ConvNet。如果在 16×16×3 的大图像做滑动窗口，会划分为四部分、进行四次卷积。结果发现，这四次卷积操作中的很多计算都是重复的。</p>
<p>直接对大图进行相同的卷积操作。现在输出层为 2×2×400 而不是 1×1×400。这四次卷积分别对应输出层 2×2 的四个角。（绿色区域的卷积是其中一个例子）</p>
<p>通过把滑动窗口的很多次卷积作为一张图片输入给ConvNet进行计算，其中的公有区域可以共享很多计算。</p>
<p>同理，对 28×28×3 的图片直接进行卷积，相当于做 8×8 次步长为 2 （因为MAX POOL是2×2）的卷积，并且把结果按滑动窗口的顺序排列起来。</p>
<p>总结：对大图直接进行卷积，一次得到所有预测值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/160.png' width="80%" height="80%"/ loading="lazy">

<p>这个算法效率高，但仍然存在一个缺点：边界框的位置可能不够准确，由于步长的存在，滑动窗口可能不能很好的框选住物体。在下节解决这个问题。</p>
<p>其他：这个思路也被R-CNN借鉴，从而诞生了Fast R-cNN算法。</p>
<h4 id="3-5-Bounding-Box-预测-YOLO-算法"><a href="#3-5-Bounding-Box-预测-YOLO-算法" class="headerlink" title="3.5 Bounding Box 预测 - YOLO 算法"></a>3.5 Bounding Box 预测 - YOLO 算法</h4><p>更精准的边界框预测算法：YOLO算法。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/161.png' width="80%" height="80%"/ loading="lazy">

<p>把图片分成若干格，对每一格应用之前讲过的 classification with localization 算法。对于里面有车的格子，预测出绿色、黄色的标签；对于其他格子，预测出来紫色的标签。<strong>每个单元格负责预测中点位于该格子内的物体和物体的边界框。</strong></p>
<p>以图中 3×3 格子，长度为 8 的向量，总的输出尺寸是 3×3×8。</p>
<p>如果我们训练这样一个神经网络：输入为 100×100×3 的图片，经过一个ConvNet，最终映射到 3×3×8的输出。通过反向传播训练这个网络，使其能将任意输入x映射到这类输出向量y。这个神经网络可以输出精确的边界框。</p>
<p>细节补充：</p>
<ul>
<li><p>根据物体的中心点划分它所在的格子，即使对象可以跨越多个格子，也只会被分配到一个格子。</p>
</li>
<li><p>在实践中可以使用更精细的格子划分，比如 19×19，降低不同物体中心位于同一个格子的概率。</p>
</li>
<li><p>在YOLO中也使用滑动窗口的卷积实现，不会分别计算每个格子经过ConvNet的输出。这加速了YOLO算法的运行，实际上它的运行速度很快，可以达到实时识别。</p>
</li>
</ul>
<p>表示 bounding box 的约定：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/162.png' width="80%" height="80%"/ loading="lazy">

<p>bx, by, bh, bw 单位是相对格子尺度的比例。</p>
<ul>
<li>bx，by 必须在 0 到 1 之间。</li>
<li>bw，bh 可以大于1，因为车的尺寸可能比一个格子大。</li>
</ul>
<p>其他：YOLO论文比较难懂，一些细节很难理解。</p>
<h4 id="3-6-交并比（Intersection-over-union）"><a href="#3-6-交并比（Intersection-over-union）" class="headerlink" title="3.6 交并比（Intersection over union）"></a>3.6 交并比（Intersection over union）</h4><p>如何判断 object detection 算法运作良好？定义IoU的概念。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/163.png' width="80%" height="80%"/ loading="lazy">

<p>实际的 bounding box 是红色，我们预测的是紫色。IoU 计算它们交集（intersection）和并集（union）的比值，并跟一个阈值进行对比。</p>
<h4 id="3-7-非极大值抑制（Non-max-suppression）"><a href="#3-7-非极大值抑制（Non-max-suppression）" class="headerlink" title="3.7 非极大值抑制（Non-max suppression）"></a>3.7 非极大值抑制（Non-max suppression）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/164.png' width="80%" height="80%"/ loading="lazy">

<p>分格子进行检测后，不仅中心点的格子会认为自己含有某个物体，周围的格子也会认为自己检测出了这个物体。导致很多格子的 P_c 值会比较高（图中的数字）。导致对同一个对象做出多次检测。</p>
<p><strong>非极大值抑制</strong>：首先看概率最大的一个，标记这里检测出了车（图中高亮）；然后逐一检查剩下的矩形，对所有与这个bounding box有很高交并比（IoU）的其他bounding box的输出抑制。</p>
<p>通过非极大值抑制，确保算法对每个对象只检测一次。</p>
<p>算法步骤：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/165.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-8-Anchor-Boxes"><a href="#3-8-Anchor-Boxes" class="headerlink" title="3.8 Anchor Boxes"></a>3.8 Anchor Boxes</h4><p>现在的问题：每个格子只能检测出一个对象。如何能让一个格子检测多个（中点位于格子内的）对象？</p>
<p>方法：设置几个anchor box。以两个为例：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/166.png' width="80%" height="80%"/ loading="lazy">

<p>如果有两个anchor box，标签变为16维，输出从 3×3×8 变为 3×3×16。</p>
<p>文字描述：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/167.png' width="80%" height="80%"/ loading="lazy">

<p>检测到的物体，寻找跟哪个 anchor box 的交并比更高，然后填入 $y$ 相应的位置。</p>
<p>一个具体例子：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/168.png' width="80%" height="80%"/ loading="lazy">

<p>如果指定 anchor box 1 大约是行人形状，anchor box 2 大约是汽车形状，同时有人和车、只有车的 $y$ 如图右侧所示。</p>
<p>异常情况：</p>
<ul>
<li><p>如果一个格子中有三个对象，但只设置了两个 anchor box</p>
</li>
<li><p>同一个格子的几个 anchor box 形状相似</p>
</li>
</ul>
<p>这两种情况发生时，算法没有好的解决方法，需要引入一些打破僵局的默认手段专门处理这些情况。</p>
<p>其他：</p>
<ul>
<li>anchor box 要处理的格子有多个对象的中点问题出现的很少（361个格子很难重复），但设立 anchor box 的好处在于能让学习算法更具有对数据集的针对性，尤其是数据集中有一些很瘦很高的对象，比如行人，或者汽车这样很宽的对象。</li>
<li>如何选择 anchor box 的形状？<ul>
<li>一般是手工指定。选择 5~10 个，涵盖想要检测的对象的各种形状。</li>
<li>另一个更高级的方法：使用 k-means ，将两类对象形状聚类，来选择最具代表性的 anchor box。</li>
</ul>
</li>
</ul>
<h4 id="3-9-整合起来：YOLO-算法"><a href="#3-9-整合起来：YOLO-算法" class="headerlink" title="3.9 整合起来：YOLO 算法"></a>3.9 整合起来：YOLO 算法</h4><p><strong>训练模型</strong>：训练一个卷积神经网络，输入图片，输出相应的标签 $y$。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/169.png' width="80%" height="80%"/ loading="lazy">



<p><strong>进行预测</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/170.png' width="80%" height="80%"/ loading="lazy">



<p><strong>进行非最大值抑制</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/171.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>对于每个格子，都有两个 anchor box 预测的结果。有些 bounding box可以超出所在格子的宽高。  </p>
</li>
<li><p>丢掉概率很小（P_c）的预测。</p>
</li>
<li><p>对每个类别，单独运行非最大值抑制。分别找到独立的行人、汽车、摩托。</p>
</li>
</ul>
<h4 id="3-10-候选区域（Region-proposals-）"><a href="#3-10-候选区域（Region-proposals-）" class="headerlink" title="3.10 *候选区域（Region proposals ）"></a>3.10 *候选区域（Region proposals ）</h4><p>在目标检测领域论文中的常见算法，在少数窗口上运行CNN分类器。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/172.png' width="80%" height="80%"/ loading="lazy">

<p>有些滑动窗口并没有进行识别的价值。因此 R-CNN 算法尝试选出一些区域，在这些区域上运行CNN分类器是有意义的。</p>
<p>运行图像分割算法，找到一些色块（如2000个），在色块上放置边界框，然后跑CNN分类器查看结果。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/173.png' width="80%" height="80%"/ loading="lazy">

<p>可以看出，R-CNN运行比较慢。</p>
<p>basic R-CNN使用某种算法求出候选区域，然后对每个候选区域跑一下CNN分类器，每个区域输出一个标签+边界框。（得到的边界框更精确，而不是色块的边界框）</p>
<p>Fast R-CNN：滑动窗口的卷积实现。加速了R-CNN。</p>
<p>得到预选区域的聚类步骤（propose regions）仍然比较缓慢。</p>
<p>Faster R-CNN：使用CNN而不是传统图像分割算法，来获得候选区域色块。</p>
<p>其他：大多数更快的R-CNN算法实现还是比YOLO慢很多。因为R-CNN需要两步，先得出预选区域，然后进行识别；而YOLO是 you only look once。</p>
<h3 id="第四周-特殊应用：人脸识别和风格迁移"><a href="#第四周-特殊应用：人脸识别和风格迁移" class="headerlink" title="第四周 特殊应用：人脸识别和风格迁移"></a>第四周 特殊应用：人脸识别和风格迁移</h3><h4 id="4-1-什么是人脸识别？"><a href="#4-1-什么是人脸识别？" class="headerlink" title="4.1 什么是人脸识别？"></a>4.1 什么是人脸识别？</h4><p>识别人脸+活体检测。</p>
<p>一些术语：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/174.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>人脸验证（verification）：输入图片和身份，验证图片和身份是否对应。一对一。</p>
</li>
<li><p>人脸识别（recognition）：输入图片，识别出身份。一对多。</p>
</li>
</ul>
<p>先构建verification系统。如果表现够好，就将其用在recognition系统上。</p>
<h4 id="4-2-One-Shot学习（One-shot-learning）"><a href="#4-2-One-Shot学习（One-shot-learning）" class="headerlink" title="4.2 One-Shot学习（One-shot learning）"></a>4.2 One-Shot学习（One-shot learning）</h4><p>需要通过单张图片或单个人脸样例，就可以识别这个人的身份。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/175.png' width="80%" height="80%"/ loading="lazy">

<p>如果使用图中画出的结构，训练CNN进行softmax识别，那么如果新加入一个人，就要修改输出数量并且重新训练CNN。</p>
<p>需要做到one-shot：只通过一个样本进行学习。</p>
<p>具体来说：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/176.png' width="80%" height="80%"/ loading="lazy">

<p>不训练CNN，而是学习similarity函数 $d(img1,1mg2)$ 。这个函数输入两张图片，输出<strong>差异值</strong>。</p>
<p>用输入图片跟数据库中的图片计算差异值，跟阈值比较，进行判断。</p>
<p>添加新的图片，也可以正常工作。</p>
<h4 id="4-3-Siamese-网络（Siamese-network）"><a href="#4-3-Siamese-网络（Siamese-network）" class="headerlink" title="4.3 Siamese 网络（Siamese network）"></a>4.3 Siamese 网络（Siamese network）</h4><p>想要学习similarity函数 $d(img1,1mg2)$ ，输入两张图片，输出它们的差异度或相似度。</p>
<p>实现的一个方式就是用 Siamese 网络，思路是：<strong>对于不同的输入运行相同的CNN，然后比较它们的输出</strong>。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/177.png' width="80%" height="80%"/ loading="lazy">

<p>有一个网络，输入一张图片，可以得到一个128维的向量。如果把两张图片 $x^{(1)},x^{(2)}$ 输入同样的网络，可以得到输出向量 $f(x^{(1)}),f(x^{(2)})$ 。</p>
<p>将 $d(x^{(1)},x^{(2)})$ 定义为两张图片编码之差的范数：</p>
<p>$$d(x^{(1)},x^{(2)})&#x3D;||f(x^{(1)})-f(x^{(2)})||^2_2$$ </p>
<p>从训练层面上：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/178.png' width="80%" height="80%"/ loading="lazy">

<p>训练神经网络参数，目标是：相同人物的图片，输出相似；不同任务的图片，输出相差较大。</p>
<p>下节：有了训练目标，具体定义怎样的损失函数？</p>
<h4 id="4-4-Triplet-损失"><a href="#4-4-Triplet-损失" class="headerlink" title="4.4 Triplet 损失"></a>4.4 Triplet 损失</h4><p>想让神经网络学习到比较好的图片编码，方法之一是定义三元组损失函数（triplet loss），然后应用梯度下降。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/179.png' width="80%" height="80%"/ loading="lazy">

<p>同时看三张图片：样本图片Anchor、同样身份的图片Positive、不同身份的图片Negative。</p>
<p>我们想要：</p>
<p>$$||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 ≤ 0   $$ </p>
<p>为了确保不学习到全为0，来满足方程的编码方式：设置超参数 $\alpha$ ，作为margin。</p>
<p>$$||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + \alpha ≤ 0   $$ </p>
<p>也就是要求<strong>不同身份的编码差异度要比相同身份的编码差异度大很多</strong>。</p>
<p>triplet loss 定义如下：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/180.png' width="80%" height="80%"/ loading="lazy">

<p>$$L(A,P,N) &#x3D; max(||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + \alpha ,0   )$$ </p>
<p>$$J&#x3D;\sum^m_{i&#x3D;1}L(A^{(i)},P^{(i)},N^{(i)}) $$ </p>
<p>使用max函数，达到让第一项小于等于0的效果，而且不关心比0小多少。</p>
<p>注意：为了定义<strong>三元组的数据集</strong>，需要成对的A、P，也就是<strong>需要同一个人的多张照片</strong>。这也是图中举例的数据集，1k人有10k图片的原因。在训练结束后，就达到了one-shot的目的，可以<strong>给某个人的一张照片进行识别</strong>。</p>
<p>数据集的问题：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/181.png' width="80%" height="80%"/ loading="lazy">

<p>如果随机选择A、P、N，那么约束条件 $d(A,p)+\alpha ≤ d(A,N)$ 很容易满足，因为随机选择的图片 A和P 的差异比 A和N 的差异小很多。网络并不能从中学习到什么。</p>
<p>我们需要<strong>尽可能选择“难以识别”的三元组，组合成数据集</strong>。</p>
<p>总览：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/182.png' width="80%" height="80%"/ loading="lazy">

<p>制作三元组数据集，使用三元组loss，让模型输出一个图片的编码。</p>
<p>其他：如今已经有很多公司使用特别多的数据、训练了大型的模型，并开源了模型参数。所以相比于从头开始，可以<strong>下载别人的预训练模型</strong>。同时也要了解这些模型的训练方法。在下一节讲Siamese的变体，以及如何训练这些模型。</p>
<h4 id="4-5-面部验证与二分类（Face-verification-and-binary-classification）"><a href="#4-5-面部验证与二分类（Face-verification-and-binary-classification）" class="headerlink" title="4.5 面部验证与二分类（Face verification and binary classification）"></a>4.5 面部验证与二分类（Face verification and binary classification）</h4><p>除了triplet loss，也可以把verification当作一个二分类问题。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/183.png' width="80%" height="80%"/ loading="lazy">

<p>使用siamese的架构，两张图片分别计算编码，然后输入一个逻辑回归单元，进行预测，相同的身份输出1，不同的身份输出0。</p>
<p>注意：</p>
<ul>
<li>siamese架构，上下两个网络是相同的。</li>
<li>输入是两张图片，输出一个0或1。</li>
<li>假设上面是每次要识别的新图片，下面是数据集的图片，可以把数据集图片的编码都保存下来，不需要每次都经过网络，也不需要保存原图片。</li>
</ul>
<p>总览：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/184.png' width="80%" height="80%"/ loading="lazy">

<p>创建二元组数据集。</p>
<h4 id="4-6-什么是神经风格迁移？（neural-style-transfer）"><a href="#4-6-什么是神经风格迁移？（neural-style-transfer）" class="headerlink" title="4.6 什么是神经风格迁移？（neural style transfer）"></a>4.6 什么是神经风格迁移？（neural style transfer）</h4><p>把 Style 迁移到 Content 上。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/185.png' width="80%" height="80%"/ loading="lazy">

<p>要完成这个任务，需要组合不同深度、不同层的卷积神经网络的中间值。</p>
<h4 id="4-7-深度卷积网络在学习什么？"><a href="#4-7-深度卷积网络在学习什么？" class="headerlink" title="4.7 深度卷积网络在学习什么？"></a>4.7 深度卷积网络在学习什么？</h4><p>直观理解：卷积网络中深度较大的层在做什么。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/186.png' width="80%" height="80%"/ loading="lazy">

<p>可视化 Layer 1 的9个最大程度激活的隐藏units。可以看到，每个units在理解一些比较简单的信息，比如颜色、边缘。</p>
<p>随着层数加大，网络理解一些高维、完整的特征。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/187.png' width="80%" height="80%"/ loading="lazy">

<p>可以联想“感受野”的概念。</p>
<h4 id="4-8-代价函数"><a href="#4-8-代价函数" class="headerlink" title="4.8 代价函数"></a>4.8 代价函数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/188.png' width="80%" height="80%"/ loading="lazy">

<p>$$J(G) &#x3D; \alpha J_{content}(C, G) + \beta J_{style}(S,G)  $$  </p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/189.png' width="80%" height="80%"/ loading="lazy">

<p>获得生成图片G的步骤：</p>
<ul>
<li><p>随机初始化G</p>
</li>
<li><p>梯度下降，在最小化损失函数的过程中更新G</p>
</li>
</ul>
<h4 id="4-9-内容代价函数"><a href="#4-9-内容代价函数" class="headerlink" title="4.9 内容代价函数"></a>4.9 内容代价函数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/190.png' width="80%" height="80%"/ loading="lazy">

<p>用预训练卷积神经网络的隐藏层 $l$ 来计算内容损失，通常选择 $l$ 在网络的中间层，不太浅也不太深。</p>
<p>就像本章前面所讲，我们用这张图片的编码来计算内容损失。</p>
<p>$$J_{content}(C,G) &#x3D; \frac{1}{2}||a^{(l)(C)}- a^{(l)(G)}||^2  $$</p>
<h4 id="4-10-风格代价函数"><a href="#4-10-风格代价函数" class="headerlink" title="4.10 风格代价函数"></a>4.10 风格代价函数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/191.png' width="80%" height="80%"/ loading="lazy">

<p>把图片的风格定义为：$l$ 层中各个通道之间激活项的相关性系数。</p>
<p>如何计算这个系数？</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/192.png' width="80%" height="80%"/ loading="lazy">

<p>比如我们有红色、黄色两个channel的激活值，如上图左下方所示，红色channel学习垂直纹理，黄色channel学习橘红色，什么时候两个通道有较高的相关性呢？</p>
<ul>
<li>图片中出现垂直纹理的地方，有很大概率是橘红色的，就说这两个channel有相关性。</li>
<li>图片中有垂直纹理的地方，很小概率是橘红色的，就说这两个channel没有相关性。</li>
</ul>
<p>相关系数度量的就是这个概率。</p>
<p>在生成图像G中，测量channel之间的相关系数，与S的相关系数做对比。这样就能测量生成图像G的风格和输入图像S的相似程度了。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/193.png' width="80%" height="80%"/ loading="lazy">

<p>对于G和S，分别计算一个风格矩阵。</p>
<ul>
<li>用第 $l$ 层来测量风格，$a^{[l]}_{i,j,k} $ 是 $l$ 层中 $(i,j,k)$ 位置的激活值。$i,j,k$ 分别是高度、宽度、channel。</li>
<li>$G^{(l)(S)}$ 是第 $l$ 层的风格矩阵，规格是 $n_c^{(l)} \times n_c^{(l)}$。<ul>
<li>$S$ 的风格矩阵：$G_{kk’}^{(l)(S)}&#x3D;\sum^{n^{(l)}_H}_{i&#x3D;1}\sum^{n^{(l)}_W}_{j&#x3D;1}a_{ijk}^{(l)(S)}a_{ijk’}^{(l)(S)}  $ </li>
<li>$G$ 的风格矩阵：$G_{kk’}^{(l)(G)}&#x3D;\sum^{n^{(l)}_H}_{i&#x3D;1}\sum^{n^{(l)}_W}_{j&#x3D;1}a_{ijk}^{(l)(G)}a_{ijk’}^{(l)(G)}  $ </li>
<li>用 $G$ 这个字母来表示是因为在线性代数中这也叫 “gram matrix”</li>
<li>做的事情：遍历图中各个高度和宽度，将 $k$ 和 $k’$ 通道中对应位置的激活项相乘并求和。如果两个通道对应的激活值 $a$ 相关，$G$ 就大。</li>
</ul>
</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/194.png' width="80%" height="80%"/ loading="lazy">

<p>最终的风格损失函数：</p>
<p>$$ J_{style}^{(l)}(S,G) &#x3D; ||G^{(l)(S)} - G^{(l)(G)}  ||^2_F    $$</p>
<p>也可以定义神经网络所有层的风格损失函数：</p>
<p>$$ J_{style}(S,G)&#x3D;\sum _l \lambda^{(l)} J_{style}^{(l)}(S,G)   $$</p>
<h4 id="4-11-图像的一维和三维扩展"><a href="#4-11-图像的一维和三维扩展" class="headerlink" title="4.11 图像的一维和三维扩展"></a>4.11 图像的一维和三维扩展</h4><p>从2D图像到1D心电图：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/195.png' width="80%" height="80%"/ loading="lazy">

<p>用一个 1×5 filter 卷积 1×14 原数据，得到 1×10 的输出。</p>
<p>多通道filters同理。用32个 5×16 filter 卷积 10×16 原数据，得到 6×32 的输出。</p>
<p>当然也有RNN等专门处理序列化数据的网络。</p>
<p>3D数据举例：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/196.png' width="80%" height="80%"/ loading="lazy">

<p>CT扫描，若干的切片。</p>
<p>3D数据的卷积如下：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/197.png' width="80%" height="80%"/ loading="lazy">





<h2 id="第五课-序列模型-Sequence-Models"><a href="#第五课-序列模型-Sequence-Models" class="headerlink" title="第五课 序列模型(Sequence Models)"></a>第五课 序列模型(Sequence Models)</h2><p>暂时空缺</p>
<h1 id="第四部分：Tensorflow-2-0-笔记"><a href="#第四部分：Tensorflow-2-0-笔记" class="headerlink" title="第四部分：Tensorflow 2.0 笔记"></a>第四部分：Tensorflow 2.0 笔记</h1><p>整理自北大曹建老师网课，见<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1B7411L7Qt">此处</a></p>
<h3 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1  基本概念"></a>1  基本概念</h3><h4 id="1-1-张量-Tensor"><a href="#1-1-张量-Tensor" class="headerlink" title="1.1 张量 Tensor"></a>1.1 张量 Tensor</h4><p>Tensor：多维数组（列表）。阶：张量的维数。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf1.png'  width="80%" height="80%"/ loading="lazy">

<p>数据类型：</p>
<ul>
<li>整型、浮点型：<code>tf.int32</code> <code>tf.float32</code> <code>tf.float64</code> </li>
<li>bool型：<code>tf.constant([True, False])</code> </li>
<li>string型：<code>tf.constant(&quot;Hello, world.&quot;)</code></li>
</ul>
<p>创建张量：</p>
<ul>
<li><p>z直接创建：<code>tf.constant(张量内容, dtype=数据类型)</code></p>
</li>
<li><p>将numpy转换为Tensor：<code>tf.convert_to_tensor(原数据名, dtype=数据类型)</code></p>
</li>
<li><p>特殊张量：<code>tf.zeros(维度)</code> <code>tf.ones(维度)</code> <code>tf.fill(维度，指定值)</code>，如<code>tf.fill([3, 2, 4], 10)</code></p>
</li>
</ul>
<blockquote>
<p>维度的表示：向量[ , , ]的逗号隔开每个维度的元素个数。有几个逗号就代表tensor有+1维。</p>
</blockquote>
<ul>
<li>随机张量：<ul>
<li>正态分布：<code>tf.random.normal(维度, mean=均值, stddev=标准差)</code></li>
<li>截断式正态分布：<code>tf.random.truncated_normal(维度, mean=均值, stddev=被侦测)</code></li>
<li>均匀分布随机数：<code>tf.random.uniform(维度, minval=最小值, maxval=最大值)</code></li>
</ul>
</li>
</ul>
<blockquote>
<p>区间左闭右开</p>
</blockquote>
<h4 id="1-2-常用函数"><a href="#1-2-常用函数" class="headerlink" title="1.2 常用函数"></a>1.2 常用函数</h4><ul>
<li><p><code>tf.cast(原张量名, dtype=数据类型)</code> ，强制类型转换</p>
</li>
<li><p><code>tf.reduce_min(张量名)</code> <code>tf.reduce_max(张量名)</code> ，计算张量维度上的最值</p>
</li>
<li><p>axis：在一个二位张量或数组中，可以通过axis控制执行维度。</p>
<ul>
<li>axis&#x3D;0 代表跨第一个维度（跨行，经度，纵向操作，down）</li>
<li>axis&#x3D;1 代表跨第二个维度（跨列，纬度，横向操作，across）</li>
</ul>
</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf2.png'  width="80%" height="80%"/ loading="lazy">

<ul>
<li><p><code>tf.Variable()</code> ，标记为“可训练”。被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练的参数。</p>
</li>
<li><p>如：<code>w = tf.Variable(tf.random.normal([2, 2], mean=0, stddev=1))</code></p>
</li>
<li><p>数学运算：</p>
<ul>
<li>四则运算：<code>tf.add</code> ，<code>tf.subtract</code> ，<code>tf.multiply</code> ，<code>tf.divide</code>，参数为两个张量，必须维度相同。</li>
<li>指数运算：<code>tf.square</code> ，<code>tf.pow</code> ，<code>tf.sqrt</code></li>
<li>矩阵乘法：<code>tf.matmul</code></li>
</ul>
</li>
<li><p><code>tf.data.Dataset.from_tensor_slices((输入特征，标签))</code>，把特征值和标签配对。Numpy和Tensor格式都可用该语句读入数据</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">features <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">17</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
labels <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_tensor_slices<span class="token punctuation">(</span><span class="token punctuation">(</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<ul>
<li>在with结构中，使用GradientTape，实现某个函数对指定参数的求导运算。with结构记录计算过程，gradient求出张量的梯度</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
	w <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
	loss <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
grad <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> w<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>grad<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>梯度是 2*w&#x3D;6.0，运行结果：<code>tf.Tensor(6.0, shape=(), dtype=float32)</code></p>
</li>
<li><p><code>enumerate()</code>，索引，返回索引和元素：</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">seq <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'one'</span><span class="token punctuation">,</span> <span class="token string">'two'</span><span class="token punctuation">,</span> <span class="token string">'three'</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> element <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> element<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<pre class="language-markdown" data-language="markdown"><code class="language-markdown">0 one
1 two
2 three</code></pre>
</li>
<li><p><code>tf.one_hot(带转换数据, depth=几分类)</code>，转换成one-hot编码：</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">calsses <span class="token operator">=</span> <span class="token number">3</span>
labels <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span> <span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> tf<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> depth<span class="token operator">=</span>classes<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<pre class="language-markdown" data-language="markdown"><code class="language-markdown">([[0. 1. 0.]
  [1. 0. 0.]
  [0. 0. 1.]], shape=(3, 3), dtype=float32)</code></pre>
</li>
<li><p><code>tf.nn.softmax(张量)</code>，$Softmax(y_i)&#x3D;\frac{e^{y_i}}{\sum_{j&#x3D;0}^ne^{y_i}}$，使输出符合概率分布</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">y <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.01</span><span class="token punctuation">,</span> <span class="token number">2.01</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.66</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y_pro <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y_pro<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<pre class="language-markdown" data-language="markdown"><code class="language-markdown">tf.Tensor([0.22598174 0.69583046 0.0481878 ], shape=(3,), dtype=float32)</code></pre>
</li>
<li><p><code>w.assign_sub(w要自减的值) </code>，参数自更新</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">w <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
w<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<p><code>&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=int32, numpy=3&gt;</code></p>
</li>
<li><p><code>tf.argmax(张量名, axis=操作轴)</code>，返回张量沿指定维度最大值的索引</p>
</li>
</ul>
<h3 id="2-优化方法"><a href="#2-优化方法" class="headerlink" title="2 优化方法"></a>2 优化方法</h3><h4 id="2-1-一些函数"><a href="#2-1-一些函数" class="headerlink" title="2.1 一些函数"></a>2.1 一些函数</h4><ul>
<li><code>tf.where(条件语句, A, B)</code>，条件为真返回A，条件为假返回B</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> tf<span class="token punctuation">.</span>where<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>greater<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<p><code>tf.Tensor([1, 2, 3, 4, 5], shape=(5,), dtype=int32)</code></p>
</li>
<li><p><code>np.random.RandomState.rand(维度)</code>，返回指定维度的 [0, 1] 的随机数。维度为空则返回一个标量</p>
</li>
<li><p><code>np.vstack(数组1, 数组2)</code>，将两个数组按垂直方向叠加</p>
</li>
<li><p>构建网格坐标点：</p>
<ul>
<li><p><code>np.mgrid[起始值:结束值:步长, 起始值:结束值:步长, ...]</code></p>
</li>
<li><p><code>x.ravel()</code>，把x变为一维数组，把变量x拉直</p>
</li>
<li><p><code>np.c_[数组1, 数组2, ...]</code>，使返回的间隔数值点配对</p>
</li>
</ul>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">x<span class="token punctuation">,</span> y <span class="token operator">=</span> np<span class="token punctuation">.</span>mgrid<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">:</span><span class="token number">0.5</span><span class="token punctuation">]</span>
grid <span class="token operator">=</span> np<span class="token punctuation">.</span>c_<span class="token punctuation">[</span>x<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x:"</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"y:"</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"grid:"</span><span class="token punctuation">,</span> grid<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<pre class="language-markdown" data-language="markdown"><code class="language-markdown">x:[[1. 1. 1. 1.]
   [2. 2. 2. 2.]]
y:[[2.  2.5 3.  3.5]
   [2.  2.5 3.  3.5]]
grid:
 [[1.  2. ]
  [1.  2.5]
  [1.  3. ]
  [1.  3.5]
  [2.  2. ]
  [2.  2.5]
  [2.  3. ]
  [2.  3.5]]</code></pre></li>
</ul>
<h4 id="2-2-指数衰减学习率"><a href="#2-2-指数衰减学习率" class="headerlink" title="2. 2 指数衰减学习率"></a>2. 2 指数衰减学习率</h4><pre class="language-none"><code class="language-none">LR_BASE &#x3D; 0.2
LR_DECAY &#x3D; 0.99
LR_STEP &#x3D; 1
for epoch in range(epoch):
lr &#x3D; LR_BASE * LR_DECAY ** (epoch &#x2F; LR_STEP)</code></pre>

<h4 id="2-3-激活函数"><a href="#2-3-激活函数" class="headerlink" title="2.3 激活函数"></a>2.3 激活函数</h4><ul>
<li><code>tf.nn.sigmoid(x)</code>，sigmoid的导数为 [0, 0.25] 的小数，多层神经网络链式求导时，如果连续相乘会造成梯度消失</li>
<li><code>tf.math.tanh(x)</code>，输出是0均值了，但依然存在梯度消失、幂运算问题</li>
<li><code>tf.nn.relu(x)</code>，解决了梯度消失问题，并且计算速度快；输出非0均值，收敛慢，并且存在dead relu问题<ul>
<li>dead relu是经过relu函数的负数特征过多导致的，可以合理参数初始化、设置小的学习率</li>
</ul>
</li>
<li><code>tf.nn.leaky_relu(x)</code>，解决了dead relu问题</li>
</ul>
<p>建议：首选relu函数；学习率设置较小值；输入特征标准化；初始化参数中心化</p>
<h4 id="2-4-损失函数"><a href="#2-4-损失函数" class="headerlink" title="2.4 损失函数"></a>2.4 损失函数</h4><ul>
<li><p>均方误差mse：$MSE(y_,y) &#x3D; \frac{\sum_{i&#x3D;1}^n(y-y_)^2}{n}$ </p>
<ul>
<li><code>loss_mse = tf.reduce_mean(tf.square(y_ - y))</code></li>
</ul>
</li>
<li><p>交叉熵损失函数ce：$H(y_, y) &#x3D; -\Sigma y_*lny$ </p>
<ul>
<li><code>loss_ce = tf.losses.categorical_crossentropy(y_, y)</code> </li>
<li>softmax与交叉熵结合，输出先进行softmax，再计算y与y_的交叉熵损失函数<ul>
<li><code>tf.nn.softmax_cross_entropy_with_logits(y_, y)</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2-5-缓解过拟合"><a href="#2-5-缓解过拟合" class="headerlink" title="2.5 缓解过拟合"></a>2.5 缓解过拟合</h4><ul>
<li>欠拟合的解决方法：增加模型复杂度，增加特征，增加参数；减小正则化系数</li>
<li>过拟合的解决方法：数据清洗；增大数据集；增大正则化参数</li>
</ul>
<p>正则化：</p>
<ul>
<li><p><code>loss = loss(y与y_)+ REGULARIZER *loss(w)​</code></p>
<ul>
<li><p>$loss_{L1}(w) &#x3D; \sum_i|w_i|$ ，L1正则化会让参数变为0，减少参数数量，降低复杂度</p>
</li>
<li><p>$loss_{L2}(w) &#x3D; \sum_i|w_i^2|$ ，L2正则化会使参数接近0但不为0，降低复杂度</p>
</li>
</ul>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">loss_mse <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>y_train <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">)</span>
loss_regularization <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
loss_regularization<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>l2_loss<span class="token punctuation">(</span>w1<span class="token punctuation">)</span><span class="token punctuation">)</span>
loss_regularizaiton<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>l2_loss<span class="token punctuation">(</span>w2<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
求和，例：
x = tf.constant(([1, 1, 1], [1, 1, 1]))
tf.reduce_sum(x)
>>>6
'''</span>
loss_regularization <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>loss_regularization<span class="token punctuation">)</span>
loss <span class="token operator">=</span> loss_mse <span class="token operator">+</span> <span class="token number">0.03</span> <span class="token operator">*</span> loss_regularization</code></pre>

<h4 id="2-6-优化器"><a href="#2-6-优化器" class="headerlink" title="2.6 优化器"></a>2.6 优化器</h4><p>待优化参数$w$，损失函数$loss$，学习率$lr$，每次迭代一个$batch$，$t$表示当前$batch$迭代的总次数：</p>
<ol>
<li>计算 $t$ 时刻损失函数关于参数的梯度 $g_t &#x3D; \nabla loss &#x3D; \frac{\partial loss}{\partial w_t}$ </li>
<li>计算 $t$ 时刻一阶动量 $m_t$ 和二阶动量 $V_t$ </li>
<li>计算 $t$ 时刻下降梯度 $\eta_t &#x3D; lr \sdot m_t&#x2F;\sqrt{V_t} $ </li>
<li>计算 $t+1$ 时刻参数 $w_{t+1} &#x3D; w_t - \eta_t &#x3D; w_t - lr \sdot m_t &#x2F; \sqrt{V_t}$</li>
</ol>
<h5 id="几种优化器"><a href="#几种优化器" class="headerlink" title="几种优化器"></a>几种优化器</h5><p>SGD：随机梯度下降（无momentum）</p>
<ul>
<li><p>$m_t &#x3D; g_t \qquad V_t &#x3D; 1$ </p>
</li>
<li><p>$w_{t+1} &#x3D; w_t - lr \sdot \frac{\partial loss}{\partial w_t} $</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">w1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>

<p>SGDM：含momentum的SGD</p>
<ul>
<li><p>$m_t &#x3D; \beta \sdot m_{t-1}+(1-\beta)\sdot g_t \qquad V_t &#x3D; 1$ </p>
</li>
<li><p>$w_{t+1} &#x3D; w_t - lr \sdot (\beta \sdot m_{t-1}+(1-\beta)\sdot g_t) $</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">m_w <span class="token operator">=</span> beta <span class="token operator">*</span> m_w <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta<span class="token punctuation">)</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
m_b <span class="token operator">=</span> beta <span class="token operator">*</span> m_b <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta<span class="token punctuation">)</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
w1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> m_w<span class="token punctuation">)</span>
b1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> m_b<span class="token punctuation">)</span></code></pre>

<p>Adagrad：在SGD（无momentum）的基础上增加二阶动量</p>
<ul>
<li><p>$m_t &#x3D; g_t \qquad V_t &#x3D; \sum^t_{\tau&#x3D;1}g_\tau^2$ </p>
</li>
<li><p>$w_{t+1} &#x3D; w_t - lr \sdot g_t&#x2F;(\sqrt{\sum^t_{\tau&#x3D;1}g_\tau^2})$</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">v_w<span class="token punctuation">,</span> v_b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
<span class="token comment"># adagrad</span>
v_w <span class="token operator">+=</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
v_b <span class="token operator">+=</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
w1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_w<span class="token punctuation">)</span><span class="token punctuation">)</span>
b1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_b<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<p>RMSProp：在SGD（无momentum）的基础上增加二阶动量</p>
<ul>
<li><p>$m_t &#x3D; g_t \qquad V_t &#x3D; \beta \sdot V_{t-1}+(1-\beta) \sdot g_t^2$ </p>
</li>
<li><p>$w_{t+1} &#x3D; w_t - lr \sdot g_t&#x2F;(\sqrt{\beta \sdot V_{t-1}+(1-\beta) \sdot g_t^2})$</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">v_w<span class="token punctuation">,</span> v_b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
beta <span class="token operator">=</span> <span class="token number">0.9</span>
<span class="token comment"># rmsprop</span>
v_w <span class="token operator">=</span> beta <span class="token operator">*</span> v_w <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
v_b <span class="token operator">=</span> beta <span class="token operator">*</span> v_b <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
w1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_w<span class="token punctuation">)</span><span class="token punctuation">)</span>
b1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_b<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<p>Adam：同时结合SGDM一阶动量和RMSProp二阶动量</p>
<ul>
<li><p>$m_t &#x3D; \beta_1 \sdot m_{t-1}+(1-\beta_1)\sdot g_t \qquad V_t &#x3D; \beta_2 \sdot V_{step-1}+(1-\beta_2) \sdot g_t^2$ </p>
</li>
<li><p>修正一阶动量的偏差 $\widehat{m_t} &#x3D; \frac{m_t}{1-\beta_1^t}$ </p>
</li>
<li><p>修正二阶动量的偏差 $\widehat{V_t} &#x3D; \frac{V_t}{1-\beta_2^t}$ </p>
</li>
<li><p>$w_{t+1} &#x3D; w_t - lr \sdot \frac{m_t}{1-\beta_1^t}&#x2F;\sqrt{\frac{V_t}{1-\beta_2^t}}$</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">m_w<span class="token punctuation">,</span> m_b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
v_w<span class="token punctuation">,</span> v_b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
beta1<span class="token punctuation">,</span> beta2 <span class="token operator">=</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span>
delta_w<span class="token punctuation">,</span> delta_b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
global_step <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># adam</span>
m_w <span class="token operator">=</span> beta1 <span class="token operator">*</span> m_w <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta1<span class="token punctuation">)</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
m_b <span class="token operator">=</span> beta1 <span class="token operator">*</span> m_b <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta1<span class="token punctuation">)</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
v_w <span class="token operator">=</span> beta2 <span class="token operator">*</span> v_w <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta2<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
v_b <span class="token operator">=</span> beta2 <span class="token operator">*</span> v_b <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta2<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

m_w_correction <span class="token operator">=</span> m_w <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>beta1<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>global_step<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
m_b_correction <span class="token operator">=</span> m_b <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>beta1<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>global_step<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
v_w_correction <span class="token operator">=</span> v_w <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>beta2<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>global_step<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
v_b_correction <span class="token operator">=</span> v_b <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>beta2<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>global_step<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

w1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> m_w_correction <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_w_correction<span class="token punctuation">)</span><span class="token punctuation">)</span>
b1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> m_b_correction <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_b_correction<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>





<h3 id="3-神经网络搭建八股"><a href="#3-神经网络搭建八股" class="headerlink" title="3 神经网络搭建八股"></a>3 神经网络搭建八股</h3><h4 id="3-1-使用Sequential搭建神经网络"><a href="#3-1-使用Sequential搭建神经网络" class="headerlink" title="3.1 使用Sequential搭建神经网络"></a>3.1 使用Sequential搭建神经网络</h4><p>tf.keras搭建神经网络六步法（使用Sequential）：</p>
<ol>
<li><p>import</p>
</li>
<li><p>train, test：划分数据集</p>
</li>
<li><p>model &#x3D; tf.keras.models.Sequential：逐层描述网络结构，前向传播</p>
</li>
<li><p>model.compile：配置训练方法，选择优化器、损失函数、评测指标</p>
</li>
<li><p>model.fit：执行训练过程</p>
</li>
<li><p>model.summary：打印网络的结构和参数统计</p>
</li>
</ol>
<h5 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h5><p><code>model = tf.keras.models.Sequential([网络结构])</code> </p>
<ul>
<li>拉直层：<code>tf.keras.layers.Flatten()</code> </li>
<li>全连接层：<code>tf.keras.layers.Dense(神经元个数, activation=&quot;激活函数&quot;, kernel_regularizer=哪种正则化)</code> <ul>
<li>激活函数：<code>relu</code>, <code>softmax</code>, <code>sigmoid</code>, <code>tanh</code> </li>
<li>正则化：<code>tf.keras.regularizers.l1()</code>, <code>tf.keras.regularizers.l2()</code></li>
</ul>
</li>
<li>卷积层：<code>tf.keras.layers.Conv2D(fitlers=卷积核个数, kernel_size=卷积核尺寸, strides=步长, padding=&quot;valid&quot; or &quot;same&quot;) </code></li>
<li>LSTM层：<code>tf.keras.layers.LSTM()</code></li>
</ul>
<h5 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h5><p><code>model.compile(optimizer=优化器, loss=损失函数, metrics=[&quot;准确率&quot;])</code> </p>
<ul>
<li>optimizer 可选：<ul>
<li><code>&#39;sgd&#39;</code> or <code>tf.keras.optimizers.SGD(lr=学习率, momentum=动量)</code> </li>
<li><code>&#39;adagrad&#39;</code> or <code>tf.keras.optimizers.Adagrad(lr=学习率)</code> </li>
<li><code>&#39;adadelta&#39;</code> or <code>tf.keras.optimizers.Adadelta(lr=学习率)</code> </li>
<li><code>&#39;adam&#39;</code> or <code>tf.keras.optimizers.Adam(lr=学习率, beta_1=0.9, beta_2=0.999)</code></li>
</ul>
</li>
<li>loss 可选： <ul>
<li><code>&#39;mse&#39;</code> or <code>tf.keras.losses.MeanSquaredError()</code> </li>
<li><code>&#39;sparse_categorical_crossentropy&#39;</code> or <code>tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)</code> ，神经网络预测前没有经过概率分布则是True，经过概率分布就是False</li>
</ul>
</li>
<li>metrics 可选：<ul>
<li><code>accuracy</code>：y_ 和 y 都是数值，如 y_&#x3D;[1]  y&#x3D;[1]</li>
<li><code>categorical_accuracy</code>：y_ 和 y 都是独热码，如 y_&#x3D;[0, 1, 0]  y&#x3D;[0.256, 0.695, 0.048]</li>
<li><code>sparse_categorical_accuracy</code>：y_ 是数值，y 是独热码，如 y_&#x3D;[1]  y&#x3D;[0.256, 0.695, 0.048]</li>
</ul>
</li>
</ul>
<h5 id="fit"><a href="#fit" class="headerlink" title="fit"></a>fit</h5><pre class="language-python" data-language="python"><code class="language-python">model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>训练集特征<span class="token punctuation">,</span> 训练集标签<span class="token punctuation">,</span> 
batch_size<span class="token operator">=</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token punctuation">,</span> 
validation_data<span class="token operator">=</span><span class="token punctuation">(</span>测试集特征<span class="token punctuation">,</span> 测试集标签<span class="token punctuation">)</span> <span class="token keyword">or</span> validation_split<span class="token operator">=</span>从训练集划分多少比例给测试集<span class="token punctuation">,</span> validation_freq<span class="token operator">=</span>多少epoch测试一次<span class="token punctuation">)</span></code></pre>

<h5 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h5><p><code>model.summary()</code></p>
<h5 id="举例：鸢尾花识别"><a href="#举例：鸢尾花识别" class="headerlink" title="举例：鸢尾花识别"></a>举例：鸢尾花识别</h5><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 1. import</span>
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># 2. train, test</span>
x_train <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data
y_train <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>target

np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>y_train<span class="token punctuation">)</span>
tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span>set_seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>

<span class="token comment"># 3. Sequential</span>
model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 4. compile</span>
model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 5. fit</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> validation_split<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>

<span class="token comment"># 6. summary</span>
model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>



<h4 id="3-2-使用class搭建神经网络"><a href="#3-2-使用class搭建神经网络" class="headerlink" title="3.2  使用class搭建神经网络"></a>3.2  使用class搭建神经网络</h4><p>Sequential支持搭建上层输入是下层输出的神经网络，如果有跳连，可以用class搭建。</p>
<p>tf.keras搭建神经网络六步法（使用class）：</p>
<ol>
<li><p>import</p>
</li>
<li><p>train, test</p>
</li>
<li><p><strong>class MyModel(Model) model&#x3D;MyModel</strong></p>
</li>
<li><p>model.compile</p>
</li>
<li><p>model.fit</p>
</li>
<li><p>model.summary</p>
</li>
</ol>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token builtin">super</span><span class="token punctuation">(</span>MyModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
		定义网络结构块
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    	调用网络结构块，实现前向传播
    	<span class="token keyword">return</span> y
model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<p>以鸢尾花分类的网络为例：</p>
 <pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 1. import</span>
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras <span class="token keyword">import</span> Model
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># 2. train, test</span>
x_train <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data
y_train <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>target

np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>y_train<span class="token punctuation">)</span>
tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span>set_seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>

<span class="token comment"># 3. class</span>
<span class="token keyword">class</span> <span class="token class-name">IrisModel</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token builtin">super</span><span class="token punctuation">(</span>IrisModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>d1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    	y <span class="token operator">=</span> self<span class="token punctuation">.</span>d1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    	<span class="token keyword">return</span> y
    
model <span class="token operator">=</span> IrisModel<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 4. compile</span>
model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 5. fit</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> validation_split<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>

<span class="token comment"># 6. summary</span>
model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>



<h4 id="3-3-MNIST-数据集"><a href="#3-3-MNIST-数据集" class="headerlink" title="3.3 MNIST 数据集"></a>3.3 MNIST 数据集</h4><p>MNIST数据集有 7 万张 28*28 像素的手写数字，其中 6 万张用于训练，1 万张用于测试。</p>
<h5 id="Sequential-1"><a href="#Sequential-1" class="headerlink" title="Sequential"></a>Sequential</h5><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

mnist <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>mnist
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
x_train<span class="token punctuation">,</span> x_test <span class="token operator">=</span> x_train <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> x_test <span class="token operator">/</span> <span class="token number">255.0</span>  <span class="token comment"># 把[0, 255]变为[0, 1]，输入特征值小更易于神经网络吸收</span>

model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span> 
              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<h5 id="class"><a href="#class" class="headerlink" title="class"></a>class</h5><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense<span class="token punctuation">,</span> Flatten
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras <span class="token keyword">import</span> Model

mnist <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>mnist
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
x_train<span class="token punctuation">,</span> x_test <span class="token operator">=</span> x_train <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> x_test <span class="token operator">/</span> <span class="token number">255.0</span>

<span class="token keyword">class</span> <span class="token class-name">MnistModel</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MnistModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d2 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
	<span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>d2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y

model <span class="token operator">=</span> MnistModel<span class="token punctuation">(</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span> 
              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>





<h3 id="4-八股扩展"><a href="#4-八股扩展" class="headerlink" title="4 八股扩展"></a>4 八股扩展</h3><p>自制数据集、数据增强、断点续训（实时存取模型）、参数提取（把参数存入文本）、acc&#x2F;loss可视化、应用程序。</p>
<h4 id="4-1-自制数据集"><a href="#4-1-自制数据集" class="headerlink" title="4.1 自制数据集"></a>4.1 自制数据集</h4><p>读写文件、建立数据集的操作，详见代码。</p>
<h4 id="4-2-数据增强"><a href="#4-2-数据增强" class="headerlink" title="4.2 数据增强"></a>4.2 数据增强</h4><pre class="language-python" data-language="python"><code class="language-python">image_gen_train <span class="token operator">=</span> ImageDataGenerator<span class="token punctuation">(</span>
	rescale <span class="token operator">=</span> 所有数据将乘以该值<span class="token punctuation">,</span> 
    rotation_range <span class="token operator">=</span> 随机旋转角度<span class="token punctuation">,</span> 
    width_shift_range <span class="token operator">=</span> 随机宽度偏移量<span class="token punctuation">,</span> 
    height_shift_range <span class="token operator">=</span> 随机高度偏移量<span class="token punctuation">,</span> 
    horizontal_flip <span class="token operator">=</span> 是否随机水平翻转<span class="token punctuation">,</span> 
    zoom_range <span class="token operator">=</span> 随即缩放 
<span class="token punctuation">)</span>
image_gen_train<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span></code></pre>

<p>x_train要求是四维数据，需要进行reshape，最后一个是通道数量：<code>x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)</code> </p>
<p>model.fit步骤变为：<code>model.fit(image_gen_train.flow(x_train, y_train, batch_size=32))</code> </p>
<p>数据增强的效果需要在实际应用程序中体会。</p>
<h4 id="4-3-断点续训"><a href="#4-3-断点续训" class="headerlink" title="4.3 断点续训"></a>4.3 断点续训</h4><h5 id="读取模型"><a href="#读取模型" class="headerlink" title="读取模型"></a>读取模型</h5><p><code>load_weights(路径) </code></p>
<p>保存模型时，会自动生成.index索引表文件。如果路径中已经有保存好的模型，就直接加载模型参数：</p>
<pre class="language-python" data-language="python"><code class="language-python">checkpoint_save_path <span class="token operator">=</span> <span class="token string">"./checkpoint/mnist.ckpt"</span>
<span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>checkpoint_save_path <span class="token operator">+</span> <span class="token string">'.index'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'---load the model---'</span><span class="token punctuation">)</span>
	model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>checkpoint_save_path<span class="token punctuation">)</span></code></pre>

<h5 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h5><pre class="language-python" data-language="python"><code class="language-python">cp_callback <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>ModelCheckpoint<span class="token punctuation">(</span>
    filepath<span class="token operator">=</span>checkpoint_save_path<span class="token punctuation">,</span> <span class="token comment"># 文件存储路径</span>
    save_weights_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># 是否只保留模型参数</span>
    save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>	<span class="token comment"># 是否只保留最优结果</span>

history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> 
                   	validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_data<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> 
                    callbacks<span class="token operator">=</span><span class="token punctuation">[</span>cp_callback<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 模型训练时加入 callbacks 选项，记录到 history 中</span></code></pre>



<h4 id="4-4-参数提取"><a href="#4-4-参数提取" class="headerlink" title="4.4 参数提取"></a>4.4 参数提取</h4><p><code>model.trainable_variables</code> 返回模型中可训练参数。</p>
<p><code>np.set_printoptions(threshold=超过多少省略显示)</code> </p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">)</span>
<span class="token builtin">file</span> <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'./weights.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> v <span class="token keyword">in</span> model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">:</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>name<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
<span class="token builtin">file</span><span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>



<h4 id="4-5-acc-x2F-loss-曲线"><a href="#4-5-acc-x2F-loss-曲线" class="headerlink" title="4.5 acc&#x2F;loss 曲线"></a>4.5 acc&#x2F;loss 曲线</h4><p>在 model.fit 执行训练过程的同时，同步记录了：</p>
<ul>
<li>训练集loss：<code>loss </code> </li>
<li>测试集loss：<code>val_loss </code> </li>
<li>训练集准确率：<code>sparse_categorical_accuracy </code> </li>
<li>测试集准确率：<code>val_sparse_categorical_accuracy</code></li>
</ul>
<p>可用 history.history 提取出来。</p>
<pre class="language-python" data-language="python"><code class="language-python">history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> 
                   	validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_data<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> 
                    callbacks<span class="token operator">=</span><span class="token punctuation">[</span>cp_callback<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>

<pre class="language-python" data-language="python"><code class="language-python">acc <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span>
val_acc <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_sparse_categorical_accuracy'</span><span class="token punctuation">]</span>
loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span>
val_loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_loss'</span><span class="token punctuation">]</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>acc<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Training Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>val_acc<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Validation Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training and Validation Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Training Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>val_loss<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Validation Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training and Validation Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>



<h4 id="4-6-模型应用程序"><a href="#4-6-模型应用程序" class="headerlink" title="4.6 模型应用程序"></a>4.6 模型应用程序</h4><p><code>predict(输入特征, batch_size=)</code> 返回前向传播计算结果。</p>
<p>分以下几步：</p>
<ul>
<li>复现模型的前向传播Sequential</li>
<li>加载模型参数</li>
<li>进行预测</li>
</ul>
<p>附手写数字识别任务的代码：</p>
<p><strong>A. 模型训练和保存</strong></p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> os
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt


mnist <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>mnist
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
x_train<span class="token punctuation">,</span> x_test <span class="token operator">=</span> x_train <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> x_test <span class="token operator">/</span> <span class="token number">255.0</span>  <span class="token comment"># 把[0, 255]变为[0, 1]，输入特征值小更易于神经网络吸收</span>

model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>
              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 查看有没有可以加载的模型</span>
checkpoint_save_path <span class="token operator">=</span> <span class="token string">"./checkpoint/mnist.ckpt"</span>
<span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>checkpoint_save_path <span class="token operator">+</span> <span class="token string">'.index'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'-------------load the model-----------------'</span><span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>checkpoint_save_path<span class="token punctuation">)</span>

cp_callback <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>ModelCheckpoint<span class="token punctuation">(</span>filepath<span class="token operator">=</span>checkpoint_save_path<span class="token punctuation">,</span>
                                                 save_weights_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                                 save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>


history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>
          validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
          callbacks<span class="token operator">=</span><span class="token punctuation">[</span>cp_callback<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 通过调用回调函数，保存模型</span>

model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token comment"># 参数写入txt</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">)</span>
<span class="token builtin">file</span> <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'./weights.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> v <span class="token keyword">in</span> model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">:</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>name<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
<span class="token builtin">file</span><span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token comment"># acc/loss可视化</span>
acc <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span>
val_acc <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_sparse_categorical_accuracy'</span><span class="token punctuation">]</span>
loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span>
val_loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_loss'</span><span class="token punctuation">]</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>acc<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Training Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>val_acc<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Validation Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training and Validation Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Training Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>val_loss<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Validation Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training and Validation Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>



<p><strong>B. 模型加载和预测</strong></p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

model_save_path <span class="token operator">=</span> <span class="token string">'./checkpoint/mnist.ckpt'</span>

<span class="token comment"># 复现前向传播</span>
model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 加载模型</span>
model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>model_save_path<span class="token punctuation">)</span>


preNum <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">(</span><span class="token string">"执行几次识别任务:"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>preNum<span class="token punctuation">)</span><span class="token punctuation">:</span>
    image_path <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">(</span><span class="token string">"图片文件名:"</span><span class="token punctuation">)</span>
    img <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>image_path<span class="token punctuation">)</span>
    img <span class="token operator">=</span> img<span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Image<span class="token punctuation">.</span>ANTIALIAS<span class="token punctuation">)</span>  <span class="token comment"># 转化为28*28</span>
    img_arr <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>img<span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'L'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 转化为灰度图片</span>

    <span class="token comment"># 白底黑字，转换成黑底白字，并降噪</span>
    <span class="token comment"># img_arr = 255 - img_arr</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> img_arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">200</span><span class="token punctuation">:</span>
                img_arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">255</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                img_arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>

    img_arr <span class="token operator">=</span> img_arr <span class="token operator">/</span> <span class="token number">255.0</span>  <span class="token comment"># 归一化</span>

    <span class="token comment"># 神经网络训练时，都是按照batch送入网络，所以给img_arr前面添加一个维度</span>
    <span class="token comment"># img_arr:(28, 28)</span>
    <span class="token comment"># x_predict:(1, 28, 28)</span>
    x_predict <span class="token operator">=</span> img_arr<span class="token punctuation">[</span>tf<span class="token punctuation">.</span>newaxis<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>

    result <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_predict<span class="token punctuation">)</span>   <span class="token comment"># 前向传播</span>
    pred <span class="token operator">=</span> tf<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>result<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># 输出最大概率值的索引</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>
    tf<span class="token punctuation">.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"预测数字："</span><span class="token punctuation">,</span> pred<span class="token punctuation">,</span> <span class="token string">"\n"</span><span class="token punctuation">)</span></code></pre>





<h3 id="5-卷积神经网络"><a href="#5-卷积神经网络" class="headerlink" title="5 卷积神经网络"></a>5 卷积神经网络</h3><p>实际问题中，图片尺寸大、通道多，直接输入全连接层不现实，需要进行特征的提取，再输入到全连接层。卷积计算是提取特征的常用、有效的方法。</p>
<h4 id="5-1-卷积概念"><a href="#5-1-卷积概念" class="headerlink" title="5.1 卷积概念"></a>5.1 卷积概念</h4><h5 id="卷积计算"><a href="#卷积计算" class="headerlink" title="卷积计算"></a>卷积计算</h5><p>输入特征图的channel数决定了卷积核的channel数；卷积核的个数决定输出特征图的channel数。</p>
<p>如果觉得某层特征提取能力差，可以在这一层多用几个卷积核。</p>
<p>可以根据下图记住参数的个数。每个格子是一个w，每个卷积核有一个b。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf3.png'  width="80%" height="80%"/ loading="lazy">

<p>当卷积核是多channel的，也是执行全部相乘、求和的计算过程。</p>
<p>当有多个卷积核，把每个卷积核得到的特征图叠加起来，形成多channel的输出特征图。</p>
<h5 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h5><p>感受野的概念如图：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf4.png'  width="80%" height="80%"/ loading="lazy">

<p>用两层 3*3 卷积核，和一层 5*5 卷积核，最终得到的单个输出像素的感受野都是 5，我们说<strong>这两种卷积方法的特征提取能力是一样的</strong>。</p>
<p>此时，考虑它们的带训练参数个数和计算量，在不同的卷积方法中进行选择：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf5.png'  width="80%" height="80%"/ loading="lazy">

<p>这也是现在神经网络经常使用两层 3*3 卷积核替换一层 5*5 卷积核的原因。</p>
<h4 id="5-2-tensorflow卷积层的实现"><a href="#5-2-tensorflow卷积层的实现" class="headerlink" title="5.2 tensorflow卷积层的实现"></a>5.2 tensorflow卷积层的实现</h4><h5 id="A-padading-全零填充"><a href="#A-padading-全零填充" class="headerlink" title="A. padading-全零填充"></a>A. padading-全零填充</h5><p>全零填充在输入图片周围补0，使卷积计算的输出保持跟输入相同的尺寸。</p>
<ul>
<li><p><code>padding=&#39;SAME&#39;</code>：使用全零填充</p>
</li>
<li><p><code>padding=&#39;VALID&#39;</code>：不使用全零填充</p>
</li>
</ul>
<h5 id="B-Conv2D-描述卷积层"><a href="#B-Conv2D-描述卷积层" class="headerlink" title="B. Conv2D-描述卷积层"></a>B. Conv2D-描述卷积层</h5><pre class="language-python" data-language="python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>
    filters<span class="token operator">=</span>卷积核个数
    kernel_size<span class="token operator">=</span>卷积核尺寸<span class="token punctuation">,</span> 
    strides<span class="token operator">=</span>滑动步长<span class="token punctuation">,</span> 
    padding<span class="token operator">=</span><span class="token string">"same"</span> <span class="token keyword">or</span> <span class="token string">"valid"</span><span class="token punctuation">,</span> 
    activation<span class="token operator">=</span><span class="token string">"relu"</span> <span class="token keyword">or</span> <span class="token string">"softmax"</span> <span class="token keyword">or</span> <span class="token string">"tanh"</span> <span class="token keyword">or</span> <span class="token string">"sigmoid"</span><span class="token punctuation">,</span>  <span class="token comment"># 如果有BN，此处不写</span>
    input_shape<span class="token operator">=</span><span class="token punctuation">(</span>高<span class="token punctuation">,</span> 宽<span class="token punctuation">,</span> 通道数<span class="token punctuation">)</span> <span class="token comment"># 输入特征图的维度，可以省略</span>
<span class="token punctuation">)</span></code></pre>

<p>kernel_size、strides等，如果横纵相同，就写一个长整数；否则写 <code>(纵向, 横向)</code> </p>
<p>卷积层三种不同的传参方法：</p>
<pre class="language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
    Conv2D<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    MaxPool2D<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    
    Conv2D<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
    MaxPool2D<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    
	Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
    MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    
    Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>



<h5 id="C-BN-批标准化"><a href="#C-BN-批标准化" class="headerlink" title="C. BN-批标准化"></a>C. BN-批标准化</h5><p>神经网络对0附近的数据更敏感，梯度更大。</p>
<ul>
<li>标准化：使数据符合0均值，1标准差的分布</li>
<li>批标准化：对一batch的数据做标准化处理</li>
</ul>
<p>$$ {H_i^k}^\prime  &#x3D; \frac{H_i^k - \mu^k_{batch}}{\sigma^k_{batch}} $$</p>
<p><strong>BN常用在卷积操作和激活操作之间。</strong> </p>
<p>均值、标准差都是针对第 k 个卷积核生成的 batch 张（同channel）图中的所有像素点。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf6.png'  width="80%" height="80%"/ loading="lazy">

<p>这种简单的标准化，使输入数据集中在激活函数的线性区域。</p>
<p>因此在BN操作中，为每个像素 $H$ 引入<strong>两个可训练参数</strong>，缩放因子 $\gamma$ 和偏移因子 $\beta$，标准正态分布后的输入数据通过这两个因子，优化了特征数据分布的宽窄、偏移量（如图），<strong>保证了网络的非线性表达力</strong>。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf7.png'  width="80%" height="80%"/ loading="lazy">

<pre class="language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
	Conv2D<span class="token punctuation">(</span>fitlers<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>



<h5 id="D-Pooling-池化"><a href="#D-Pooling-池化" class="headerlink" title="D. Pooling-池化"></a>D. Pooling-池化</h5><p>用于减少特征的数量。</p>
<ul>
<li>最大池化：提取图片纹理</li>
<li>均值池化：保留背景特征</li>
</ul>
<p><code>padding=&#39;same&#39;</code> 时，<strong>不可整除的会填充成可整除，而不是不改变输出的尺寸</strong> </p>
<pre class="language-python" data-language="python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>MaxPool2D<span class="token punctuation">(</span>
	pool_size<span class="token operator">=</span>池化核尺寸<span class="token punctuation">,</span> 
    strides<span class="token operator">=</span>池化步长<span class="token punctuation">,</span>  <span class="token comment"># 默认跟 pool_size 相等</span>
    padding<span class="token operator">=</span><span class="token string">'valid'</span> <span class="token keyword">or</span> <span class="token string">'same'</span> <span class="token comment"># 是否使用全零填充的池化</span>
<span class="token punctuation">)</span>
tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>AveragePooling2D<span class="token punctuation">(</span>
	pool_size<span class="token operator">=</span>池化核尺寸<span class="token punctuation">,</span> 
    strides<span class="token operator">=</span>池化步长<span class="token punctuation">,</span> 
    padding<span class="token operator">=</span><span class="token string">'valid'</span> <span class="token keyword">or</span> <span class="token string">'same'</span> 
<span class="token punctuation">)</span></code></pre>

<p>例子见之前的代码。 </p>
<h5 id="E-Dropout-舍弃"><a href="#E-Dropout-舍弃" class="headerlink" title="E. Dropout-舍弃"></a>E. Dropout-舍弃</h5><p>为了防止过拟合，按照一定概率，把一部分神经元从社交网络中暂时舍弃。使用神经网络时，被舍弃的神经元恢复连接。</p>
<p><code>tf.keras.layers.Dropout(舍弃概率)</code> </p>
<p>例子见之前的代码。 </p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>卷积神经网络：借助卷积核提取特征，然后送入全连接网络。</p>
<p>特征提取分四步：</p>
<ul>
<li>Conv2D</li>
<li>BN</li>
<li>Activation</li>
<li>Pooling</li>
</ul>
<p>Q：卷积是什么？</p>
<p>A：卷积就是特征提取器，就是CBAPD。</p>
<pre class="language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
	Conv2D<span class="token punctuation">(</span>fitlers<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>



<h4 id="5-3-卷积神经网络搭建示例"><a href="#5-3-卷积神经网络搭建示例" class="headerlink" title="5.3 卷积神经网络搭建示例"></a>5.3 卷积神经网络搭建示例</h4><h5 id="Cifar-10-数据集"><a href="#Cifar-10-数据集" class="headerlink" title="Cifar 10 数据集"></a>Cifar 10 数据集</h5><ul>
<li>5万张 32*32 像素的 10 分类彩色图片和标签，作为训练集 </li>
<li>1万张 32*32 像素的 10 分类彩色图片和标签，作为训练集</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">cifar10 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>cifar10
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> cifar10<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<h5 id="搭建示例"><a href="#搭建示例" class="headerlink" title="搭建示例"></a>搭建示例</h5><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Baseline</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Baseline<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b1 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a1 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p1 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d1 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d2 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f2 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>f2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y</code></pre>



<h4 id="5-4-LeNet"><a href="#5-4-LeNet" class="headerlink" title="5.4 LeNet"></a>5.4 LeNet</h4><p><strong>在统计卷积神经网络层数时，一般只统计卷积计算层和全连接计算层。</strong> </p>
<p>LeNet的五层结构如图：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf8.png'  width="80%" height="80%"/ loading="lazy">

<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LeNet5</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LeNet5<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p1 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c2 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p2 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f2 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f3 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>f3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y</code></pre>



<h4 id="5-5-AlexNet"><a href="#5-5-AlexNet" class="headerlink" title="5.5 AlexNet"></a>5.5 AlexNet</h4><p>AlexNet 的八层结构：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf9.png'  width="80%" height="80%"/ loading="lazy">

<p>（图中有些数不准确，详见代码。）</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf10.png'  width="80%" height="80%"/ loading="lazy">

<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">AlexNet8</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>AlexNet8<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b1 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a1 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p1 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c2 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b2 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a2 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p2 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c3 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>\
                         activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c4 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>\
                         activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c5 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>\
                         activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p3 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>f1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d1 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>f2 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d2 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>f3 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>f3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y</code></pre>



<h4 id="5-6-VGGNet"><a href="#5-6-VGGNet" class="headerlink" title="5.6 VGGNet"></a>5.6 VGGNet</h4><p>使用小尺寸卷积核，在减少参数的同时提高了识别准确率。</p>
<p>VGG网络结构规整，适合硬件加速。</p>
<p>（图中有些数不准确，详见代码。）</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf11.png'  width="80%" height="80%"/ loading="lazy">

<p>卷积核个数 64 –&gt; 128 –&gt; 256 –&gt; 512，因为越靠后，特征图尺寸越小，通过增加通道，增加了特征图的深度，保持了信息的承载能力。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">VGG16</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>VGG16<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b1 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a1 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c2 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b2 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a2 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p1 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>， strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d1 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c3 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b3 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a3 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c4 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b4 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a4 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p2 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>， strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d2 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c5 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b5 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a5 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c6 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b6 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a6 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c7 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b7 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a7 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p3 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>， strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d3 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c8 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b8 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a8 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c9 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b9 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a9 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c10 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b10 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a10 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p4 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>， strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d4 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c11 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b11 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a11 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c12 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b12 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a12 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c13 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b13 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a13 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p5 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>， strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d5 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d6 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>f2 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d7 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>f3 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c6<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b6<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a6<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c7<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b7<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a7<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c8<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b8<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a8<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c9<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b9<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a9<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c10<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b10<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a10<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c11<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b11<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a11<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c12<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b12<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a12<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c13<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b13<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a13<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d6<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d7<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        y <span class="token operator">=</span> self<span class="token punctuation">.</span>f3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y</code></pre>



<h4 id="5-7-Inception-Net"><a href="#5-7-Inception-Net" class="headerlink" title="5.7 Inception Net"></a>5.7 Inception Net</h4><p>在同一层网络内使用不同尺寸的卷积核，提升了模型感知力。</p>
<p>单个Inception结构块如图：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf12.png'  width="80%" height="80%"/ loading="lazy">

<p>卷积连接器Filter concatenation将四个分支按深度方向堆叠在一起。</p>
<p>由于卷积操作都是 C-B-A 的结构，将这种三步卷积操作定义为一个类，减少代码长度：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ConvBNRelu</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ConvBNRelu<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
            Conv2D<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token punctuation">,</span> strides<span class="token operator">=</span>strides<span class="token punctuation">,</span> padding<span class="token operator">=</span>padding<span class="token punctuation">)</span><span class="token punctuation">,</span> 
            BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
            Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span>
	<span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x</code></pre>

<p>然后，Inception结构块可以这样实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">InceptionBlk</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ch<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>InceptionBlk<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ch <span class="token operator">=</span> ch
        self<span class="token punctuation">.</span>strides <span class="token operator">=</span> strides
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> strides<span class="token operator">=</span>strides<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c2_1 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> strides<span class="token operator">=</span>strides<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c2_2 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c3_1 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> strides<span class="token operator">=</span>strides<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c3_2 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>p4_1 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c4_2 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> strides<span class="token operator">=</span>strides<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x2_1 <span class="token operator">=</span> self<span class="token punctuation">.</span>c2_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x2_2 <span class="token operator">=</span> self<span class="token punctuation">.</span>c2_2<span class="token punctuation">(</span>x2_1<span class="token punctuation">)</span>
        x3_1 <span class="token operator">=</span> self<span class="token punctuation">.</span>c3_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x3_2 <span class="token operator">=</span> self<span class="token punctuation">.</span>c3_2<span class="token punctuation">(</span>x3_1<span class="token punctuation">)</span>
        x4_1 <span class="token operator">=</span> self<span class="token punctuation">.</span>p4_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x4_2 <span class="token operator">=</span> self<span class="token punctuation">.</span>c4_2<span class="token punctuation">(</span>x4_1<span class="token punctuation">)</span>
        <span class="token comment"># 堆叠不同分支的输出</span>
        x <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2_2<span class="token punctuation">,</span> x3_2<span class="token punctuation">,</span> x4_2<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x</code></pre>



<p>精简InceptionNet：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf13.png'  width="80%" height="80%"/ loading="lazy">

<p>每个 block 由两个 InceptionBlk 组成，第一个的卷积 strides&#x3D;1，第二个的 strides&#x3D;2.</p>
<p>这使 block_0 的输出特征图尺寸减半，因此我们把输出特征图深度加深为2倍，保持信息承载能力。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Inception10</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_blocks<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> init_ch<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Inception10<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> init_ch
        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> init_ch
        self<span class="token punctuation">.</span>num_blocks <span class="token operator">=</span> num_blocks
        self<span class="token punctuation">.</span>init_ch <span class="token operator">=</span> init_ch
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>init_ch<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>blocks <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> block_id <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">for</span> layer_id <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> layer_id <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    block <span class="token operator">=</span> InceptionBlk<span class="token punctuation">(</span>self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    block <span class="token operator">=</span> InceptionBlk<span class="token punctuation">(</span>self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>blocks<span class="token punctuation">.</span>add<span class="token punctuation">(</span>block<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>out_channels <span class="token operator">*=</span> <span class="token number">2</span>
        self<span class="token punctuation">.</span>p1 <span class="token operator">=</span> GlobalAveragePooling2D<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>blocks<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>f1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y

   model <span class="token operator">=</span> Inception10<span class="token punctuation">(</span>num_blocks<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span></code></pre>



<h4 id="5-8-ResNet"><a href="#5-8-ResNet" class="headerlink" title="5.8 ResNet"></a>5.8 ResNet</h4><p>提出层间残差跳连，引入前方信息，缓解梯度消失，可以让网络层数增加。</p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>miu</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://acbgzm.github.io/2021/05/27/dlang/" title="dl &amp; ai - andrew ng">http://acbgzm.github.io/2021/05/27/dlang/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2022/01/26/misc-1/" rel="prev" title="博客里的我，堂堂复活"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">博客里的我，堂堂复活</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2021/03/25/ml2014ang/" rel="next" title="machine learning - andrew ng"><span class="post-nav-text">machine learning - andrew ng</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"><style>.utterances {
  max-width: 100%;
}</style><script src="https://utteranc.es/client.js" repo="ACBGZM/ACBGZM.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> miu</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:search-line"></span></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script src="https://fastly.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js"></script><script src="/js/search/local-search.js" defer type="module"></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><span class="icon iconify" data-icon="ri:close-line"></span></span></div><div class="search-input-container"><input class="search-input" id="local-search-input" type="text" placeholder="搜索..." value=""></div><div class="search-result-container"></div></div></body></html>