<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>去年写的秋招总结（含个人博客特典）</title>
    <url>/2023/03/28/job-2/</url>
    <content><![CDATA[<p>本文原文发布于牛客，点击链接：<a href="https://www.nowcoder.com/discuss/428252230693683200">23届游戏开发&#x2F;游戏客户端开发岗位个人秋招全记录，面经+总结</a></p>
<p>今年毕业，去年秋招。为了总结整理一下，当然也是为了回馈一下牛客社区，我就写了一篇比较长、比较全的面经。文章发出后各种数据一直在稳定增长，直到今天，我都在回复各种私信、微信之类的。希望真的对一些人有所帮助吧。</p>
<p>毕竟是为了发出来，在写的时候我特地在避免我特有的意义不明的长篇大论（虽然到最后还是写了很多字），就把一些比较个人的废话和其他行业的面经都删掉了。在博客里就把这些有的没的当作特典加上吧。</p>
<h4 id="〇-个人博客特典"><a href="#〇-个人博客特典" class="headerlink" title="〇 个人博客特典"></a>〇 个人博客特典</h4><h5 id="运气"><a href="#运气" class="headerlink" title="运气"></a>运气</h5><p>面经无非就是记录一下公司的面试流程和问的问题，让后面的人做一下心理或者知识上的准备。每个人在准备秋招时，当然也都是朝着多看新东西、多学新知识的方向上努力。</p>
<p>但不得不说，我觉得对于我等大部分普通人，校招还是运气成分比较多，主要就是看行业今年能容纳多少人吧。知识上的准备是准备不完的，大部分候选人在公司眼里也大差不差。投递时间合不合适、捞的组招不招人、问的问题会不会、面试官心情好不好，都不是个人能决定的。我们能做的也就只有多投多面，多碰碰运气了。</p>
<p>我觉得有这样的心态很重要，但我并没在面经里写这个，因为写了也没什么用，而且准备的时候还是专注于知识比较好。但好像一些人会把某些公司的面试看得特别重，不知道这样的心态是好还是不好。</p>
<h5 id="怎么说呢。。"><a href="#怎么说呢。。" class="headerlink" title="怎么说呢。。"></a>怎么说呢。。</h5><p>再次不得不说，虽然行业和环境都不景气，但真正的精英肯定还是不愁找工作的。就比如实习群里同样没转正的清华哥、交大天才少年，人家找工作当然是比较方便的，腾讯网易米哈游再不行都能把他们招进去。也不用说名校本科生了，很多18岁小伙都比我25岁老同志厉害很多，各种竞赛奖项、理论知识、实践经历看得我汗颜。这段时间我也是涨了不少见识。</p>
<p>对于这些人我想说，你们来找我问秋招这么难该怎么办，就像三叔跟张顺飞学刷野，是想问出点啥？我也就是普通人帮普通人，记下来的问题大多可以速成，不要太当真好嘛？有在名校苦读打下的基础，或者有融入了热情和梦想的游戏项目，大概也不会经历各种小公司的奇怪面试。而且还有些年轻的天才开始担心起一些有的没的。怎么说呢，让我产生了一丝负罪感……</p>
<p>如果能选的话，我还是想以技术大牛的身份跟这些精英交流，作为睿智的前辈传授给他们一些知识和经验——而不是像各种动画里除了年功序列啥都没有的老头，一张嘴就是“我吃的盐比你们吃的饭还多！”……</p>
<p>搞得我现在看到自己的面经都觉得有点羞耻了。在个人博客里犯犯病吧。</p>
<h5 id="迪子？华子？"><a href="#迪子？华子？" class="headerlink" title="迪子？华子？"></a>迪子？华子？</h5><p>我第一个拿到的offer其实是比亚迪。刚开始准备秋招的第一天，辅导员在群里发了比亚迪的信息，当天晚上我把信息填好投递，两天后就二面完了，基本上也没问什么有意义的问题，流程很快就结束了，也没有线上查流程的地方，总感觉有点草率。对于迪子，都没听说投了的同学有没拿offer的，让人不禁想问“到底要招多少人？”。我面的是用Unity做车模渲染的部门，感觉也还不错，但后面还是觉得有点太快，薪资也是大锅饭，并且当然想往游戏行业努力一下，所以就拒绝掉了。</p>
<p>据别人说，6、7年前的华为也是这么招人的，是很多人的保底之底中底。即使是在我本科毕业的2020年，华为也算不上是很难进的公司。但现在，华子已经是我们这个层次高攀不起的存在了，面试每次都是养大鱼，池子一泡两个月。现在的华子不会就是过几年的迪子吧？</p>
<p>不过在2023年，迪子能提供这么多的工作岗位，respect。</p>
<p>从未如此美妙的开局！今年的迪子确实让我产生了莫名的信心，但后面一个多月屡战屡败的日子就显得更难受了一点。</p>
<h5 id="国企：中车四方所（中车制动）"><a href="#国企：中车四方所（中车制动）" class="headerlink" title="国企：中车四方所（中车制动）"></a>国企：中车四方所（中车制动）</h5><p>中车四方所是青岛比较有名的国企了，也是我投的为数不多的国企。面试流程比较古板，一面是一大堆人一起等、按顺序叫人，每个人10m左右。然后几位面试官如果有兴趣就私下联系二面。</p>
<p>青岛的企业，像海尔、海信我投了都没动静，说实话自信心有点被打击到了。四方所一面的那天我有四场面试，本来想鸽了的，但还是腾出点时间去做了个自我介绍。第二天，有个大叔加了我微信，联系了二面。</p>
<p>二面是用微信开了个视频聊天，一共聊了95分钟。过程以我讲为主，性质有点像给其他领域的老同志做工作介绍，顺便介绍一些计算机技术概念，算是比较轻松。在最后的拉家常环节，我们竟然发现，面试官的小孩跟我是高中同一级不同班的同学，只不过他的小孩高中就去阿美了，所以并不认识。（不知为啥，在这个环节之前，面试官说他一直以为我是南方人。）</p>
<p>怎么说呢，这场面试对我来说真的挺治愈的。在经过一个来月游戏行业的锤炼后，这位面试官让我觉得其实还是有一些欣赏我的人的，我的各种知识也是有用武之地的（至少表现出来的是这意思）。</p>
<p>最后面试官问：我总有种感觉，如果给你offer，是不是也不一定来？我说确实是这样的，虽然屡屡受挫，但我还是想往自己喜欢的方向发展一下，找工作也更多在争取南方大城市的机会。面试官赞同了我的想法，建议我出去打拼，并鼓励我对自己的能力拿出信心，也聊了一点兴趣和未来发展之类的事情。总之最后的结果是如果决定要回青岛工作，就在毕业前联系他就好。</p>
<p>中车四方所的风评并不好，有伺候领导、7×24 on call、减薪之类的说法。中车制动不知道是什么情况。但我觉得，进入一个能让自己得到别人认可的环境，回到家人身边，也是一件很好的事情。但咋说呢，在青岛我想找一个游戏开发工作已经是不可能了，找一个各方面都不错的企业也是相当难，更不要说老家县城了。</p>
<p>在11月底，我还跟这位面试官交流了几句技术问题。后面我说准备去杭州工作了，他说：“杭州真好，dream city for innovation！”。</p>
<h5 id="初创：分岔点VR"><a href="#初创：分岔点VR" class="headerlink" title="初创：分岔点VR"></a>初创：分岔点VR</h5><p>最后值得记录的是一个初创公司的面试经历。牛客、脉脉上都有招我面试的hr的信息，一、二面的流程也非常专业，面试前都是邮件提醒+飞书，面试强度也不低，都是1.5h带刷题。</p>
<p>知道是小公司，但没想到是初创。二面才知道，公司目前一共就5个人，其中hr和两面的面试官就是其中三位了。二面面试官就是老板，是抖音早期推荐系统的开发人员，年纪轻轻就开始追求自己的梦想了。羡慕啊，羡慕。</p>
<p>两面的刷题我都做得不算好，都是常规题，但以我的水平确实要么用的低级方法，要么写了很长时间才好。其他的，感觉跟面试官也聊不到很一起去，VR、Unity我接触的并不多。而且，毕竟是初创，如果让我来当老板，招的每一个人都是要跟团队的各方面都契合的。二面后过几天，hr就说我跟当前的团队要求并不符合。</p>
<p>如果去大公司，当然平台更大，接受的培训也更好，对职业生涯更有利。但去小团队，一方面能接触到更完整的业务，另一方面好像也有了一飞冲天的可能吧。</p>
<h5 id="完"><a href="#完" class="headerlink" title="完"></a>完</h5><p>博客特典就写这些吧。又变得更像是我在随意记录生活了。</p>
<p>以下原文。</p>
<hr>
<h4 id="一-关于本文"><a href="#一-关于本文" class="headerlink" title="一 关于本文"></a>一 关于本文</h4><p>一方面，我觉得我赶上了中国游戏行业的变革期。产品开始追求品质，重度手游越来越多，赚到钱的也不少；技术上，3D、大世界、虚幻引擎都是这几年的热点，看上去是处在各公司进行技术积累或者抢占下一个爆款的关键时期。近期有新消息的像『百面千相』、『无限暖暖』、『王者荣耀世界』，看了确实让人觉得游戏行业前景大好、作为一名校招生进入游戏行业大有可为。</p>
<p>但另一方面，在环境和各种政策加持下，今年的秋招确实是寒气逼人，老实说我也没想到工作会这么难找。</p>
<p>我是去年暑假决定以后要找游戏开发工作的，今年的秋招也是瞄准了游戏公司来投。我投递了几乎所有开了秋招的游戏公司的游戏客户端开发岗位，以及一些非游戏行业的UE4&#x2F;U3D开发岗位。结局是我没签游戏客户端开发的工作，因为确实没拿到相对满意的项目或者公司的客户端开发offer。从游戏开发&#x2F;游戏客户端开发岗位来说确实是失败了。</p>
<p>这篇文章算是一个普通人的游戏开发岗秋招实录，我会把个人准备情况、面经、各公司的面试体验和考察重点都记录下来，也会写一些针对秋招的反思、经验教训之类的，大家当作失败经验来看就好。</p>
<p>祝大家都能找到理想的工作！有任何问题欢迎留言或者私信。</p>
<h4 id="二-个人秋招情况总结"><a href="#二-个人秋招情况总结" class="headerlink" title="二 个人秋招情况总结"></a>二 个人秋招情况总结</h4><p>个人背景：双非本末流985硕，科班，几个简单项目（两个Unity 2D小游戏，一个腾讯游戏公开课UE4蓝图项目），一段大厂暑期实习。</p>
<p>个人知识：基本的八股，一些UE4引擎源码知识，入门的图形学知识。比较薄弱的是做题，笔试难的就做不出来了，面试手撕基本上也是现场思考。</p>
<p>在今年我的简历应该是勉强够看的水平。项目、学历没什么亮点，实习虽然是客户端开发岗，但做的是引擎开发、性能优化相关的工作，另一个薄弱的地方就是我从头到尾都没有比较深入的gameplay开发经验。</p>
<p>我实习结束、开始准备秋招是9月初，简历投递集中在9&#x2F;10、9&#x2F;16、10&#x2F;7这几个时间左右。投得比较晚，没有赶上任何一家公司的提前批，错过了大部分公司的第一批。</p>
<p>总得来说，我基本上是逮着游戏客户端开发岗位投（公司之间技术栈有所不同），也投了几个非游戏公司的UE4&#x2F;U3D开发岗位。约60家，情况总结在下面：</p>
<p>1.未转正（1个游戏厂）：</p>
<p>​    ①腾讯天美</p>
<p>2.没进面（39家公司，34个游戏厂）：</p>
<p>​    ①简历挂：4399，鹰角网络，Garena，海尔（UE4），散爆</p>
<p>​    ②投了没动静：搜狐畅游，阿里巴巴灵犀互娱，哔哩哔哩，龙图游戏，云畅游戏，飞鱼科技，椰岛游戏，盛趣，迷你创想，索尼中国（VR&#x2F;AR）</p>
<p>​    ③投递后联系说必须实习：电魂网络，心动</p>
<p>​    ④笔试挂：网易雷火，吉比特&#x2F;雷霆游戏，91Act，西山居，百田&#x2F;百奥，友塔游戏，米哈游</p>
<p>​    ⑤笔试后没动静：多益，巨人，funplus，英雄游戏，竞技世界，快手游戏，勇仕，梦加网络，高通中国（图形学&#x2F;游戏），Unity中国</p>
<p>​    ⑥笔试后筛选挂&#x2F;不匹配&#x2F;不合适：完美世界（还是我难得ak了的），博乐科技，乐狗游戏，途游游戏，小米</p>
<p>3.进面挂（14家公司，10个游戏厂）：</p>
<p>​    ①一面挂：tap4fun，红海无限，极致游戏</p>
<p>​    ②一面完没进展：SNK中国，永航科技，联想研究院（unity）</p>
<p>​    ③二面挂：诗悦网络，字节跳动朝夕光年，剑心互娱，分岔点（VR），酷家乐（渲染开发）</p>
<p>​    ④二面完没进展：祖龙游戏，柠檬微趣</p>
<p>​    ⑤三面挂：中国系统（UE4）</p>
<p>4.全面完（3家公司，还有一个最后去的不写了）：</p>
<p>​    ①通知通过：盖娅互娱（谈薪后第二天说hc调整，不发offer了）</p>
<p>​    ②收到offer：数字天空</p>
<p>5.流程中主动放弃（9个游戏厂）：</p>
<p>​    ①笔试放弃：腾讯凯博（冲突没做），掌趣游戏（10月下，不想做了），冰川网络（不想去），沐瞳游戏（10月下，不想做了），乐元素（11月中，拿到其他offer了）</p>
<p>​    ②一面放弃：库洛游戏（11月，拿到其他offer了），yahaha studios（不是合适岗位）</p>
<p>​    ③hr面放弃：游族（听说必须实习），360游戏（听说必须实习）</p>
<p>直到国庆假期期间，我一共面试了不到10场。为了防止失业，也投了其他行业的一些工作岗位。大部分是C++开发，没备注的就是没动静的，也有懒得记录了的：</p>
<p>​    ①恒生（笔试放弃），同花顺（二面放弃，总是大晚上面），比亚迪（offer，太早了拒了），中车四方车辆研究所（算是口头给offer了），新华三H3C（笔试放弃），阅文集团（笔试放弃）</p>
<p>​    ②民生科技，中电科南湖研究所，云鲸智能，导远电子，海信，智已汽车</p>
<p>所有厂加起来一共面了44场。其中，大家普遍印象中的游戏公司应该是15家进面、5家收到hr面通知（放弃了2家）、2家offer，一共面了29场。</p>
<p>整个流程回忆下来，10&#x2F;1之前面试比较少，基本上都是笔试和测评。面试都集中在国庆假期后的两周内，10&#x2F;10-10&#x2F;14、10&#x2F;17-10&#x2F;21这两周我分别是16面、12面，天天赶场子。这两周之后我就比较累了，剩下的流程就全都推了。</p>
<p>到了10月底就没有流程了，11月中旬签好了三方，秋招就大结局了。</p>
<h4 id="三-面经总结"><a href="#三-面经总结" class="headerlink" title="三 面经总结"></a>三 面经总结</h4><p>我没有给面试录音或者及时记录的习惯，就凭回忆在这里整理一下，如果想起来更多会更新。</p>
<p>很多贴子记录了具体公司的面试题，大家可以按需搜索查看。我觉得我按照知识点把个人被问到的问题记录一下比较好。</p>
<p>可以保证下面都是我在面试中被问到的问题。</p>
<h5 id="3-1-C-八股"><a href="#3-1-C-八股" class="headerlink" title="3.1 C++八股"></a>3.1 C++八股</h5><p>1.内存</p>
<p>​    ①内存分区</p>
<p>​        几个区域分别存放什么</p>
<p>​        堆和栈的区别</p>
<p>​        static变量在哪里</p>
<p>​        什么const在常量区，什么const在栈区，什么const放入符号表优化</p>
<p>​        虚函数表既然希望类的所有对象共享为什么不放在全局区</p>
<p>​    ②几个基本数据类型占多少空间</p>
<p>​    ③内存对齐</p>
<p>​        为什么要内存对齐</p>
<p>​        自定义类型的内存对齐规则，举例子算占多少内存空间</p>
<p>​        什么时候不希望进行内存对齐</p>
<p>​        #pragma pack</p>
<p>2.多态</p>
<p>​    ①虚函数</p>
<p>​        构造&#x2F;析构函数能否是虚函数</p>
<p>​        构造&#x2F;析构函数里能否调用虚函数</p>
<p>​        讲讲虚函数指针和虚函数表</p>
<p>​        虚函数指针什么时候创建、存放在哪里</p>
<p>​    ②模板</p>
<p>​        模板是要解决什么问题，有什么缺点</p>
<p>​        模板的声明和定义为什么不能分开写，要想分开写该怎么做</p>
<p>​        模板特化、偏特化</p>
<p>​        模板在编译时生成的代码是否会相同，生成的相同的代码如何处理</p>
<p>3.其他语法</p>
<p>​    ①继承时一般要写类的哪些成员函数</p>
<p>​    ②拷贝构造函数和移动构造函数，深浅拷贝等等</p>
<p>​    ③内联是什么，为了解决什么问题，递归函数的内联会有什么问题</p>
<p>​    ④野指针是什么</p>
<p>​    ⑤指针和引用</p>
<p>​        有什么区别（说越全越好）</p>
<p>​        值类型、引用类型，C++的值、指针、引用</p>
<p>​        它们两个传参时是否要复制</p>
<p>​        除了传值，还有什么情况C++会拷贝构造临时对象</p>
<p>​    ⑥new&#x2F;delete和malloc&#x2F;free</p>
<p>​        有什么区别（说越全越好）</p>
<p>​        delete如何知道该释放多大的空间，这些信息存在什么位置</p>
<p>​        delete[]和delete的区别，基本数据类型的数组使用delete可以释放完全吗</p>
<p>​    ⑦怎样让对象只能创建在栈&#x2F;堆&#x2F;内存池中</p>
<p>​    ⑧具体如何重载operator new</p>
<p>​    ⑨RTTI原理，type_info信息存在虚函数表的哪里</p>
<p>4.C++ 11</p>
<p>​    ①Lambda表达式</p>
<p>​        Lambda表达式如何对应到函数对象</p>
<p>​        圆括号传参数是如何实现的</p>
<p>​        方括号捕获外部变量（闭包）是如何实现的</p>
<p>​    ②智能指针</p>
<p>​        有哪些智能指针，为了解决什么问题</p>
<p>​        shared_ptr的引用计数机制，它的问题，如何用weak_ptr来解决</p>
<p>​    ③类型转换</p>
<p>​        四种cast简述，分别的使用场景</p>
<p>​        dynamic_cast和虚函数的区别（dynamic_cast是为了使用子类的非虚方法或成员变量）</p>
<p>​    ④右值</p>
<p>​        左值、右值的概念，C++哪些情况下会产生临时对象</p>
<p>​        移动语义</p>
<p>​        移动构造函数和拷贝构造函数的区别</p>
<p>​        转发和完美转发的概念</p>
<p>5.STL</p>
<p>​    ①vector</p>
<p>​        vector的capacity和扩容机制，使用时要注意什么</p>
<p>​        接着上一题，如果扩容时会引发自定义类型挨个复制构造，C++有什么机制来避免这一点（右值，swap）</p>
<p>​    ②list</p>
<p>​        vector和list的区别，内存、复杂度等</p>
<p>​    ③map</p>
<p>​        二叉排序树、平衡二叉树、红黑树的设计思想</p>
<p>​        红黑树的几个性质，各种操作的时间复杂度</p>
<p>​    ④unordered_map</p>
<p>​        哈希表跟红黑树的比较，优缺点、适用场合，各种操作的时空复杂度</p>
<p>​        unordered_map容器的具体实现，怎么取哈希值，怎么处理哈希冲突</p>
<p>6.其他问题</p>
<p>​    ①C++静态链接库（lib）和动态链接库（dll）的区别</p>
<p>​    ②顺序遍历链表和顺序遍历数组哪个更快，为什么（我答的数组，原因先猜的不用指针寻址，再猜的cache命中率高，面试官都不认可）</p>
<p>​    ③memory_move和memory_copy两者的作用和区别</p>
<p>​    ④一段代码怎么样在main函数之前调用</p>
<p>​    ⑤你觉得C++有什么功能跟面向对象的三大特性相悖</p>
<h5 id="3-2-游戏引擎问题"><a href="#3-2-游戏引擎问题" class="headerlink" title="3.2 游戏引擎问题"></a>3.2 游戏引擎问题</h5><p>面试官会找你擅长的引擎问。我这里记录的都是UE4的问题。</p>
<p>我实习内容跟Slate渲染流程的性能优化相关，就把UI模块的一部分源码看得比较细，有的面试官会顺着我会的聊很多，所以下面我写下来的问题比较多。但整体上来看，AI、网络同步是更多面试官喜欢问的，如果要准备推荐先看这些模块的资料。</p>
<p>UE4 C++是做UE的厂必问的，差不多就问gameplay架构、反射、GC、智能指针这几部分，不过大概都会被问到不会为止。</p>
<p>1.UE4 UI模块</p>
<p>​    ①UMG和Slate的关系</p>
<p>​    ②UMG的几种Widget分类</p>
<p>​    ③Slate tick的大致流程，invalidation下的tick流程和缓存更新细节</p>
<p>​    ④LayerId的分配机制（跟第②个问题相对应。SLeafWidget、SCompoundWidget派生的控件基本上都直接用父类的，SPanel派生的Overlay和ConstraintCanvas重写了比较特殊的规则）</p>
<p>​    ⑤Slate的update flag和invalidate reason举几个例子，分别是什么意思</p>
<p>​    ⑥render batch合批的规则</p>
<p>​    ⑦对游戏UI框架设计的思考</p>
<p>​    ⑧其他的性能优化方法</p>
<p>2.UE4 AI模块</p>
<p>​    ①EQS是什么</p>
<p>​    ②UE4的AI行为树有什么结点</p>
<p>​    ③为什么要单独用黑板存储数据</p>
<p>​    ④AI perception是什么</p>
<p>3.UE4 网络同步</p>
<p>​    ①UE4的网络架构（C&#x2F;S、状态同步、UDP）</p>
<p>​    ②主端、服务端、模拟端的概念</p>
<p>​    ③UE4 RPC能够调用的条件是什么，除了authority是player controller之外的其他条件</p>
<p>4.UE4 C++（这些会先问有什么了解，然后根据你的回答往细了问）</p>
<p>​    ①Gameplay架构（做UE4的厂必问。主要是知乎上大钊的一系列文章）</p>
<p>​        讲讲UE4的Gameplay架构</p>
<p>​        GameMode和Actor都继承自UObject，那么什么样的UObject才能被拖到场景中</p>
<p>​        Level能有几个，跟World什么关系，GameInstance是什么</p>
<p>​    ②反射</p>
<p>​        为什么要用反射</p>
<p>​        UBT、UHT具体做什么，如何支持反射</p>
<p>​        MetaData是什么，派生出什么，跟我们用的UPROPERTY、UFUNCTION等宏如何对应</p>
<p>​        平时写的宏有什么用，比如UCLASS代表什么</p>
<p>​    ③GC</p>
<p>​        UObject本身的GC使用的标记-清除法的大概流程，分帧之类的细节</p>
<p>​        如果派生自UObject的对象不想使用引擎的这套GC，该怎么做</p>
<p>​        如果非UObject的自定义类型想使用这套GC，该怎么做</p>
<p>​    ④智能指针</p>
<p>​        UE4 C++的智能指针和C++ 11的智能指针有什么区别和联系，要解决什么问题</p>
<p>​        TWeakPtr等能不能指向UObject类型的对象，能指向UObject的智能指针是什么</p>
<p>​        TWeakObjectPtr怎么判断指向的UObject对象是否invalid</p>
<p>​        智能指针GC和UE4自己的GC是否会冲突</p>
<p>5.开发经验</p>
<p>​    ①用rider for Unreal（IDE）调试的心得</p>
<p>​    ②Widget Reflector（调试UI的引擎工具）的使用经验</p>
<p>​    ③Gameplay开发中，UE4变量、方法之类的哪些应该写在C++里，哪些写在蓝图里</p>
<h5 id="3-3-游戏开发问题"><a href="#3-3-游戏开发问题" class="headerlink" title="3.3 游戏开发问题"></a>3.3 游戏开发问题</h5><p>关于游戏开发的通用问题。各个模块会再问一些特定引擎的问题，对照上面的3.2。</p>
<p>1.游戏AI</p>
<p>​    ①常用的做法简述（NavMesh + 寻路）</p>
<p>​    ②NavMesh的生成过程简述</p>
<p>​    ③A*寻路算法的原理</p>
<p>​    ④是否了解多个AI都在自动寻路时的动态避障算法</p>
<p>2.物理碰撞</p>
<p>​    ①如何判断子弹射击到敌人（然后继续问包围盒、碰撞检测算法等），如果子弹特别快怎么办</p>
<p>​    ②BVH、八叉树等场景管理加速结构，在这些树上检测碰撞的过程</p>
<p>3.网络同步</p>
<p>​    ①状态同步和帧同步的原理</p>
<p>​    ②上面两个在反作弊、断线重连、实时性等等场合，用哪种同步策略好</p>
<p>​    ③TCP和UDP应用上的区别和游戏开发中的使用偏向（不用细说它们的原理）</p>
<p>4.场景题</p>
<p>​    ①跳跃到最高点自动开枪，这个功能应该怎么做</p>
<p>​    ②游戏分辨率变换、窗口尺寸变换时，UI应该怎么适配</p>
<p>​    ③角色可以穿脱装备，每个装备对角色的血量有不同的buff，该如何设计这个功能</p>
<p>​    ④水的着色应该写在vertex shader还是fragment shader</p>
<p>​    ⑤游戏中物体从不透明变成透明，对渲染有什么难度</p>
<p>5.游戏设计模式</p>
<p>​    ①单例模式的作用，使用单例模式和只创建一个static对象有什么区别</p>
<p>​    ②工厂模式，有构造函数为什么还要设计抽象工厂模式</p>
<p>​    ③观察者模式，MVC是什么、怎样对应观察者模式，观察者模式的应用场景</p>
<p>6.其他</p>
<p>​    ①Unity对象生命周期（乍一听不知道是什么意思，实际上是问MonoBehaviour类的tick过程中，各函数调用的顺序，Awake、Update、FixedUpdate、OnDisable等等）</p>
<p>​    ②让你设计一个游戏引擎UI框架，有什么想法，要考虑什么</p>
<p>​    ③游戏引擎的RHI是干嘛的</p>
<p>​    ④游戏开发中的对象池技术是干嘛的</p>
<h5 id="3-4-图形学"><a href="#3-4-图形学" class="headerlink" title="3.4 图形学"></a>3.4 图形学</h5><p>对于客户端开发，图形学只是加分项，不算特别重要。我是入门水平，也没有相关的项目，所以被问到的不多，绝大多数公司只让讲一下渲染管线就好了。</p>
<p>1.图形渲染管线，会先让你大致讲一下渲染管线，然后问一些细节问题</p>
<p>​    ①drawcall是什么，为什么要减小drawcall</p>
<p>​    ②粗细粒度culling的区别</p>
<p>​    ③MVP、viewport分别进行什么变换</p>
<p>​    ④深度测试是什么，early-z是什么</p>
<p>​    ⑤延迟渲染管线是什么，有什么优缺点</p>
<p>​    ⑥半透明物体的渲染顺序</p>
<p>​    ⑦shader中为什么要避免使用if语句</p>
<p>2.抗锯齿技术</p>
<p>​    ①MSAA、SSAA的区别</p>
<p>​    ②延迟渲染为什么用不了硬件AA</p>
<p>​    ③DLSS原理的简单说明</p>
<p>3.光照模型</p>
<p>​    ①BRDF概念</p>
<p>​    ②简单描述blinn-phong光照模型，specular、ambiant、diffuse</p>
<p>​    ③简单描述pbr光照模型，lambert漫反射 + cooktorrence反射（DGF）</p>
<h5 id="3-5-计算机基础"><a href="#3-5-计算机基础" class="headerlink" title="3.5 计算机基础"></a>3.5 计算机基础</h5><p>五百年没看了，只记得几个经典知识点，再问详细的实在是忘了。跟面试官说不会，基本上也不会再细究。</p>
<ul>
<li>编译的过程，几个阶段分别发生什么</li>
<li>寻址的过程</li>
<li>虚拟内存空间的概念</li>
<li>进程和线程的概念</li>
<li>进程间通信方式</li>
<li>进程的多个线程是否共享进程的全部资源</li>
<li>进程切换、线程切换，是否引起上下文切换、用户态&#x2F;内核态切换</li>
<li>为什么线程太多会导致性能下降（1s的进程分10个线程，比分100个线程的执行时间要更短，为什么？我答了调度有消耗但面试官不认同）</li>
<li>怎么发现、定位内存泄漏</li>
<li>OSI网络模型</li>
<li>TCP、UDP的区别</li>
<li>在应用层如何保证UDP的可靠传输</li>
<li>TCP的三次握手四次挥手</li>
</ul>
<h5 id="3-6-算法题、数学题、脑筋急转弯"><a href="#3-6-算法题、数学题、脑筋急转弯" class="headerlink" title="3.6 算法题、数学题、脑筋急转弯"></a>3.6 算法题、数学题、脑筋急转弯</h5><ul>
<li>口述：判断扇形攻击命中（2D空间判断点在扇形内）</li>
<li>口述：2D空间中，在三角形&#x2F;圆内如何随机并且均匀采样点，均匀是指点各自占的面积尽量均匀</li>
<li>口述：判断点在三角形内（比较基础，但经常被问。重心坐标、叉乘法、面积求和）</li>
<li>口述：层次遍历二叉树</li>
<li>口述：快速排序</li>
<li>口述：绳子分成3段，要求三段长度乘积最大</li>
<li>口述：两个鸡蛋判断最少从哪层楼扔下来会碎，怎么弄比较好</li>
<li>口述：如何检测链表相交的所有情况，包括有环、在环上相交、入环点不一样等等</li>
<li>手撕：1、2、3、4、5、6、7、8的链表变成1、8、2、7、3、6、4、5，不使用额外存储空间</li>
<li>手撕：合并n个有序链表</li>
<li>手撕：有重复元素的顺序数组，查找新来的元素最前面的插入位置</li>
<li>手撕：DFS解数独</li>
<li>手撕：数组的最长递增连续子序列长度</li>
<li>手撕：两个字符串的最长不连续公共子串（逆天，看一眼直接放弃）</li>
<li>手撕：链表的倒数第k个元素</li>
<li>手撕：顺时针90度旋转矩阵</li>
<li>手撕：数组的第k大元素</li>
<li>手撕：判断汉字的unicode编码是否合法（题很长，记不清了）</li>
<li>手撕：判断回文链表，不使用额外存储空间</li>
</ul>
<h4 id="四-各公司面试体验"><a href="#四-各公司面试体验" class="headerlink" title="四 各公司面试体验"></a>四 各公司面试体验</h4><h5 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h5><ul>
<li>一面挂：tap4fun，红海无限，极致游戏，SNK中国，永航科技，联想研究院（U3D）</li>
<li>二面挂：诗悦网络，字节跳动朝夕光年，剑心互娱，分岔点VR，酷家乐（渲染开发），祖龙游戏，柠檬微趣</li>
<li>三面挂：中国系统（UE4）</li>
<li>放弃hr面：游族（听说必须实习），360游戏（听说必须实习）</li>
<li>全部面完：盖娅互娱（必须实习），数字天空</li>
</ul>
<p>总结如下（大致按一面时间排序）。</p>
<h5 id="中国系统（UE4）"><a href="#中国系统（UE4）" class="headerlink" title="中国系统（UE4）"></a>中国系统（UE4）</h5><p>风评很差的伪国企。看到有UE4岗位，投了练手的。说是刚成立的部门，布局元宇宙，招来的人不进具体项目，而是培养几年作为未来项目的骨干。</p>
<p>8&#x2F;30投递，9&#x2F;6笔试，9&#x2F;9一面，9&#x2F;16二面，9&#x2F;27三面。</p>
<p>一面技术面30m左右，节奏较快，只问C++八股和实习经历。二面hr面也是30m，面试官很喜欢纠结字眼。三面准备3页ppt，8分钟时间讲。</p>
<p>9&#x2F;30感谢信。</p>
<h5 id="祖龙娱乐（UE4）"><a href="#祖龙娱乐（UE4）" class="headerlink" title="祖龙娱乐（UE4）"></a>祖龙娱乐（UE4）</h5><p>祖龙连着裁了两年的应届生了，也是当作积累面试经验投的。直接面项目组，我的是祖龙北京，UE4 FPS游戏，类似Apex。</p>
<p>9&#x2F;8投递，没笔试直接约面，9&#x2F;20下午一二面连着，加起来约2h15m。</p>
<p>一面上来先做题，然后问一些基础知识，C++、数学、图形学等等。一面完说通过了，如果后面有时间可以连着二面。二面就直接全程做题，面试官不太耐烦的样子。</p>
<p>虽然是UE4项目组，不过面试不问实习和UE4 C++之类的。祖龙看样子是对算法要求比较高。我刷题比较弱，当时也是我秋招的第一场像样的技术面试，整体表现不太好。二面后没消息。</p>
<h5 id="诗悦网络（游戏客户端）"><a href="#诗悦网络（游戏客户端）" class="headerlink" title="诗悦网络（游戏客户端）"></a>诗悦网络（游戏客户端）</h5><p>得知是去了再分项目组，最好去实习。听别人说今年诗悦招人很多，进了再来一轮筛选，不太厚道。</p>
<p>9&#x2F;6投递，9&#x2F;11笔试，9&#x2F;22一面，9&#x2F;29二面。</p>
<p>一面20m左右，先问一些项目、实习心得，再问一点C++、数据结构简单题，无难度。二面hr面，问能不能来实习。</p>
<p>二面后挂。10&#x2F;11感谢信。</p>
<h5 id="红海无限（游戏开发）"><a href="#红海无限（游戏开发）" class="headerlink" title="红海无限（游戏开发）"></a>红海无限（游戏开发）</h5><p>在北京，做cocos2d，说最好去实习。</p>
<p>9&#x2F;15投递，9&#x2F;17笔试，9&#x2F;22一面。</p>
<p>一面1h，先细问实习经历，然后问职业规划、兴趣、业余学习之类的，问一下渲染管线，然后就是做道题（DFS解数独）。我没完全写出来。</p>
<p>9&#x2F;23感谢信。</p>
<p><strong>联想研究院（unity）</strong></p>
<p>投的是武汉的unity岗位，得知是新部门、做渲染方面的工作，用什么引擎也还没确定。</p>
<p>9&#x2F;16投递，没有笔试，9&#x2F;24一面。</p>
<p>一面两个面试官，一共不到30m。技术面试官问了下实习经历，然后讲了下渲染管线，问了几个unity引擎的实操题。然后另一位hr面试官也问了几个常规问题。</p>
<p>一面后无后续。</p>
<h5 id="极致游戏（游戏开发）"><a href="#极致游戏（游戏开发）" class="headerlink" title="极致游戏（游戏开发）"></a>极致游戏（游戏开发）</h5><p>9&#x2F;16投递，9&#x2F;17笔试，9&#x2F;27一面。</p>
<p>一面有两位面试官，一位技术一位hr，共1h多一点。知识上问得不多，主要就是各种八股，难度不高。hr面试官问得很多，有实习感悟、项目成长之类的，也有一些常规无聊hr问题。总体上聊得还是挺愉快的，面试时间也挺长，但反手就给挂了。</p>
<p>10&#x2F;8感谢信。</p>
<h5 id="SNK中国（UE4）"><a href="#SNK中国（UE4）" class="headerlink" title="SNK中国（UE4）"></a>SNK中国（UE4）</h5><p>也是UE4项目，好像是MMO，但大概也是去了再分。会问倾向于进上线了的项目组还是在研项目组。</p>
<p>9&#x2F;16投递，9&#x2F;24笔试，9&#x2F;29一面。</p>
<p>参加得比较早的面试，有C++八股，也有一些UE4的内容。我答得不是很好。</p>
<p>一面后无后续。</p>
<h5 id="tap4fun（游戏客户端）"><a href="#tap4fun（游戏客户端）" class="headerlink" title="tap4fun（游戏客户端）"></a>tap4fun（游戏客户端）</h5><p>9&#x2F;8投递，9&#x2F;16笔试，9&#x2F;29一面。</p>
<p>一面30m。20m细问实习，5m问项目（碰撞、网络同步等功能）和渲染管线，剩下开始问网络、OS，太久没看了实在答不上来。</p>
<p>放完国庆假期挂了。10&#x2F;10感谢信。</p>
<h5 id="盖娅互娱（unity）"><a href="#盖娅互娱（unity）" class="headerlink" title="盖娅互娱（unity）"></a>盖娅互娱（unity）</h5><p>在北京，unity项目，类似江南百景图。必须实习转正，可以寒假去实习。</p>
<p>9&#x2F;15投递，没笔试，10&#x2F;9一面，10&#x2F;10二面。</p>
<p>一二面都是30m左右。一面无八股，按照游戏开发的各个模块来问，渲染、AI、物理、网络、脚本语言等，渲染问得挺多。二面印象中C++八股问得多一点。</p>
<p>流程倒是飞快，二面完hr在微信上谈薪。谈完薪第二天又说hc调整，不发offer了。可能也是想找能去长时间实习的。</p>
<h5 id="游族（游戏客户端）"><a href="#游族（游戏客户端）" class="headerlink" title="游族（游戏客户端）"></a>游族（游戏客户端）</h5><p>我面的是游族上海的少年工作室，做cocos2d。每轮面试可以自选时间，也有hr微信联系，挺专业的。看到牛客上游族面经非常多，我应该是比较靠后面的批次。</p>
<p>9&#x2F;6投递，9&#x2F;23笔试，10&#x2F;11一面，10&#x2F;21二面，10&#x2F;25约hr面。</p>
<p>一面常规技术面，30m，两个面试官，问八股为主（因为我没用过cocos），问对不同的引擎有什么看法，接不接受转cocos。二面像是leader面，1h左右，技术问得少，会问一些规划、感悟，还问发了offer会不会来，我就比较真诚地说会对比考虑。</p>
<p>感觉表现得还不错，面到底应该有可能拿到offer。二面后约hr面，但当时马上11月了，一是对cocos没兴趣并且已经拿到了UE4的offer，二是考虑直接不做游戏客户端了，三是看帖子说要实习，就主动放弃了。</p>
<p>室友跟我分到一个工作室了，面试时间相近，也面到底、拿offer了，钱给的挺多，也对他挺器重的，并且也没说必须实习。游族是在特定领域很有成功经验的老厂了，如果对项目、城市感兴趣，选游族也是一个不错的选择。</p>
<h5 id="360游戏（游戏客户端）"><a href="#360游戏（游戏客户端）" class="headerlink" title="360游戏（游戏客户端）"></a>360游戏（游戏客户端）</h5><p>360北京，做出海手游，unity引擎，有轻中度各类项目，去了分。</p>
<p>9&#x2F;6投递，9&#x2F;9笔试，10&#x2F;10一面，10&#x2F;19二面，约10&#x2F;25三面。</p>
<p>一面25m左右，问了一些简单的八股，我答得都不错。二面1h左右，这场面试应该是我整个秋招印象最深刻的面试了，面试官基本上是以一个前辈的身份来聊聊天，一方面讲游戏开发技术的专精、探索领域和职业晋升路线，另一方面也介绍一些相关的重点知识，偶尔穿插着问几个问题。面完了感觉像上了一节课一样，而且接触到了一些工作经验、职业路线方面的东西。作为还没入行的新人，我们之前学习的重点都在“入行”这件事情上，这种工作后的经验确实很难了解到。</p>
<p>同样是二面面试官说的，360游戏现在确实没什么名气，但也刚做了一些调整，大家都希望能把360游戏做起来。只以二面的感受来说，我觉得去360游戏也真不是不行。</p>
<p>但差不多也是10月底了，出于跟游族类似的考虑，放弃三面了。室友面了三面，要求提前实习。</p>
<h5 id="酷家乐（图形渲染引擎开发）"><a href="#酷家乐（图形渲染引擎开发）" class="headerlink" title="酷家乐（图形渲染引擎开发）"></a>酷家乐（图形渲染引擎开发）</h5><p>做家具场景的渲染引擎的公司。10&#x2F;9投递，10&#x2F;11一面，10&#x2F;13二面。</p>
<p>一面其实就是笔试，给1h开摄像头做一道大模拟题，大意是识别车牌。</p>
<p>二面前搜了一些面经，看到都是图形学、渲染问题，但实际面试问我的全是C++八股，30m左右，答得还可以。</p>
<p>11&#x2F;1感谢信。</p>
<h5 id="剑心互娱（游戏开发）"><a href="#剑心互娱（游戏开发）" class="headerlink" title="剑心互娱（游戏开发）"></a>剑心互娱（游戏开发）</h5><p>9&#x2F;11投递，9&#x2F;14笔试，10&#x2F;12一面，10&#x2F;18二面。</p>
<p>一二面都是30m左右。一面问简单八股。二面之前要填一大堆个人资料，包括家庭信息什么的，跟多益要的那些信息类似。二面问的东西偏向计算机基础，有一道编程题。</p>
<p>二面的内容对于我来说算是比较偏了。前面记录的一些怪问题都出自这一面，比如顺序遍历数组和链表哪个更快、为什么分10个线程不如分5个线程。出的编程题涉及到汉字转unicode编码，这个确实没接触过。</p>
<p>听完问题感觉是不难的，但一下子也不知道面试官想问什么知识点。跟面试官交流想法（比如确定一下思考的方向）是得不到任何有用信息的，反正两位面试官说完题目就开始闭麦聊天，跟我交流就是把题目翻来覆去地说。体验并不比字节二面好，唯一好的就是面试官不PUA，觉得我不行就直接不理人了。或者说字节二面我还能学点东西，剑心二面真的是意义不明。</p>
<p>10&#x2F;19感谢信。</p>
<h5 id="数字天空（UE4）"><a href="#数字天空（UE4）" class="headerlink" title="数字天空（UE4）"></a>数字天空（UE4）</h5><p>成都的中小厂，做UE4主机游戏，B站上有当家项目project dt的宣传片。去了公司再分项目，所有项目都是UE4主机游戏。</p>
<p>9&#x2F;15投递，9&#x2F;30笔试，10&#x2F;12一面，10&#x2F;17二面，10&#x2F;18三面（hr），10&#x2F;20四面（cto）。</p>
<p>前两面是技术面，各2位面试官，都是1h多，不刷题。问得挺硬核的。可以说对于UE4，你会的所有东西都会被问到。先细问实习、项目，再问一点八股，然后UE4 C++也问得很多，当时我很多东西只听过大致的概念，细节问题挺多都没答上来。</p>
<p>四面完当天电话谈薪，马上收到offer邮件。流程很快，也不逼签，给三个月随便考虑。</p>
<p>数字天空秋招体验确实挺好的，面试官比较专业，流程也快，（除了四面里有三面是临时通知推迟的，5m到1h不等，）薪资在成都也算不错了，并且不加班。</p>
<p>一句话来说，不管是秋招体验还是在做主机游戏，都有种清流的感觉。在今年的环境下确实让人很有好感。</p>
<p>不过也有几点需要考虑的，一是project dt被腾讯拆了，制作人出走。二是网上对数字天空的技术水平评价不高。但其实我都不算很在意，最后想的是如果做游戏客户端开发那就去数字天空了。但后面决定转行了，比较可惜。</p>
<h5 id="字节跳动朝夕光年（游戏客户端）"><a href="#字节跳动朝夕光年（游戏客户端）" class="headerlink" title="字节跳动朝夕光年（游戏客户端）"></a>字节跳动朝夕光年（游戏客户端）</h5><p>更是重量级。朝夕光年深圳，只说是UE4项目。</p>
<p>9&#x2F;10投递，9&#x2F;18笔试，10&#x2F;13一面，10&#x2F;19二面。</p>
<p>一面和二面流程类似，都是1h多，问得比较全面。先讲实习，然后再问UE4各模块的东西。然后就是C++八股，UE4 C++。最后出一道编程题。比较基础的八股也会一直往深了问。</p>
<p>二面的那天我面到字节是第四家了，脑子不太清楚。字节问得深，主要是我也菜，加上面试官一直在说“掌握得不好啊”、“我觉得挺可惜的”这种话，全程歪着身子叹气翻白眼，总之当时确实心态不太好。最后的算法题并不难，但我一遍写完发现没写对，一下子就懒得调了，跟面试官说放弃了。算是从实习到秋招的压力累计而导致的破防时刻吧，总要有这么一次的。</p>
<p>10&#x2F;21感谢信。</p>
<h5 id="永航科技（UE4）"><a href="#永航科技（UE4）" class="headerlink" title="永航科技（UE4）"></a>永航科技（UE4）</h5><p>也是用UE4的公司，在北京，之前做了生死轮回的项目组。</p>
<p>9&#x2F;30投递，10&#x2F;13笔试，10&#x2F;19一面。</p>
<p>一面一共20分钟吧。面试官听说我的公开课项目主要用蓝图做的，而且实习也不是做gameplay的，就不太想聊了。随便问了几个UE4的游戏场景题，答不上来。</p>
<p>一面后无后续。</p>
<h5 id="柠檬微趣（unity）"><a href="#柠檬微趣（unity）" class="headerlink" title="柠檬微趣（unity）"></a>柠檬微趣（unity）</h5><p>项目以unity休闲游戏为主。</p>
<p>国庆节后在牛客投递，10&#x2F;15笔试，10&#x2F;20一面，10&#x2F;21二面。</p>
<p>每面1h左右，都是先细问实习，然后问八股，最后口述几个算法题，再手撕做一道。</p>
<p>我两面自认为表现都还可以（最晚的面试，水平巅峰了）。手撕题不算顺利但都做出来了。可能技术栈不匹配，二面后无后续。</p>
<h4 id="五-经验教训"><a href="#五-经验教训" class="headerlink" title="五 经验教训"></a>五 经验教训</h4><h5 id="5-1-准备"><a href="#5-1-准备" class="headerlink" title="5.1 准备"></a>5.1 准备</h5><p>今年的秋招情况简单来说，就是大公司不招人或者很早就招完了，导致很多人都在海投中小公司。“师兄投5家、我投50+”确实发生在我身上了。</p>
<p>因此，今年的中小公司也是优中选优，竞争激烈。实习让我认识到游戏行业是倾注了很多人的热情和聪明才智的地方，如果像我一样学历、聪明程度、努力程度、对技术的热爱程度、各方面的知识都没什么亮点，要做好一次次碰壁的心理准备，或者早做打算、另谋出路。</p>
<p>也是因为今年我等普通人的归宿大部分都不是大厂，越小的公司越希望马上来一个即战力，需要注意：第一是这些公司跟大公司相比，面试考察的重心会稍微偏向项目、实践，像基础、八股、刷题都不会多问，因此如果有比较好的gameplay开发经历是占尽优势的；第二，非常非常多的公司会要求毕业前提前实习（至少是希望你来实习，也有公司直接发实习offer，来了再转正），如果跟我一样老师不放人的，那可以直接pass了。</p>
<p>而说到gameplay开发经历，首先简历上有大厂实习经历肯定是好的，如果你平时就可以随便实习，就尽量早找、早去，到了暑假就不要再实习了，除非有100%转正的把握。如果跟我一样老师只让研二暑假去实习，也一定不要全身心地投入工作，而应该把准备秋招当成主业，积极投递提前批、第一批。一方面，面试确实是有必要先练习的，我也感觉自己面到后面，不管是知识上还是特定问题的表达上都有了一些进步；另一方面，早的都是大厂、提前批，好机会更多，而中小厂8月把笔试都做完，9月也可以第一批进面。</p>
<p>我是在天美上海实习，6月在线上没怎么进入状态，解封后去了上海，把7、8两个月全部投入到实习工作了，当时的想法是有hc就转正，转不了正就拿这一段比较有含金量的工作经历来弥补一下刷题等秋招准备上的不足。但我工作的内容却跟gameplay开发不相关，而是去做UE4引擎本身的一个功能。虽然在面试时多少能讲一点，面试官对这段经历的评价也都不错，但实在是跟各个公司的需要不太符合，这时候我缺乏gameplay经验、秋招准备也不足的短板就都显露出来了。大家不要走我的老路。</p>
<p>如果简历上没有实习，做个好点的demo应该也差不多？看上去一大把的公司就是想招一个实习生直接干到毕业，那么之前有没有实习或许就显得没那么重要了。我不了解也不瞎说了，但相对的各种知识的准备一定要够硬。</p>
<h5 id="5-2-心态"><a href="#5-2-心态" class="headerlink" title="5.2 心态"></a>5.2 心态</h5><p>再有就是很多面经都在说的心态问题。人人都说要坚持，因为秋招确实坚持就是胜利。开始时的不顺利是很普遍的，我和我室友进度走到比较后面的公司反而全都是10月才开始面的。</p>
<p>此外，最好也不要因为投晚了、准备得不好、跟岗位要求不是很符合之类的原因就放弃一些机会。比如我当时就是抱着试一试的心态，给我最后选择去的公司和岗位投了简历，整个面试过程问得也不算难，甚至知识上的问题问得都不多，都不知道该记录点啥。</p>
<p>另外，由于海投，所有人的战线应该都不短，如果能在秋招期间利用好时间，应该也能把刷题练熟，或者直接准备一个好项目了。然而我是做不到的，这几个月各方面进度都是停滞的，没怎么学过新东西了。刚好最近在玩『P5R』，也知道了拥有强大的内心是不容易但非常重要的一件事情。</p>
<p>其实我想说的是更长远的心态。我们每个人都是怀着某种理想与热情，想要进入游戏行业，做自己感兴趣的工作的。但对我这种普通人来说，现实往往就是一边看着各种新游戏PV和知乎的技术分享觉得心潮澎湃，另一方面发现自己连入行的机会都没有，很容易因为理想与现实之间的差距而感到沮丧。再结合自己从小到大越来越努力，日子却越来越难过的事实，去怀疑人生意义也不是没有可能。</p>
<p>说到底，今年的情况大家都能看到，秋招找到工作也不意味着万事大吉。offer被毁、试用期被卡、应届被裁、项目被砍、行业被叫停的事情越来越多，在各种环境下保持一种积极学习的心态，会是更长期也更重要的修行。因此，如果秋招期间能建立这样的心态或许是一件好事，比多拿一个offer要更重要。<a href="https://www.nowcoder.com/users/254135030">@BBBourne</a>在他的面经里说：“无论环境形式好与坏，提升自己的能力是最关键的。”</p>
<p>当然了，我心里还是希望我所经历的就是最难的一届秋招了。再次祝大家都能找到理想的工作，祝今后的游戏人都顺利入行！</p>
<p>有任何问题欢迎留言或者私信。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>工作</tag>
        <tag>生活小记</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>dl &amp; ai - andrew ng</title>
    <url>/2021/05/27/dlang/</url>
    <content><![CDATA[<p>其实最近一直在看一些具体的任务了，之前基础的深度学习笔记没有整理成文章，现在归纳一下。主要看了这些：</p>
<ul>
<li>3B1B的神经网络视频</li>
<li>胡浩基老师网课的一部分</li>
<li>吴恩达deeplearning.ai的一部分（主要看的这个）</li>
<li>曹建老师的tensorflow2视频</li>
</ul>
<h1 id="第一部分：3B1B的神经网络视频"><a href="#第一部分：3B1B的神经网络视频" class="headerlink" title="第一部分：3B1B的神经网络视频"></a>第一部分：3B1B的神经网络视频</h1><p>（主要是一些概念的直观理解。）</p>
<p>学习的含义：找到特定的 $\omega$ 和 $b$ ，使代价函数最小化。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3b1b1.png'  width="80%" height="80%"/ loading="lazy">



<p>反向传播：计算单个训练样本想怎样修改 $\omega$ 和 $b$ 。不仅是每个参数应该变大还是变小，还包括这些变化的比例是多大，才能最快下降梯度函数。一个真正的梯度下降过程要对所有的训练样本求平均，但计算太慢，就先把所有的样本分到minibatch中，，计算一个minibatch来作为梯度下降的一步，最终会收敛到局部最优点。</p>
<p>为了使 $a_{i+1}$ 的某个输出增大，可以</p>
<ul>
<li>增大 $b$ </li>
<li>增大 $\omega$ ：增加上层活跃的神经元的权重更好。依据对应权重大小，对激活值做出成比例的改变。</li>
<li>改变 $a_i$</li>
</ul>
<p>反向传播的链式法则</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3b1b2.png'  width="80%" height="80%"/ loading="lazy">



<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3b1b3.png'  width="80%" height="80%"/ loading="lazy">





<h1 id="第二部分：胡浩基老师NN、CNN部分"><a href="#第二部分：胡浩基老师NN、CNN部分" class="headerlink" title="第二部分：胡浩基老师NN、CNN部分"></a>第二部分：胡浩基老师NN、CNN部分</h1><p>手写了笔记，其实重复的地方比较多，就不每张拍照上传了。后续把几个手推的公式拍一下。</p>
<h1 id="第三部分：吴恩达deeplearning-ai网课"><a href="#第三部分：吴恩达deeplearning-ai网课" class="headerlink" title="第三部分：吴恩达deeplearning.ai网课"></a>第三部分：吴恩达deeplearning.ai网课</h1><p>看的网易云课堂做的字幕版本，可惜右上方有水印，有些地方有遮挡。</p>
<h2 id="第一课-神经网络和深度学习（Neural-Networks-and-Deep-Learning）"><a href="#第一课-神经网络和深度学习（Neural-Networks-and-Deep-Learning）" class="headerlink" title="第一课 神经网络和深度学习（Neural Networks and Deep Learning）"></a>第一课 神经网络和深度学习（Neural Networks and Deep Learning）</h2><h3 id="第一周：深度学习引言（Introduction-to-Deep-Learning）"><a href="#第一周：深度学习引言（Introduction-to-Deep-Learning）" class="headerlink" title="第一周：深度学习引言（Introduction to Deep Learning）"></a>第一周：深度学习引言（Introduction to Deep Learning）</h3><h4 id="1-1-欢迎"><a href="#1-1-欢迎" class="headerlink" title="1.1 欢迎"></a>1.1 欢迎</h4><p>关于课程安排</p>
<h4 id="1-2-什么是神经网络？"><a href="#1-2-什么是神经网络？" class="headerlink" title="1.2 什么是神经网络？"></a>1.2 什么是神经网络？</h4><p>神经网络可以当作一个函数。通过数据集计算从x到y的精准映射函数，然后对于新的数据x，给出预测的结果y。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/1.png'  width="80%" height="80%"/ loading="lazy">



<h4 id="1-3-神经网络的监督学习-Supervised-Learning-with-Neural-Networks"><a href="#1-3-神经网络的监督学习-Supervised-Learning-with-Neural-Networks" class="headerlink" title="1.3 神经网络的监督学习(Supervised Learning with Neural Networks)"></a>1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</h4><p>监督学习：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/2.png' width="80%" height="80%"/ loading="lazy">

<p>对于图片，使用CNN。对于序列信息（音频、语言信息等）使用RNN。</p>
<p>结构化数据：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/3.png' width="80%" height="80%"/ loading="lazy">

<p>神经网络能帮助计算机理解无结构化数据。</p>
<h4 id="1-4-为什么神经网络会流行？"><a href="#1-4-为什么神经网络会流行？" class="headerlink" title="1.4 为什么神经网络会流行？"></a>1.4 为什么神经网络会流行？</h4><p>数据和计算规模的进展。现在获得了很大的数据量、计算了很复杂的网络。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/4.png' width="80%" height="80%"/ loading="lazy">

<p>其他原因：算法的改进，比如从sigmoid函数到relu函数</p>
<h4 id="1-5-1-6-课程安排"><a href="#1-5-1-6-课程安排" class="headerlink" title="1.5~1.6 课程安排"></a>1.5~1.6 课程安排</h4><p>略</p>
<h3 id="第二周：神经网络的编程基础（Basics-of-Neural-Network-Programming）"><a href="#第二周：神经网络的编程基础（Basics-of-Neural-Network-Programming）" class="headerlink" title="第二周：神经网络的编程基础（Basics of Neural Network Programming）"></a>第二周：神经网络的编程基础（Basics of Neural Network Programming）</h3><h4 id="2-1-二分类-Binary-Classification"><a href="#2-1-二分类-Binary-Classification" class="headerlink" title="2.1 二分类(Binary Classification)"></a>2.1 二分类(Binary Classification)</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/6.png' width="80%" height="80%"/ loading="lazy">

<p>数据集按列组成矩阵：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/5.png' width="80%" height="80%"/ loading="lazy">

<p>X.shape &#x3D; (n<sub>x</sub>, m)</p>
<p>y.shape &#x3D; (1, m)</p>
<h4 id="2-2-逻辑回归-Logistic-Regression"><a href="#2-2-逻辑回归-Logistic-Regression" class="headerlink" title="2.2 逻辑回归(Logistic Regression)"></a>2.2 逻辑回归(Logistic Regression)</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/7.png' width="80%" height="80%"/ loading="lazy">

<p>在神经网络中，将 $b$ 和 $w$ 分开表示，不采用逻辑回归那样组合成 $\theta$ 的形式。</p>
<h4 id="2-3-逻辑回归的代价函数"><a href="#2-3-逻辑回归的代价函数" class="headerlink" title="2.3 逻辑回归的代价函数"></a>2.3 逻辑回归的代价函数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/8.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-4-梯度下降（Gradient-Descent）"><a href="#2-4-梯度下降（Gradient-Descent）" class="headerlink" title="2.4 梯度下降（Gradient Descent）"></a>2.4 梯度下降（Gradient Descent）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/9.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-5-2-6-导数"><a href="#2-5-2-6-导数" class="headerlink" title="2.5~2.6 导数"></a>2.5~2.6 导数</h4><p>略</p>
<h4 id="2-7-计算图（Computation-Graph）"><a href="#2-7-计算图（Computation-Graph）" class="headerlink" title="2.7 计算图（Computation Graph）"></a>2.7 计算图（Computation Graph）</h4><p>计算图表示从左向右的计算过程。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/10.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-8-计算图导数"><a href="#2-8-计算图导数" class="headerlink" title="2.8 计算图导数"></a>2.8 计算图导数</h4><p>根据计算图，从右到左计算函数 J 的导数。（链式求导）</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/11.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-9-逻辑回归的梯度下降"><a href="#2-9-逻辑回归的梯度下降" class="headerlink" title="2.9 逻辑回归的梯度下降"></a>2.9 逻辑回归的梯度下降</h4><p>用计算图理解逻辑回归的梯度下降。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/12.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-10-梯度下降的例子"><a href="#2-10-梯度下降的例子" class="headerlink" title="2.10 梯度下降的例子"></a>2.10 梯度下降的例子</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/13.png' width="80%" height="80%"/ loading="lazy">

<p>dw<sub>1</sub>、 dw<sub>2</sub>、db 作为累加器。数据集循环后，J、 dw<sub>1</sub>、 dw<sub>2</sub>、db 除以样本个数。</p>
<p>一次梯度下降有两层循环，外层循环遍历所有数据样本（m个），内层循环遍历所有特征 w（n个）。</p>
<p>在深度学习中，数据量很大，<strong>为了不用显式的for循环，使用向量化</strong>。</p>
<h4 id="2-11-向量化-Vectorization"><a href="#2-11-向量化-Vectorization" class="headerlink" title="2.11 向量化(Vectorization)"></a>2.11 向量化(Vectorization)</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/14.png' width="80%" height="80%"/ loading="lazy">

<p>向量化是很有必要的。在上图1000000维向量相乘运算中，使用向量化比使用for循环节省300倍的时间。</p>
<h4 id="2-12-更多的向量化例子"><a href="#2-12-更多的向量化例子" class="headerlink" title="2.12 更多的向量化例子"></a>2.12 更多的向量化例子</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/15.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-13-向量化逻辑回归"><a href="#2-13-向量化逻辑回归" class="headerlink" title="2.13 向量化逻辑回归"></a>2.13 向量化逻辑回归</h4><p>向量化逻辑回归的正向传播：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/16.png' width="80%" height="80%"/ loading="lazy">

<p>Z &#x3D; np.dot(w.T, X) + b</p>
<blockquote>
<p>w.shape  &#x3D;(n_x, 1)，每个特征对应一个w，列向量</p>
<p>X.shape &#x3D; (n_x, m)</p>
<p>z.shape &#x3D; (1, m)</p>
<p>b本来是一个实数，python的broadcasting机制在相加时，把b扩展为 （1， m) 维的行向量。</p>
</blockquote>
<h4 id="2-14-向量化逻辑回归的梯度计算"><a href="#2-14-向量化逻辑回归的梯度计算" class="headerlink" title="2.14 向量化逻辑回归的梯度计算"></a>2.14 向量化逻辑回归的梯度计算</h4><p>向量化逻辑回归的反向传播：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/17.png' width="80%" height="80%"/ loading="lazy">

<p>dZ &#x3D; A - Y</p>
<p>db &#x3D; np.sum(dZ) &#x2F; m</p>
<p>dw &#x3D;  np.dot(X, dZ.T) &#x2F; m</p>
<blockquote>
<p>X.shape &#x3D; (n_x, m)</p>
<p>dZ.shape &#x3D; (1, m)</p>
<p>dw.shape &#x3D; (n_x, 1)</p>
</blockquote>
<p>一次梯度下降：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/18.png' width="80%" height="80%"/ loading="lazy">





<h4 id="2-15-Python中的广播机制（Broadcasting-in-Python）"><a href="#2-15-Python中的广播机制（Broadcasting-in-Python）" class="headerlink" title="2.15 Python中的广播机制（Broadcasting in Python）"></a>2.15 Python中的广播机制（Broadcasting in Python）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/19.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-16-关于-Python与numpy向量的使用"><a href="#2-16-关于-Python与numpy向量的使用" class="headerlink" title="2.16 关于 Python与numpy向量的使用"></a>2.16 关于 Python与numpy向量的使用</h4><p><code> a = np.random.randn(5)</code></p>
<blockquote>
<p>a.shape &#x3D; (5, )</p>
</blockquote>
<p>这是numpy的特殊格式”rank 1 array”，<code>a.T</code> 操作仍然得到这种格式的数组。</p>
<p>在神经网络编程中，避免使用这种秩为1的数组。</p>
<p>用 <code> a = np.random.randn(5, 1)</code> 作为替代。此时就可以用 <code>np.dot(a, a.T)</code> 得到一个矩阵了。</p>
<p>也可以用 <code>assert</code> 或 <code>reshape</code> 修改为矩阵格式。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/20.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-17-Jupyter-x2F-iPython-Notebooks快速入门"><a href="#2-17-Jupyter-x2F-iPython-Notebooks快速入门" class="headerlink" title="2.17 Jupyter&#x2F;iPython Notebooks快速入门"></a>2.17 Jupyter&#x2F;iPython Notebooks快速入门</h4><p>略</p>
<h4 id="2-18-逻辑回归损失函数详解"><a href="#2-18-逻辑回归损失函数详解" class="headerlink" title="2.18* 逻辑回归损失函数详解"></a>2.18* 逻辑回归损失函数详解</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/21.png' width="80%" height="80%"/ loading="lazy">



<p>在整个数据集上的情况：假设样本是独立同分布，可以累乘，做最大似然估计使这个式子取最大值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/22.png' width="80%" height="80%"/ loading="lazy">





<h3 id="第三周：浅层神经网络（Shallow-Neural-Networks）"><a href="#第三周：浅层神经网络（Shallow-Neural-Networks）" class="headerlink" title="第三周：浅层神经网络（Shallow Neural Networks）"></a>第三周：浅层神经网络（Shallow Neural Networks）</h3><h4 id="3-1-神经网络概述"><a href="#3-1-神经网络概述" class="headerlink" title="3.1 神经网络概述"></a>3.1 神经网络概述</h4><p>正向传播，计算损失函数 $L$ ；反向传播，计算梯度下降需要的 $dW$、$db$ 。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/23.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-2-神经网络的表示"><a href="#3-2-神经网络的表示" class="headerlink" title="3.2 神经网络的表示"></a>3.2 神经网络的表示</h4><p>$ a^{[i]}$ 表示第 $i$ 层的激活值，$a^{[0]}&#x3D;X$</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/24.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-3-计算一个神经网络的输出"><a href="#3-3-计算一个神经网络的输出" class="headerlink" title="3.3 计算一个神经网络的输出"></a>3.3 计算一个神经网络的输出</h4><p>向量化的前向传播</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/25.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-4-多样本向量化"><a href="#3-4-多样本向量化" class="headerlink" title="3.4 多样本向量化"></a>3.4 多样本向量化</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/26.png' width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/27.png' width="80%" height="80%"/ loading="lazy">

<p>$X,Z,A$ 都是按列组合起来的矩阵。</p>
<h4 id="3-5-向量化实现的解释"><a href="#3-5-向量化实现的解释" class="headerlink" title="3.5 向量化实现的解释"></a>3.5 向量化实现的解释</h4><p>$Z^{[1]}$ 的每一列都是一个训练样本 $X_i$ 经过 $W^{[1]}$ 计算而来的。</p>
<p>当处理多个训练样本时，$X$ 是列向量拼起来的形式，则 $Z$ 也是 $X$ 的每一列的计算结果。</p>
<p>把 $b$ 加上也一样，可能用到广播机制。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/28.png' width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/29.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-6-激活函数（Activation-functions）"><a href="#3-6-激活函数（Activation-functions）" class="headerlink" title="3.6 激活函数（Activation functions）"></a>3.6 激活函数（Activation functions）</h4><p>不同层的激活函数可以不一样。在隐藏层中，tanh函数效果比sigmoid好；但在输出层，二分类任务用sigmoid比较好，因为输出是0~1.</p>
<p>tanh和sigmoid函数的问题是：当x很大或很小，函数的梯度约为0，会拖慢梯度下降。因此在隐藏层用ReLU函数更好。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/30.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-7-为什么需要非线性激活函数？"><a href="#3-7-为什么需要非线性激活函数？" class="headerlink" title="3.7 为什么需要非线性激活函数？"></a>3.7 为什么需要非线性激活函数？</h4><p>如果不用非线性激活函数，神经网络的输出就是 $X$ 的线性组合。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/31.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-8-激活函数的导数"><a href="#3-8-激活函数的导数" class="headerlink" title="3.8 激活函数的导数"></a>3.8 激活函数的导数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/30.png' width="80%" height="80%"/ loading="lazy">

<p>Sigmoid：</p>
<p>$$a &#x3D; g(z) &#x3D; \frac{1}{1+e^{-z}}$$</p>
<p>$$g’(z)&#x3D;\frac{d}{dz}g(z) &#x3D; g(z)(1-g(z))&#x3D;a(1-a)$$</p>
<p>tanh：</p>
<p>$$g(z)&#x3D;\frac{e^z-e^{-z}}{e^z+e^{-z}}$$</p>
<p>$$g’(z)&#x3D;\frac{d}{dz}g(z) &#x3D; 1-(g(z))^2$$</p>
<p>ReLU：在0处可以指定导数的值</p>
<p>$$g(z)&#x3D;max(0, z)$$</p>
<p>$$ g’(z)&#x3D; \begin{cases}  0,&amp;\text{if } z&lt;0 \ 1,&amp;\text{if } z≥0 \end{cases} $$</p>
<p>Leaky ReLU：</p>
<p>$$g(z)&#x3D;max(0.01z, z)$$</p>
<p>$$ g’(z)&#x3D; \begin{cases}  0.01,&amp;\text{if } z&lt;0 \ 1,&amp;\text{if } z≥0 \end{cases} $$</p>
<h4 id="3-9-神经网络的梯度下降"><a href="#3-9-神经网络的梯度下降" class="headerlink" title="3.9 神经网络的梯度下降"></a>3.9 神经网络的梯度下降</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/32.png' width="80%" height="80%"/ loading="lazy">

<p>前向传播过程：</p>
<blockquote>
<p>$Z^{[1]} &#x3D; W^{[1]}X + b^{[1]}$</p>
<p>$A^{[1]} &#x3D; g^{[1]}(Z^{[1]})$</p>
<p>$Z^{[2]} &#x3D; W^{[2]}A^{[1]} + b^{[2]}$</p>
<p>$A^{[2]} &#x3D; g^{[2]}(Z^{[2]})$</p>
</blockquote>
<p>反向传播过程：</p>
<blockquote>
<p>$dZ^{[2]}&#x3D;A^{[2]}-Y$</p>
<p>$dW^{[2]}&#x3D;\frac{1}{m} dZ^{[2]} (A^{[1]} )^T$</p>
<p>$db^{[2]} &#x3D; \frac{1}{m}np.sum(dZ^{[2]}, axis&#x3D;1, keepdims&#x3D;ture)$ </p>
<p>$dZ^{[1]} &#x3D;( W^{[2]})^T  dZ^{[2]} * g^{‘[1]}(Z^{[1]})$</p>
<p>$dW^{[1]}&#x3D;\frac{1}{m} dZ^{[1]} X^T$</p>
<p>$db^{[1]} &#x3D; \frac{1}{m}np.sum(dZ^{[1]}, axis&#x3D;1, keepdims&#x3D;ture)$</p>
</blockquote>
<h4 id="3-10（选修）直观理解反向传播（Backpropagation-intuition）"><a href="#3-10（选修）直观理解反向传播（Backpropagation-intuition）" class="headerlink" title="3.10（选修）直观理解反向传播（Backpropagation intuition）"></a>3.10（选修）直观理解反向传播（Backpropagation intuition）</h4><p>想一想矩阵的维度。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/33.png' width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/34.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-11-随机初始化（Random-Initialization）"><a href="#3-11-随机初始化（Random-Initialization）" class="headerlink" title="3.11 随机初始化（Random Initialization）"></a>3.11 随机初始化（Random Initialization）</h4><p>不能用全0初始化神经网络，这会导致神经元的对称性和反向传播失效。</p>
<p>随机初始化：用很小的随机数初始化 $W$ ，用0初始化 $b$ .</p>
<p>如果 $W$ 的值太大，$z$ 会落在tanh函数和sigmoid函数平缓的部分，梯度下降会很慢。如果完全没有用到这两个函数就没有影响。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/35.png' width="80%" height="80%"/ loading="lazy">

<p>训练浅层神经网络，0.01是可用的；当训练深层神经网络，要使用其他的常数，在之后讲。</p>
<h3 id="第四周：深层神经网络"><a href="#第四周：深层神经网络" class="headerlink" title="第四周：深层神经网络"></a>第四周：深层神经网络</h3><h4 id="4-1-深层神经网络"><a href="#4-1-深层神经网络" class="headerlink" title="4.1 深层神经网络"></a>4.1 深层神经网络</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/36.png' width="80%" height="80%"/ loading="lazy">

<p>$L$：层数，从0开始计数。</p>
<p>$n^{[l]}$：$l$ 层的神经元个数。</p>
<p>图中 $L&#x3D;4$ ，$n^{[L]}&#x3D;1$，$n^{[1]} &#x3D; n^{[2]} &#x3D;5$ 。</p>
<h4 id="4-2-前向传播"><a href="#4-2-前向传播" class="headerlink" title="4.2 前向传播"></a>4.2 前向传播</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/37.png' width="80%" height="80%"/ loading="lazy">

<p>基本过程：</p>
<ul>
<li>$z^{[l]} &#x3D; w^{[l]} a^{[l-1]} +b^{[l]}$ </li>
<li>$a^{[l]} &#x3D; g(z^{[l]})$</li>
</ul>
<p>向量化见图右下方。</p>
<p>在前向传播的实现过程中，需要使用显示的for循环，来遍历从输入层到输出层。</p>
<h4 id="4-3-检查矩阵的维数"><a href="#4-3-检查矩阵的维数" class="headerlink" title="4.3 检查矩阵的维数"></a>4.3 检查矩阵的维数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/38.png' width="80%" height="80%"/ loading="lazy">

<p>同样本排列成一列（如 $(x_1,x_2)^T$、$(z_1,z_2)^T$），不同的样本m纵向组合起来（如 $(A[0],A[1])$、$(Z[1],Z[2])$）。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/39.png' width="80%" height="80%"/ loading="lazy">

<p>在向量化的场合，python的broadcasting机制把 $b[1]$ 维度 $(n^{[1]},1)$ 扩展成 $(n^{[1]},m)$。</p>
<h4 id="4-4-为什么使用深层表示？"><a href="#4-4-为什么使用深层表示？" class="headerlink" title="4.4 为什么使用深层表示？"></a>4.4 为什么使用深层表示？</h4><p>神经网络可以不用很大，但深层有好处。</p>
<p>在直觉层面理解，深层神经网络能组合从简单到复杂的信息。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/40.png' width="80%" height="80%"/ loading="lazy">



<p>另一种直觉理解，从电路角度，用小规模但深层的电路结构，可以进行复杂的计算；但用浅层的电路模型，要用指数级增长的运算单元才能实现相同的功能。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/41.png' width="80%" height="80%"/ loading="lazy">



<h4 id="4-5-搭建深层神经网络块"><a href="#4-5-搭建深层神经网络块" class="headerlink" title="4.5 搭建深层神经网络块"></a>4.5 搭建深层神经网络块</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/42.png' width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/43.png' width="80%" height="80%"/ loading="lazy">



<h4 id="4-6-前向和反向传播"><a href="#4-6-前向和反向传播" class="headerlink" title="4.6 前向和反向传播"></a>4.6 前向和反向传播</h4><p>前向传播的实现：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/44.png' width="80%" height="80%"/ loading="lazy">

<p>反向传播的实现：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/45.png' width="80%" height="80%"/ loading="lazy">



<h4 id="4-7-参数VS超参数（Parameters-vs-Hyperparameters）"><a href="#4-7-参数VS超参数（Parameters-vs-Hyperparameters）" class="headerlink" title="4.7 参数VS超参数（Parameters vs Hyperparameters）"></a>4.7 参数VS超参数（Parameters vs Hyperparameters）</h4><p>Parameters: W, b</p>
<p>Hyperparameters:</p>
<ul>
<li>learning rate(α), #iterations, #hidden layers(L), #hidden units(n), choice of activation function.</li>
<li>momentum, mini-batch size, regularization parameters, …</li>
</ul>
<p>尝试不同的超参数值，找到合适的值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/46.png' width="80%" height="80%"/ loading="lazy">



<h4 id="4-8-深度学习和人类大脑的关联性"><a href="#4-8-深度学习和人类大脑的关联性" class="headerlink" title="4.8 深度学习和人类大脑的关联性"></a>4.8 深度学习和人类大脑的关联性</h4><p>目前对人脑的认识没有达到建立数学模型的程度。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/47.png' width="80%" height="80%"/ loading="lazy">







<h2 id="第二课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization"><a href="#第二课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization" class="headerlink" title="第二课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks : Hyperparameter tuning, Regularization and Optimization)"></a>第二课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks : Hyperparameter tuning, Regularization and Optimization)</h2><h3 id="第一周：深度学习的实用层面"><a href="#第一周：深度学习的实用层面" class="headerlink" title="第一周：深度学习的实用层面"></a>第一周：深度学习的实用层面</h3><blockquote>
<p>深度学习的应用方法。数据集的划分；偏差&#x2F;方差；通过正则化来防止过拟合（包括L1L2，dropout，其他方法如数据增强和提前停止）；输入归一化；通过合理的权重初始化来避免梯度消失和梯度爆炸；进行梯度检验确保梯度下降算法正确运行。最后一节讲了以上方法的实践经验。</p>
</blockquote>
<h4 id="1-1-训练-x2F-验证-x2F-测试集（Train-x2F-Dev-x2F-Test-sets）"><a href="#1-1-训练-x2F-验证-x2F-测试集（Train-x2F-Dev-x2F-Test-sets）" class="headerlink" title="1.1 训练&#x2F;验证&#x2F;测试集（Train &#x2F; Dev &#x2F; Test sets）"></a>1.1 训练&#x2F;验证&#x2F;测试集（Train &#x2F; Dev &#x2F; Test sets）</h4><p>在训练集进行训练，根据在验证集上的得分选择最好的模型，在测试集上进行评估。</p>
<p>在数据集很大的情况下，可以把验证集、测试集划分得少一点。在百万条数据的情况下，甚至可以划分99.5%&#x2F;0.25%&#x2F;0.25%。</p>
<ul>
<li><p>注意1：<strong>保证验证集和测试集的数据来自同一分布</strong>。</p>
<p>如：训练集是网站上比较精美、清晰的图片；验证集、训练集是用户随手拍的图片。</p>
</li>
<li><p>注意2：不做测试集也可以。如果不需要对最终的神经网络做无偏评估，也可以不设置测试集。</p>
</li>
</ul>
<h4 id="1-2-偏差-x2F-方差（Bias-x2F-Variance）"><a href="#1-2-偏差-x2F-方差（Bias-x2F-Variance）" class="headerlink" title="1.2 偏差&#x2F;方差（Bias &#x2F;Variance）"></a>1.2 偏差&#x2F;方差（Bias &#x2F;Variance）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/48.png' width="80%" height="80%"/ loading="lazy">

<p>前提：基本error很低；验证集和测试集来自同一分布。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/49.png' width="80%" height="80%"/ loading="lazy">

<p>训练集的error要跟基本error比，基本error通常是人工识别的error。</p>
<h4 id="1-3-先后顺序"><a href="#1-3-先后顺序" class="headerlink" title="1.3 先后顺序"></a>1.3 先后顺序</h4><p>按步骤确认：</p>
<ol>
<li><p>high bias？ 增大网络规模、训练更长时间、（修改网络结构）</p>
</li>
<li><p>high variance？ 获得更多数据、正则化、（修改网络结构）</p>
</li>
<li><p>完成，获得 low bias &amp; variance 的模型。</p>
</li>
</ol>
<p>在现在深度学习、大数据的环境中，可以做到在减小bias或variance的过程中，不对另一方产生过多不良影响。我们不用太过关注如何 tradeoff。</p>
<h4 id="1-4-正则化（Regularization）"><a href="#1-4-正则化（Regularization）" class="headerlink" title="1.4 正则化（Regularization）"></a>1.4 正则化（Regularization）</h4><p>逻辑回归的正则化：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/50.png' width="80%" height="80%"/ loading="lazy">

<p>如果用的是L1正则化，W最终会是稀疏的，也就是W向量中有很多0。</p>
<p>现在更倾向于L2正则化。</p>
<p>$\lambda$ 也是一个需要调整的超参数。为了防止与python的关键字重复，在代码中一般写作lambd。 </p>
<p>神经网络的正则化：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/51.png' width="80%" height="80%"/ loading="lazy">

<p>由于历史原因，不叫矩阵的L2正则化，而是叫 frobenius norm。</p>
<p>在反向传播过程中，正则化项求导后加在 $dW$ 的后面，让梯度下降的幅度大一些。也被称为 weight decay 。</p>
<h4 id="1-5-为什么正则化有利于预防过拟合呢？"><a href="#1-5-为什么正则化有利于预防过拟合呢？" class="headerlink" title="1.5 为什么正则化有利于预防过拟合呢？"></a>1.5 为什么正则化有利于预防过拟合呢？</h4><p>从直观上理解，正则化项降低了 $W$ 的值，也就是降低了一些神经元的作用，简化了模型，让模型从过拟合向欠拟合发展。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/52.png' width="80%" height="80%"/ loading="lazy">



<p>第二种直观理解方法：$W$ 值变小，$z$ 集中在激活函数的线性部分，则模型的每一层都相当于线性变换，模型不适用于复杂的决策，降低了过拟合程度。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/53.png' width="80%" height="80%"/ loading="lazy">

<p>如果实施了带正则化项的损失函数，当使用梯度下降法时，为了调试梯度下降，要使用这个新定义的损失函数，否则损失函数可能不会再所有的调幅范围内都单调递减。</p>
<h4 id="1-6-dropout-正则化"><a href="#1-6-dropout-正则化" class="headerlink" title="1.6 dropout 正则化"></a>1.6 dropout 正则化</h4><p>对每个训练样本，遍历神经网络的每一层，并设置消除神经网络中节点的概率，消除一些节点，得到一个更小规模的神经网络，训练这个精简后的网络。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/54.png' width="80%" height="80%"/ loading="lazy">

<p> 一种实现方法：inverted dropout（反向随机失活）</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/55.png' width="80%" height="80%"/ loading="lazy">

<p>用不等式给d赋值为true或false，跟a相乘让a的一部分值失效。</p>
<p>有一个 <code>a/=deep_prob</code> 操作， 修正或弥补丢掉的一部分数据，让a的期望值不变。</p>
<p>在测试阶段，不使用dropout。</p>
<h4 id="1-7-理解-dropout"><a href="#1-7-理解-dropout" class="headerlink" title="1.7 理解 dropout"></a>1.7 理解 dropout</h4><p>直观上理解，dropout让神经元不依赖于某一个特征，而让权重更加分散。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/56.png' width="80%" height="80%"/ loading="lazy">

<p>如果更担心在某些层有过拟合，就把某些层的keep-prob设置得低一些。缺点是在验证集上调参工作量增大。</p>
<p>dropout本质上是一种正则化方法，用来防止过拟合。在计算机视觉问题中，输入的像素很多，以至于没有足够的数据，经常一直处于过拟合情况。因此dropout在CV应用的比较频繁。在其他领域，如果没有过拟合问题就不必使用。</p>
<p>dropout一大缺点就是代价函数 $J$ 不再被明确定义。每次迭代都随机保留神经元，很难对每次的反向传播梯度下降进行复查。也就失去了绘制递减的代价函数图像的工具。通常先关闭dropout，运行代码确保代价函数单调递减，再开启dropout。</p>
<h4 id="1-8-其他正则化方法"><a href="#1-8-其他正则化方法" class="headerlink" title="1.8 其他正则化方法"></a>1.8 其他正则化方法</h4><p>data augment</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/57.png' width="80%" height="80%"/ loading="lazy">



<p>early stopping</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/58.png' width="80%" height="80%"/ loading="lazy">

<p>建立模型的两个过程：其一是让 $J(w, b)$ 取到最小值，手段包括梯度下降等；其二是防止过拟合，又称为orthogonalization，手段包括正则化等。early stopping 的缺点是破坏了这两个过程相互的独立性。提前结束训练过程，也就是打断了第一个过程。</p>
<p>如果使用L2正则化，就避免了这个缺点，随之而来的是 $\lambda$ 的调参工作量，而不是只进行一次梯度下降就可以找到early stopping的位置。</p>
<h4 id="1-9-归一化输入（Normalizing-inputs）"><a href="#1-9-归一化输入（Normalizing-inputs）" class="headerlink" title="1.9 归一化输入（Normalizing inputs）"></a>1.9 归一化输入（Normalizing inputs）</h4><p>第一步：零均值化；第二步：方差归一化。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/59.png' width="80%" height="80%"/ loading="lazy">

<p>注意：在训练集和测试集上要用相同的 $\mu,\sigma$ 。</p>
<p>这样做的原因：让优化变快。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/60.png' width="80%" height="80%"/ loading="lazy">



<h4 id="1-10-梯度消失-x2F-梯度爆炸（Vanishing-x2F-Exploding-gradients）"><a href="#1-10-梯度消失-x2F-梯度爆炸（Vanishing-x2F-Exploding-gradients）" class="headerlink" title="1.10 梯度消失&#x2F;梯度爆炸（Vanishing &#x2F; Exploding gradients）"></a>1.10 梯度消失&#x2F;梯度爆炸（Vanishing &#x2F; Exploding gradients）</h4><p>activations以指数级增长或下降，给梯度下降造成困难。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/61.png' width="80%" height="80%"/ loading="lazy">

<p>以图中简化 $b$ 、$w$ 全部是对角矩阵的神经网络为例：$w$ 比单位矩阵大一点，激活值以指数级增长；w 比单位矩阵小一点，激活值以指数级减小。</p>
<h4 id="1-11-神经网络的权重初始化"><a href="#1-11-神经网络的权重初始化" class="headerlink" title="1.11 神经网络的权重初始化"></a>1.11 神经网络的权重初始化</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/62.png' width="80%" height="80%"/ loading="lazy">

<p>通过给 $W$ 设置合理的初始值（不能比1大&#x2F;小太多），避免梯度消失和梯度爆炸。</p>
<p>以图中去掉 $b$ 的单个神经元为例，最合理的方式是设置 $w$ 接近 $\frac{1}{n}$ 。</p>
<p>因此进行这样的初始化：$W^{[l]} &#x3D; np.random.randn(shape)*np.sqrt(\frac{2}{n^{[l-1]]}})$</p>
<p>当用ReLU函数，是 $\sqrt{\frac{2}{n^{[l-1]]}}}$ ；当用tanh函数，是 $\sqrt{\frac{1}{n^{[l-1]]}}}$；也有人用 $\sqrt{\frac{2}{n^{[l-1]]}+n^{[l]}}}$ 。</p>
<h4 id="1-12-梯度的数值近似"><a href="#1-12-梯度的数值近似" class="headerlink" title="1.12 梯度的数值近似"></a>1.12 梯度的数值近似</h4><p>在实施反向传播时，进行gradient checking，可以确保反向传播正在正确进行。</p>
<p>用 $\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}\approx g(\theta)$ 近似计算 $\theta$ 的梯度 $g(\theta)$。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/63.png' width="80%" height="80%"/ loading="lazy">



<h4 id="1-13-梯度检验（Gradient-checking）"><a href="#1-13-梯度检验（Gradient-checking）" class="headerlink" title="1.13 梯度检验（Gradient checking）"></a>1.13 梯度检验（Gradient checking）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/64.png' width="80%" height="80%"/ loading="lazy">

<p>把所有层的$w,b$ 组合成矩阵 $\theta$，所有层的$dW,db$ 组合成矩阵 $d\theta$ 。我们需要验证：$d\theta$ 是 $\theta$ 的梯度。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/65.png' width="80%" height="80%"/ loading="lazy">

<p>计算近似梯度：</p>
<p>$$ d\theta_{approx}[i] &#x3D;\frac{J(\theta_1, \theta_2,…,\theta_i+\epsilon,…)-J(\theta_1, \theta_2,…,\theta_i-\epsilon,…)}{2\epsilon} \approx d\theta[i] &#x3D; \frac{\partial J}{\partial \theta_i}$$</p>
<p>$$check: \frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{approx}||_2+||d\theta||_2} \approx 10^{-7}$$</p>
<p>如果 $\approx10^{-5}$，检查向量，确保没有一项误差过大，确保没有bug；如果 $\approx10^{-3}$，需要小心有bug。可以检查哪一项的导数计算结果和估计值偏差很大，并反推求导过程，检查bug。</p>
<h4 id="1-14-应用梯度检验的注意事项"><a href="#1-14-应用梯度检验的注意事项" class="headerlink" title="1.14 应用梯度检验的注意事项"></a>1.14 应用梯度检验的注意事项</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/65-add.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li>不要在训练过程中使用梯度检验，只用于调试。</li>
<li>如果梯度检验失败，检查哪一项的导数计算结果和估计值偏差很大，确定bug位置，比如在某一层的求导结果跟估计值差很大。</li>
<li>梯度检验的过程中，如果使用了正则化，要记住计算中应包括正则化项。</li>
<li><strong>梯度检验不能与dropout一起使用</strong>。dropout让我们难以计算 $J$ 。可以先把 keep_prob 设置为1，验证梯度下降是正确的；再开启dropout.</li>
<li>几乎不会出现的情况：随机初始化 $w,b$ 接近0，梯度下降的实施是正确的，但在运行梯度下降时，$w,b$ 变大，可能只有在 $w,b$ 接近0时，梯度下降才是正确的，但 $w,b$ 变大时它变得越来越不准确。做法（基本不用）：在随机初始化过程中进行梯度检验，然后再训练网络，如果随机初始化值比较小，$w,b$ 会有一段时间远离0 ；反复训练网络之后再重新进行梯度检验。（开始做一下梯度检验，训练后再进行一次梯度检验，保证正确。）</li>
</ul>
<h3 id="第二周：优化算法-Optimization-algorithms"><a href="#第二周：优化算法-Optimization-algorithms" class="headerlink" title="第二周：优化算法 (Optimization algorithms)"></a>第二周：优化算法 (Optimization algorithms)</h3><p>让梯度下降加速的优化方法。包括mini-batch梯度下降、momentum&#x2F;RMSprop&#x2F;Adam算法（和需要了解的指数加权平均、偏差修正基础）、学习率衰减。最后一节讲了局部最优问题。</p>
<h4 id="2-1-Mini-batch-梯度下降"><a href="#2-1-Mini-batch-梯度下降" class="headerlink" title="2.1 Mini-batch 梯度下降"></a>2.1 Mini-batch 梯度下降</h4><p>batch梯度下降：在整个数据集上进行梯度下降。</p>
<p>mini-batch梯度下降：把整个数据集划分为若干个mini-batch，在每个mini-batch上进行一次梯度下降。</p>
<p>实现过程：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/66.png' width="80%" height="80%"/ loading="lazy">

<p>前向传播、求平均损失 –&gt; 反向传播、梯度下降。在每个mini-batch中进行这样的操作。</p>
<p>完整的遍历一次训练集称为 1 epoch。mini-batch可以在 1 epoch 中完成多次梯度下降。</p>
<h4 id="2-2-理解Mini-batch-梯度下降"><a href="#2-2-理解Mini-batch-梯度下降" class="headerlink" title="2.2 理解Mini-batch 梯度下降"></a>2.2 理解Mini-batch 梯度下降</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/67.png' width="80%" height="80%"/ loading="lazy">

<p>mini-batch会让梯度下降有噪声，但最终也会收敛到比较小的水平。</p>
<p>mini-batch梯度下降的优势：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/68.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>mini-batch size &#x3D; m，即 batch 梯度下降：步长大，噪声少。单次迭代耗时长。</p>
</li>
<li><p>mini-batch size &#x3D; 1，即 随机 梯度下降：步长小，噪声多，永远不收敛（在最小值附近波动。失去向量化方法带来的计算加速。</p>
</li>
<li><p>mini-batch梯度下降：既能对样本进行向量化，又能快速迭代。</p>
</li>
</ul>
<p>选择 mini-batch size 的注意事项：</p>
<ul>
<li><p>样本集小（&lt;2000），直接用batch梯度下降。</p>
</li>
<li><p>一般把 mini-batch size 设置为2的次方数。</p>
</li>
<li><p>确保mini-batch内的数据 $(X^,Y^)$ 符合 CPU&#x2F;GPU 的内存。</p>
</li>
<li><p>这也是一个 hyperparameter ，需要多次尝试，找到让梯度下降最高效的取值。</p>
</li>
</ul>
<p>还有比梯度下降和mini-batch梯度下降都要高效得多的算法，在后面讲。</p>
<h4 id="2-3-指数加权平均（Exponentially-weighted-averages）"><a href="#2-3-指数加权平均（Exponentially-weighted-averages）" class="headerlink" title="2.3 指数加权平均（Exponentially weighted averages）"></a><span id = "2.2.3">2.3 指数加权平均（Exponentially weighted averages）</span></h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/69.png' width="80%" height="80%"/ loading="lazy">

<p>在统计学中叫指数加权移动平均值。</p>
<p>$$V_t &#x3D; \beta V_{t-1} + (1-\beta) \theta_t$$ </p>
<p>$V_t$ 可以看作在 $\frac{1}{1-\beta}$ 天中，温度$\theta$ 的平均值。</p>
<p>通过调整参数 $\beta$ ，获得不同的效果。</p>
<ul>
<li>$\beta$ 大，平均的样本多，曲线平滑但有偏移。（图中绿色线是50天的平均值）</li>
<li>$\beta$ 小，平均的样本少，曲线更拟合，但噪声大。（图中黄色线是2天的平均值，红色线是10天的平均值）</li>
</ul>
<h4 id="2-4-理解指数加权平均"><a href="#2-4-理解指数加权平均" class="headerlink" title="2.4 理解指数加权平均"></a>2.4 理解指数加权平均</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/70.png' width="80%" height="80%"/ loading="lazy">

<p>把算式展开，是sum(每天的温度×指数衰减系数)的形式。（右上方两个图中对应值相乘）</p>
<p>$$V_{100} &#x3D; 0.1\times\theta_{100}+0.1\times 0.9\times\theta_{99}+0.1\times 0.9\times 0.9\times\theta_{98}+ …$$</p>
<p>所有系数加起来近似＝1。</p>
<p>有 $(1-\epsilon)^{\frac{1}{\epsilon}}\approx \frac{1}{e}$ ，在此时权重系数衰减的下降幅度很大。因此可以近似认为，今天的 $V$ 的值是取了前 $\frac{1}{\epsilon} &#x3D; \frac{1}{1-\beta}$ 天的平均值。如图中的 $V$ 是取了 $\frac{1}{0.1}&#x3D;10$ 天的温度平均值。</p>
<p>实现：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/71.png' width="80%" height="80%"/ loading="lazy">

<p>在实现上，只需要存储单个变量 $V$ 并且不断更新即可。$V$ 近似了平均值，省去了使用滑动窗口求和求精确平均值所需的存储空间。</p>
<p>在之后的章节中，需要计算多个变量的平均值，使用指数加权平均是一个好的近似计算方法。</p>
<h4 id="2-5-指数加权平均的偏差修正（Bias-correction-in-exponentially-weighted-average）"><a href="#2-5-指数加权平均的偏差修正（Bias-correction-in-exponentially-weighted-average）" class="headerlink" title="2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted average）"></a>2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted average）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/72.png' width="80%" height="80%"/ loading="lazy">

<p>以 $\beta&#x3D;0.98$ 为例，在实际实现上，会得到紫线而不是绿线。以为初始化 $V&#x3D;0$ ，前几项会很小。（见左下算式）</p>
<p>使用偏差修正，让平均值近似计算更加准确：用 $\frac{V_t}{1-\beta^t}$ 代替 $V_t$ 。在刚开始 $t$ 较小时，$\frac{V_t}{1-\beta^t}$ 求的是 $\theta$ 的加权平均数（右下表达式）；$t$ 变大时，$\beta^t$ 接近 0。</p>
<p>偏差修正能在早期获得更好的估计，但也可以选择熬过初始时期，不使用偏差修正。</p>
<h4 id="2-6-momentum梯度下降"><a href="#2-6-momentum梯度下降" class="headerlink" title="2.6 momentum梯度下降"></a>2.6 momentum梯度下降</h4><p>计算梯度的指数加权平均数，加速梯度下降。这个方法好于普通梯度下降。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/73.png' width="80%" height="80%"/ loading="lazy">

<p>梯度下降的波动，要求我们不能用很大的学习率。在纵轴上，我们希望学习慢一点；在横轴上，希望学习快一点。</p>
<p>平均了这些梯度之后，会发现纵轴上的摆动平均值接近 0（图中红箭头），可以采用大一些的学习率了。</p>
<p>一个直观上的理解：小球从碗状函数像底部滚动，微分项 $dw,db$ 是加速度，momentum项 $V_{dw},V_{db}$ 是速度，球加速向底部滚动，而 $\beta$ 相当于摩擦力，让小球不会无限加速。不像梯度下降法每一步都独立于之前的步骤，现在小球可以向下滚，获得动量（momentum）。</p>
<p>实现：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/74.png' width="80%" height="80%"/ loading="lazy">

<p>$\beta$ 的常用值是 0.9，即平均前十次迭代的梯度。同时也可以不使用偏差修正 $\frac{V_t}{1-\beta^t}$ ，因为10次后已经可以正常近似了。</p>
<p>也有资料将 $(1-\beta)$ 忽略，使用右边的式子，这两者在效果上是相同的，只是会影响到 $\alpha$ 的最佳值。老师认为左边的计算方法更符合直觉，因为如果要调整超参数 $\beta$，就会影响到 $V_{dw}$ 和 $V_{db}$ ，也许还要修改 $\alpha$。</p>
<h4 id="2-7-RMSprop-root-mean-square-prop"><a href="#2-7-RMSprop-root-mean-square-prop" class="headerlink" title="2.7 RMSprop-root mean square prop"></a>2.7 RMSprop-root mean square prop</h4><p>另一种消除梯度下降的摆动，加快梯度下降的方法。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/75.png' width="80%" height="80%"/ loading="lazy">

<p>当在某个方向波动大（如图中举例 $db$ ，在梯度下降减去一个分母较大的数 $b:&#x3D; b-\alpha \frac{db}{\sqrt{db}}$，让梯度下降的幅度减小。在某个方向梯度下降幅度小（如图中举例 $dw$ ，在梯度下降减去一个较小的数 $w:&#x3D; w-\alpha \frac{dw}{\sqrt{dw}}$，让梯度下降的幅度增大。</p>
<p>其他：为了跟momentum结合起来，将RMSprop的超参数命名为 $\beta_2$ ；防止除以0，在分母加上很小的数 $\epsilon &#x3D; 10^{-8}$ 。</p>
<h4 id="2-8-Adam优化算法"><a href="#2-8-Adam优化算法" class="headerlink" title="2.8 Adam优化算法"></a>2.8 Adam优化算法</h4><p>把momentum和RMSprop组合起来。在不同的模型上都有很好的效果，有很广泛的应用。</p>
<p>Adam算法需要进行偏差修正。</p>
<p>momentum：$w&#x3D;w-\alpha V_{dw}$ </p>
<p>RMSprop：$w &#x3D; w - \alpha \frac{dw}{\sqrt{S_{dw}+\epsilon}}$ </p>
<p>Adam：$w &#x3D; w - \alpha \frac{V_{dw}}{\sqrt{S_{dw}+\epsilon}}$ </p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/76.png' width="80%" height="80%"/ loading="lazy">



<p>几个超参数，当应用adam算法时，$\beta_1,\beta_2,\epsilon$ 常常都是用缺省值，$\alpha$ 需要实验确定。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/77.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-9-学习率衰减（Learning-rate-decay）"><a href="#2-9-学习率衰减（Learning-rate-decay）" class="headerlink" title="2.9 学习率衰减（Learning rate decay）"></a>2.9 学习率衰减（Learning rate decay）</h4><p>随时间慢慢减小学习率。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/78.png' width="80%" height="80%"/ loading="lazy">

<p>一种方法：</p>
<p>$$\alpha &#x3D; \frac{1}{1+decay_rate \times epoch_num}\alpha_{init}$$</p>
<p>decay_rate 是需要调整的超参数。 </p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/79.png' width="80%" height="80%"/ loading="lazy">



<p>其他几种方法：指数衰减、除以epoch_num的开方、离散衰减等。也有看着模型训练过程，然后手动进行衰减的方法。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/80.png' width="80%" height="80%"/ loading="lazy">



<h4 id="2-10-局部最优问题"><a href="#2-10-局部最优问题" class="headerlink" title="2.10 局部最优问题"></a>2.10 局部最优问题</h4><p>在维数很高的情况下，更多的情况是收敛到鞍形部位（鞍点，图右方），而不是局部最优点（图左方）。在鞍点，一些方向的曲线向下弯曲，一些方向的曲线向上弯曲。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/81.png' width="80%" height="80%"/ loading="lazy">

<p>在鞍上称为plateaus问题，这段时间训练得比较慢，使用momentum等算法可以加速此过程。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/82.png' width="80%" height="80%"/ loading="lazy">





<h3 id="第三周：超参数调试，批正则化和程序框架"><a href="#第三周：超参数调试，批正则化和程序框架" class="headerlink" title="第三周：超参数调试，批正则化和程序框架"></a>第三周：超参数调试，批正则化和程序框架</h3><p>调参的基本规则和方法；batch norm让学习算法运行速度更快；softmax回归；深度学习框架。</p>
<h4 id="3-1-调参规则"><a href="#3-1-调参规则" class="headerlink" title="3.1 调参规则"></a>3.1 调参规则</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/83.png' width="50%" height="50%"/ loading="lazy">

<p>调参重要性排序：红 &gt; 黄 &gt; 紫。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/84.png' width="80%" height="80%"/ loading="lazy">

<p>在深度学习中，不要用网格取值进行实验（图左）。</p>
<p>应该随机取超参数的值并进行实验（图右）。因为不知道哪个超参数是更重要的，需要探究重要的超参数的更多潜在值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/85.png' width="80%" height="80%"/ loading="lazy">

<p>使用从粗略到精细（coarse to fine）的策略。在表现好的区域上进行更密集的取值尝试</p>
<h4 id="3-2-合适的参数取值范围"><a href="#3-2-合适的参数取值范围" class="headerlink" title="3.2 合适的参数取值范围"></a>3.2 合适的参数取值范围</h4><p>有些超参数，可以在合理的范围内，在<strong>线性轴</strong>上，做随机均匀取值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/86.png' width="80%" height="80%"/ loading="lazy">



<p>学习率等超参数，更合适的方法是在<strong>对数轴</strong>上均匀随机取值。</p>
<pre class="language-python" data-language="python"><code class="language-python">r <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">4</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">)</span>
alpha <span class="token operator">=</span> exp<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> r<span class="token punctuation">)</span></code></pre>

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/87.png' width="80%" height="80%"/ loading="lazy">





<p>在 $1-\beta$ 取值，而不是在 $\beta$ 取值。因为在 $\beta$ 越接近 1，平均的样本个数有更大的变化，需要更密集的取值。所以在 $1-\beta$ 接近 0 时进行更密集的取值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/88.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-3-超参数训练的实践：Pandas-vs-Caviar"><a href="#3-3-超参数训练的实践：Pandas-vs-Caviar" class="headerlink" title="3.3 超参数训练的实践：Pandas vs. Caviar"></a>3.3 超参数训练的实践：Pandas vs. Caviar</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/89.png' width="80%" height="80%"/ loading="lazy">

<p>在不同领域的参数设置可能有相似的部分，多了解其他工作；多进行尝试。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/90.png' width="80%" height="80%"/ loading="lazy">

<p>一种方法：在训练中照看（babysitting）模型，比如进行学习率的调整。</p>
<p>另一种方法：同时训练超参数取值不同的多个模型。</p>
<h4 id="3-4-激活函数的归一化-x2F-单一隐藏层上的批归一化（Batch-normalization）"><a href="#3-4-激活函数的归一化-x2F-单一隐藏层上的批归一化（Batch-normalization）" class="headerlink" title="3.4 激活函数的归一化&#x2F;单一隐藏层上的批归一化（Batch normalization）"></a>3.4 激活函数的归一化&#x2F;单一隐藏层上的批归一化（Batch normalization）</h4><p>batch normalization 会使参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/91.png' width="80%" height="80%"/ loading="lazy">

<p>对逻辑回归、神经网络的输入归一化而言，进行输入特征值的归一化是有效的。如图的上半部分，对 $x_1,x_2,x_3$ 进行归一化对 $w,b$ 的训练有帮助。</p>
<p>同样的思想：对深层的模型，能否对 $a^{[i]}$ 进行归一化，改进 $w^{[i+1]},b^{[i+1]}$ 的训练？</p>
<p>在实践中，我们不对 $a^{[i]}$ 做归一化，而是对 $z^{[i]}$ 做归一化。这一点一直有争论。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/92.png' width="80%" height="80%"/ loading="lazy">

<p>实现：</p>
<p>$$z_{norm}^{(i)} &#x3D; \frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}$$ </p>
<p>$$\widetilde{z}^{(i)} &#x3D; \gamma z_{norm}^{(i)} + \beta$$ </p>
<ul>
<li><p>$\gamma,\beta$ 是可以学习的参数（不是超参数）。如果 $\gamma&#x3D;\sqrt{\sigma^2+\epsilon},\beta&#x3D;\mu$，则 $\widetilde{z}^{(i)} &#x3D; {z}^{(i)}$，batch normalization不起作用。 $\gamma$ 控制方差，$\beta$ 值控制均值。通过给它们赋值，可以构造含平均值和方差的隐藏单元值。</p>
</li>
<li><p>用  $\widetilde{z}^{(i)}$ 取代 ${z}^{(i)}$ ，参与神经网络的后续计算。</p>
</li>
<li><p>不一定非要归一化成均值为0的分布。可以归一化到均值不是0，方差大一点，符合sigmoid等激活函数的特性。</p>
</li>
<li><p>batch normalization本质上是让隐藏单元值的均值和方差标准化，即 $z^{[i]}$ 有固定的均值和方差，由 $\gamma,\beta$ 两个参数控制。</p>
</li>
</ul>
<h4 id="3-5-深度神经网络的批归一化"><a href="#3-5-深度神经网络的批归一化" class="headerlink" title="3.5 深度神经网络的批归一化"></a>3.5 深度神经网络的批归一化</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/93.png' width="80%" height="80%"/ loading="lazy">

<p>batch norm是发生在计算 $z$ 和 $a$ 之间的。给神经网络添加了新的参数 $\gamma,\beta$ 。（注意，跟momentum等优化算法的超参数 $\beta$ 区分。这两者的论文都使用 $\beta$ 作为参数的名称。）</p>
<p>使用优化算法（如梯度下降或Adam等），对这些参数 $w,b,\gamma,\beta$ 进行更新。</p>
<p>在深度学习框架中，可以用一行代码完成batch norm的操作，无需自己实现。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/94.png' width="80%" height="80%"/ loading="lazy">

<p>batch norm通常和训练集的mini-batch一起使用。在每一个mini-batch上，做一次梯度下降。</p>
<p>在上一段中，提到对参数 $w,b,\gamma,\beta$ 进行更新。但实际的计算步骤为：</p>
<ul>
<li>先计算 $z^{[l]}  &#x3D; w^{[l]}a^{[l-1]}+b^{[l]}$；</li>
<li>然后对 $z^{[l]}$ 进行归一化计算 $z_{norm}^{[l]}$，在此过程中会减去均值， <strong>$b^{[l]}$ 这个加上去的参数是无效的。</strong></li>
<li>用 $\widetilde{z}^{[l]} &#x3D; \gamma^{[l]}z_{norm}^{[l]} + \beta^{[l]}$ 进行后续计算。<strong>形式上， $\beta$ 代替了参数 $b$ 。</strong></li>
</ul>
<p>实际计算步骤：</p>
<ul>
<li>$z^{[l]}  &#x3D; w^{[l]}a^{[l-1]}$ </li>
<li>计算 $z_{norm}^{[l]}$ </li>
<li>$\widetilde{z}^{[l]} &#x3D; \gamma^{[l]}z_{norm}^{[l]} + \beta^{[l]}$</li>
</ul>
<p>此外，注意参数的维度：$z,b,\beta,\gamma$ 维度都是 $(n^{[l]}, 1)$</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/95.png' width="80%" height="80%"/ loading="lazy">

<p>实现：</p>
<p>对于每一个mini-batch：在前向传播的过程中，在每个隐藏层使用 batch norm；反向传播计算梯度；进行梯度下降或使用其他优化算法。</p>
<h4 id="3-6-为什么Batch-Norm有用？"><a href="#3-6-为什么Batch-Norm有用？" class="headerlink" title="3.6 为什么Batch Norm有用？"></a>3.6 为什么Batch Norm有用？</h4><p>第一个原因：跟逻辑回归类似，让所有特征归一到同一尺度，加速梯度下降的过程。 </p>
<p>第二个原因：让权重比网络更滞后或更深层，让数值更稳定。第10层的权重比第1层的权重更robust。在之前层的权重发生改变时，$z$ 会发生变化，但batch norm保证了 $z$  的均值和方差保持不变。因此限制了在前层的参数更新对数值分布的影响。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/96.png' width="80%" height="80%"/ loading="lazy">

<p>第三个原因：batch norm有一点正则化的效果。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/97.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-7-测试时的Batch-Norm"><a href="#3-7-测试时的Batch-Norm" class="headerlink" title="3.7 测试时的Batch Norm"></a>3.7 测试时的Batch Norm</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/98.png' width="80%" height="80%"/ loading="lazy">

<p>在训练过程中，batch norm一次作用在一个mini-batch上，求这个mini-batch上的均值和方差（图左）；在评估阶段，batch norm只作用在单个测试样本上，虽然可以在整个测试集上计算 $\mu,\sigma^2$，但在实际操作中，通常使用指数加权平均（图右）。</p>
<p>追踪训练过程中每个mini-batch的 $\mu,\sigma^2$ 的值，然后使用之前求温度 $\theta_1,\theta_2,\theta_3$ 的指数加权平均的方法（见<a href="#2.2.3">第二课第二周第三节</a>，求 $\mu,\sigma^2$ 的近似值，然后用于下一步计算 $z_{norm} &#x3D; \frac{z-\mu}{\sqrt{\sigma^2+\epsilon}}$。</p>
<p>实际上，不管用什么样的估计方法，整套过程都是比较robust的。</p>
<p>当使用深度学习框架时，通常会有默认的估算 $\mu,\sigma^2$ 的方法。</p>
<h4 id="3-8-Softmax-回归"><a href="#3-8-Softmax-回归" class="headerlink" title="3.8 Softmax 回归"></a>3.8 Softmax 回归</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/99.png' width="80%" height="80%"/ loading="lazy">

<p>多分类问题中，预测一组相加为1的概率值作为神经网络的输出层。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/100.png' width="80%" height="80%"/ loading="lazy">

<p>使用softmax激活函数进行从权值到概率的转换。$t^i&#x3D;e^{z^i}$，$a^i &#x3D; \frac{t^i}{\sum t}$ </p>
<p>ReLU和Sigmoid函数输入一个实数，输出一个实数；而softmax函数因为要对所有的输出进行归一化（计算概率），需要输入一个向量，输出一个向量。</p>
<p>直观的softmax分类的例子：神经网络只有一层softmax层。神经元有三个，就分成3类，每类之间都是线性决策边界。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/101.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-9-训练一个-Softmax-分类器"><a href="#3-9-训练一个-Softmax-分类器" class="headerlink" title="3.9 训练一个 Softmax 分类器"></a>3.9 训练一个 Softmax 分类器</h4><p>softmax跟hardmax相对，把最大值更温和地转换成一个概率，而不是全部改为0和1.</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/102.png' width="80%" height="80%"/ loading="lazy">



<p><strong>训练-损失函数</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/103.png' width="80%" height="80%"/ loading="lazy">

<p>单个训练样本的损失函数：$$L(\hat{y}, y) &#x3D; -\sum^C_{j&#x3D;1}y_jlog\hat{y_j}$$</p>
<p>在真实情况中，$y_j$ 只有一个为1，其余都为0，因此损失函数是 $-log\hat{y_i}$ ，损失函数试图让 $y&#x3D;1$ 对应的 $\hat{y}$ 尽量地大。这也是最大似然估计的一种形式。</p>
<p>整个训练集的损失函数：$$J(w,b) &#x3D; \frac{1}{m}\sum^m_{i&#x3D;1} L(\hat{y}, y)$$ </p>
<p>$\hat{y},y$ 的维度都是 $(4, m) $。</p>
<p><strong>训练-梯度下降</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/104.png' width="80%" height="80%"/ loading="lazy">

<p>梯度：$$dz^{[l]} &#x3D; \hat{y} - y$$ </p>
<p>在深度学习框架中，主要精力放在将前向传播做好。通常框架自己会弄明白怎样反向传播。</p>
<h4 id="3-10-深度学习框架"><a href="#3-10-深度学习框架" class="headerlink" title="3.10 深度学习框架"></a>3.10 深度学习框架</h4><p>现存的框架；选择框架的标准。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/105.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-11-TensorFlow"><a href="#3-11-TensorFlow" class="headerlink" title="3.11 TensorFlow"></a>3.11 TensorFlow</h4><p>在tensorflow中定义损失函数cost，可以理解为tensorflow会建立起一个计算图，来自动完成后续的反向传播。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/106.png' width="80%" height="80%"/ loading="lazy">

<p>在框架中，可以只用一行代码修改很多工作，比如训练的方法是梯度下降还是Adam。这支持我们快速实现复杂的神经网络模型。</p>
<h2 id="第三课-结构化机器学习项目-Structuring-Machine-Learning-Projects"><a href="#第三课-结构化机器学习项目-Structuring-Machine-Learning-Projects" class="headerlink" title="第三课 结构化机器学习项目 (Structuring Machine Learning Projects)"></a>第三课 结构化机器学习项目 (Structuring Machine Learning Projects)</h2><p>暂时空缺</p>
<h2 id="第四课-卷积神经网络（Convolutional-Neural-Networks）"><a href="#第四课-卷积神经网络（Convolutional-Neural-Networks）" class="headerlink" title="第四课 卷积神经网络（Convolutional Neural Networks）"></a>第四课 卷积神经网络（Convolutional Neural Networks）</h2><h3 id="第一周-卷积神经网络-Foundations-of-Convolutional-Neural-Networks"><a href="#第一周-卷积神经网络-Foundations-of-Convolutional-Neural-Networks" class="headerlink" title="第一周 卷积神经网络(Foundations of Convolutional Neural Networks)"></a>第一周 卷积神经网络(Foundations of Convolutional Neural Networks)</h3><p>卷积运算（padding、stride）和不同的卷积核；将卷积核叠加的三维卷积和单层卷积网络；卷积神经网络（CONV、POOL、FN）。</p>
<h4 id="1-1-计算机视觉（Computer-vision）"><a href="#1-1-计算机视觉（Computer-vision）" class="headerlink" title="1.1 计算机视觉（Computer vision）"></a>1.1 计算机视觉（Computer vision）</h4><p>计算机视觉的应用：图片分类，目标检测，风格迁移等。</p>
<p>计算机视觉的一个问题是数据量非常大。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/107.png' width="80%" height="80%"/ loading="lazy">



<h4 id="1-2-卷积运算-边缘检测为例（Edge-detection）"><a href="#1-2-卷积运算-边缘检测为例（Edge-detection）" class="headerlink" title="1.2 卷积运算-边缘检测为例（Edge detection）"></a>1.2 卷积运算-边缘检测为例（Edge detection）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/108.png' width="80%" height="80%"/ loading="lazy">

<p>在神经网络隐藏层中，不同层识别不同的信息。比如，浅层识别物体的边缘，深层识别人脸的部位，更深层识别整个人脸。以边缘检测为例，展示卷积计算的过程。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/109.png' width="80%" height="80%"/ loading="lazy">

<p>以中间矩阵的区域，在左边矩阵的每个对应区域，进行对应元素相乘，然后加起来，作为右边矩阵的一个值。</p>
<p>左边的矩阵理解为图片；中间的矩阵是过滤器（filter）或卷积核（kernel）；右边的矩阵可以理解为另一张图片。* 是数学上的卷积运算符，但在python中， * 也被重载做很多场合的乘法运算，所以在编程中使用其他函数，比如tensorflow中是 <code>tf.nn.conv2d</code>。</p>
<p>这也是纵向边缘的计算过程：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/110.png' width="80%" height="80%"/ loading="lazy">

<p>越大的值理解为颜色越浅。本例计算出的边界比较宽，是因为原图片相对来说非常小。</p>
<h4 id="1-3-更多边缘检测内容"><a href="#1-3-更多边缘检测内容" class="headerlink" title="1.3 更多边缘检测内容"></a>1.3 更多边缘检测内容</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/111.png' width="80%" height="80%"/ loading="lazy">

<p>使用相同的filter，可以在输出图像中区分源图像从亮到暗&amp;从暗到亮这两种变化。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/112.png' width="80%" height="80%"/ loading="lazy">

<p>不同的filter可以帮助我们找到垂直或水平的边缘。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/113.png' width="80%" height="80%"/ loading="lazy">

<p>也有相关工作提出更robust的filter取值，同时也可以不手动定义filter，而把这些数字当成参数，通过反向传播学习更好的filter（之后的内容）。</p>
<p>通过合理设置filter，不仅能检查水平、垂直的边缘，也可以检测任何角度的边缘。</p>
<p>通过把filter的所有数字设置成参数，并让计算机自动学习它们，我们发现：神经网络可以学习一些低级的特征，比如图片的边缘特征。构成这些运算的基础依然是卷积运算（convolution），使得反向传播算法可以学习任何所需的3×3 filter，并在整张图片上应用它。</p>
<h4 id="1-4-Padding"><a href="#1-4-Padding" class="headerlink" title="1.4 Padding"></a>1.4 Padding</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/114.png' width="80%" height="80%"/ loading="lazy">

<p>使用 $f\times f$ 的卷积核，卷积 $n\times n$ 的源图像，得到 $(n-f+1)\times(n-f+1)$ 的新图象。</p>
<p>卷积的两个缺点：</p>
<ul>
<li>卷积会让图片尺寸缩小。可能做几次之后图像就变得很小了。</li>
<li>边缘的像素参与的卷积运算很少，中间的像素用得很多。意味着卷积丢失了图像边缘的信息。</li>
</ul>
<p>通过padding解决这两个问题：在图像周围再添加 $p$ 圈像素。</p>
<p>使用 $f\times f$ 的卷积核，卷积 $(n+p)\times (n+p)$ 的源图像，得到 $(n+2p-f+1)\times(n+2p-f+1)$ 的新图象。</p>
<p>如图 $p&#x3D;1$：</p>
<ul>
<li>$8\times 8$ 的新图象经过卷积，得到 $6\times 6$ 的图像，尺寸没有变小。</li>
<li>边缘的像素参与的卷积运算更多了。</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/115.png' width="80%" height="80%"/ loading="lazy">

<p> 关于padding多少：</p>
<ul>
<li>Valid convolution：不padding。 $(n\times n) * (f\times f) \longrightarrow (n-f+1)\times(n-f+1)$</li>
<li>Same convolution：padding后得到的输出图像尺寸是源图像尺寸。 $(n\times n) * (f\times f) \longrightarrow n\times n$，$p&#x3D;\frac{f-1}{2}$</li>
</ul>
<p>在计算机视觉问题中，$f$ 一般是奇数。</p>
<h4 id="1-5-卷积步长（Strided-convolutions）"><a href="#1-5-卷积步长（Strided-convolutions）" class="headerlink" title="1.5 卷积步长（Strided convolutions）"></a>1.5 卷积步长（Strided convolutions）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/116.png' width="80%" height="80%"/ loading="lazy">

<p>padding p，stride s：</p>
<p>$$(n\times n) * (f\times f) \longrightarrow (\lfloor \frac{n+2p-f}{2} +1\rfloor)\times(\lfloor \frac{n+2p-f}{2} +1\rfloor)$$</p>
<p>惯例：不是整数就向下取整，超出边缘的卷积不进行计算。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/117.png' width="80%" height="80%"/ loading="lazy">

<p>数学中的convolution还要进行反转filter的操作，在机器学习中则不进行。机器学习的运算在数学中被称为cross-correlation，但在论文中我们延续convolution这一说法，要注意与数学环境中的convolution做区分。</p>
<h4 id="1-6-三维卷积（Convolutions-over-volumes）"><a href="#1-6-三维卷积（Convolutions-over-volumes）" class="headerlink" title="1.6 三维卷积（Convolutions over volumes）"></a>1.6 三维卷积（Convolutions over volumes）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/118.png' width="80%" height="80%"/ loading="lazy">

<p>源图像和filter的channel数量必须相同。最终得到一个二维输出。</p>
<p>将27个数对应相乘再求和，得到输出图像上的一个数。</p>
<p>通过不同的filter的参数选择，获得不同的特征检测器。如图，可以构建只关心红色通道的纵向边缘的filter；也可以构建不关心任何颜色，只关心纵向边缘的filter。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/119.png' width="80%" height="80%"/ loading="lazy">

<p>也可以使用多个filter。如图，将纵向边缘filter、横向边缘filter卷积而来的两张图片结合起来，得到 $4\times 4\times 2$ 的新图像。这种思想使我们可以检测很多个不同的特征，并且输出的通道数等于要检测的特征数，即filter的个数。</p>
<h4 id="1-7-单层卷积网络"><a href="#1-7-单层卷积网络" class="headerlink" title="1.7 单层卷积网络"></a>1.7 单层卷积网络</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/120.png' width="80%" height="80%"/ loading="lazy">

<p>单层卷积网络的前向传播：</p>
<ul>
<li><p>卷积运算。对应 $w^{[1]}a^{[0]}$。$w^{[1]}$是filter，$a^{[0]}$是源图像。</p>
</li>
<li><p>对得到的 $4\times 4$ 矩阵加一个权值（使用 broadcasting）。对应 $z^{[1]} &#x3D; w^{[1]}a^{[0]} + b^{[1]}$。</p>
</li>
<li><p>进行非线性函数处理，如 ReLU，得到新的  $4\times 4$ 矩阵。对应 $a^{[1]} &#x3D; g(z^{[1]})$ 。</p>
</li>
<li><p>多个filter，计算结果叠加起来，得到  $$4\times 4 \times num_filters$$ 矩阵</p>
</li>
</ul>
<p>参数数量：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/121.png' width="80%" height="80%"/ loading="lazy">

<p>不管输入图片的尺寸有多大，参数的个数只跟filter有关。这是卷积神经网络的一个特性，可以避免过拟合。</p>
<p>符号总结：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/122.png' width="80%" height="80%"/ loading="lazy">

<p>每层输出图像的尺寸：</p>
<ul>
<li><p>$n_H^{[l]} &#x3D; \lfloor \frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}} +1 \rfloor  $ </p>
</li>
<li><p>$n_W^{[l]} &#x3D; \lfloor \frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}} +1 \rfloor $</p>
</li>
</ul>
<p>每个filter的尺寸需要匹配上层输出图像的channel数量：</p>
<ul>
<li>$f^{[l]} \times f^{[l]} \times n_c^{[l-1]}$</li>
</ul>
<p>所有的filter：</p>
<ul>
<li>$f^{[l]} \times f^{[l]} \times n_c^{[l-1]} \times n_c^{[l]}$</li>
</ul>
<p>本层图像经过bias和非线性函数得到的activation尺寸：</p>
<ul>
<li>$a^{[l]}:n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$</li>
</ul>
<p>一个mini-batch的所有activations：</p>
<ul>
<li>$A^{[l]}:m \times n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$</li>
</ul>
<p>也不是所有人都用这一套标记法，有些人把channel的数量写在前面。</p>
<h4 id="1-8-简单的卷积神经网络示例"><a href="#1-8-简单的卷积神经网络示例" class="headerlink" title="1.8 简单的卷积神经网络示例"></a>1.8 简单的卷积神经网络示例</h4><p>预测 $39\times 39 \times 3$ 的图像上是否有一只猫，设计以下卷积神经网络：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/123.png' width="80%" height="80%"/ loading="lazy">

<p>经过几步卷积后，获得 $7\times 7 \times 40$ 的特征图，将它们展开成 1960 长度的列向量，进行logistic或softmax回归，预测图片中是否有猫。</p>
<p>在卷积的过程中，有这样的趋势：图像的大小在减少，通道数量在增多。</p>
<p>选择超参数是一个问题，$f,s,p,num_filters$ 等。在之后的课程中会提供一些建议和指导。</p>
<p>卷积神经网络通常由三种layer组成：</p>
<ul>
<li>Convolution，卷积层，CONV</li>
<li>Pooling，池化层，POOL</li>
<li>Fully connectied，全连接层，FC</li>
</ul>
<h4 id="1-9-池化层（Pooling-layers）"><a href="#1-9-池化层（Pooling-layers）" class="headerlink" title="1.9 池化层（Pooling layers）"></a>1.9 池化层（Pooling layers）</h4><p>使用池化层，来缩减模型的大小，提高计算速度，同时让所提取的特征robust。</p>
<p>max pooling：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/124.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>max pooling对每一个通道独立处理，不改变通道个数。</p>
</li>
<li><p>有两个超参数 $f,s$ ，不需要网络学习，手动设置后就不再改变。</p>
</li>
<li><p>可以直觉理解为：数字大意味着可能提取了某些特定特征。</p>
</li>
</ul>
<p>average pooling：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/125.png' width="80%" height="80%"/ loading="lazy">

<p>跟max pooling差不多。</p>
<p>通常，max pooling更加常用；但有时，在很深的神经网络也会用到average pooling。（有时用，在下周讲）</p>
<p>池化层的超参数：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/126.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>有常用的设置 $f&#x3D;2,s&#x3D;2$ ，意味着把图片长宽都缩小一半。</p>
</li>
<li><p>可以自己增加padding参数 $p$，但极少这样做。（有意外，在下周讲）</p>
</li>
<li><p>$n_H \times n_W \times n_c \longrightarrow \lfloor \frac{n_H-f}{s}+1 \rfloor \times \lfloor \frac{n_H-f}{s}+1 \rfloor \times n_c$ ，<strong>池化层不改变通道的个数</strong>。</p>
</li>
<li><p><strong>池化层没有需要训练的参数，只有超参数</strong>。</p>
</li>
</ul>
<h4 id="1-10-卷积神经网络示例（Convolutional-neural-network-example）"><a href="#1-10-卷积神经网络示例（Convolutional-neural-network-example）" class="headerlink" title="1.10 卷积神经网络示例（Convolutional neural network example）"></a>1.10 卷积神经网络示例（Convolutional neural network example）</h4><p>手写数字识别（跟 LeNet-5 相似）：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/127.png' width="80%" height="80%"/ loading="lazy">

<p>有一种叫法是把 CONV+POOL 作为一层卷积，因为 POOL 层没有权重，只有超参数。在本例中同样将 CONV1+POOL1 作为 layer 1。</p>
<p>全连接层相当于单层普通神经网络，神经元全部相连，每条边有一个权值。</p>
<ul>
<li><p>第一层：卷积+最大池化。参数是6个filters。</p>
</li>
<li><p>第二层：卷积+最大池化。参数是16个filters。</p>
</li>
<li><p>第三层：flatten后，400到120的全连接。参数是 $w,b$。</p>
</li>
<li><p>第四层：120到84的全连接。参数是 $w,b$。</p>
</li>
<li><p>输出：对84个神经元进行 softmax ，预测手写数字。</p>
</li>
</ul>
<p>常见的模式：</p>
<ul>
<li>图像尺寸逐渐变小，通道数量逐渐增多。</li>
<li>一个或多个卷积层后接一个池化层，重复几次，最后是几个全连接层，最终进行softmax等函数输出。</li>
</ul>
<p>常规做法：尽量不要自己设置超参数，而是查看文献，使用别人在任务中效果很好的架构。（下周细讲）</p>
<p>卷积神经网络的一些细节：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/128.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li>参数<ul>
<li>池化层没有任何参数。</li>
<li>卷积层参数数量较小，这点在之前提过。只跟filter有关，跟图片的尺寸无关。<ul>
<li>416 &#x3D; 16channel * (5*5filter + 1bias)，每个filter有一个偏置。</li>
</ul>
</li>
<li>大多数参数存在于全连接层。<ul>
<li>48001 &#x3D; 120 * 400 + 1bias，每层一个偏置，可以类比普通的神经网络。</li>
</ul>
</li>
</ul>
</li>
<li>激活值<ul>
<li>随着神经网络加深，激活值会逐渐变小。如果激活值下降太快，也会影响神经网络的表现。</li>
</ul>
</li>
</ul>
<p>卷积神经网络的重点是如何更好地组织卷积层、池化层、全连接层。这要求我们多阅读论文，了解别人的模型，得到自己的insight&#x2F;intuation。下周将介绍一些表现良好的模型。</p>
<h4 id="1-11-为什么使用卷积？（Why-convolutions-）"><a href="#1-11-为什么使用卷积？（Why-convolutions-）" class="headerlink" title="1.11 为什么使用卷积？（Why convolutions?）"></a>1.11 为什么使用卷积？（Why convolutions?）</h4><p>卷积神经网络为何有效？如何整合这些卷积？如何通过标注过的训练集进行卷积神经网络的训练？</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/129.png' width="80%" height="80%"/ loading="lazy">

<p>卷积神经网络相比只有全连接的普通神经网络的优势：参数共享和稀疏连接。</p>
<ul>
<li><p>参数共享：filter的参数可以用于图片的任何区域，来提取特征。</p>
</li>
<li><p>稀疏链接：输出图像的每个像素仅与几个源图像的像素有关（不是全连接）。</p>
</li>
</ul>
<p>这两点保证了<strong>卷积神经网络可以用比较小的数据集进行训练，并且不容易过拟合</strong>。</p>
<p>卷积神经网络善于捕捉平移不变（translation invariance），因为神经网络的卷积结构保证了，即使移动几个像素，图片依然具有非常相似的特征。</p>
<p>训练卷积神经网络的过程：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/130.png' width="80%" height="80%"/ loading="lazy">

<p>通过梯度下降或其他优化算法，优化参数，让损失函数 $J$ 降到最低。</p>
<h3 id="第二周-深度卷积网络：实例探究-Deep-convolutional-models-case-studies"><a href="#第二周-深度卷积网络：实例探究-Deep-convolutional-models-case-studies" class="headerlink" title="第二周 深度卷积网络：实例探究(Deep convolutional models: case studies)"></a>第二周 深度卷积网络：实例探究(Deep convolutional models: case studies)</h3><p>一些卷积神经网络的实例分析：Classic networks（LeNet-5，AlexNet，VGG），ResNet，Inception（1×1卷积）；计算机视觉问题建立在小数据系统上，需要进行：数据增强，迁移训练，使用开源。</p>
<h4 id="2-1-为什么要进行实例探究？"><a href="#2-1-为什么要进行实例探究？" class="headerlink" title="2.1 为什么要进行实例探究？"></a>2.1 为什么要进行实例探究？</h4><p>好的网络架构可能在其他任务中也好用。</p>
<h4 id="2-2-经典网络"><a href="#2-2-经典网络" class="headerlink" title="2.2 经典网络"></a>2.2 经典网络</h4><p>红笔写的是现在基本不用的技术。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/131.png' width="80%" height="80%"/ loading="lazy">

<p>在LeNet提出时，使用的这些技术现在已经基本被取代了：sigmoid和tanh激活函数；平均池化；valid 卷积；受限于计算能力，卷积的计算方法也很复杂。现在用的是：ReLU；最大池化；same卷积。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/132.png' width="80%" height="80%"/ loading="lazy">

<p>在AlexNet中，使用了ReLU、same卷积、max-pool、设置stride、softmax等新技术。</p>
<p>LeNet-5大约有60,000个参数；AlexNet有大约60,000,000个参数。</p>
<p>在AlexNet提出时，GPU的处理速度还比较慢，所以AlexNet采用了很复杂的方法在两个GPU上训练。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/133.png' width="80%" height="80%"/ loading="lazy">

<p>VGG-16网络没有很多超参数，专注于构建卷积层。16的意思是网络中有16层有权值的地方（2+2+3+3+3&#x3D;13卷积层，3全连接层）。</p>
<ul>
<li>CONV &#x3D; 3×3 filter, s&#x3D;1, same</li>
<li>MAX-POOL &#x3D; 2×2, s&#x3D;2</li>
</ul>
<p>VGG-16 有约 138,000,000 个参数，但结构很规整，图像缩小的比例和channel增加的比例是有规律的。后面的VGG-19比这个模型更大，但这两个模型表现差不多。</p>
<h4 id="2-3-残差网络（Residual-Networks-ResNets-）"><a href="#2-3-残差网络（Residual-Networks-ResNets-）" class="headerlink" title="2.3 残差网络（Residual Networks (ResNets)）"></a>2.3 残差网络（Residual Networks (ResNets)）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/134.png' width="80%" height="80%"/ loading="lazy">

<p>很深的神经网络难以训练，因为存在梯度消失和梯度爆炸的问题。使用ResNet，可以训练北京深层的神经网络。</p>
<p>每两层组成一个残差块：浅层的激活值通过short cut，直接输入到深层的非线性函数（如ReLU）中。</p>
<p>$$a^{[l+2]} &#x3D; g(z^{[l+2]}+a^{[l]})$$ </p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/135.png' width="80%" height="80%"/ loading="lazy">

<p>论中将没有使用残差块的神经网络叫做Plain网络，在理论上层数越多，损失越小；但实际情况是，网络越深，在训练集上的误差会反弹。ResNet就会解决这一问题。</p>
<h4 id="2-4-残差网络为什么有用？"><a href="#2-4-残差网络为什么有用？" class="headerlink" title="2.4 残差网络为什么有用？"></a>2.4 残差网络为什么有用？</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/136.png' width="80%" height="80%"/ loading="lazy">

<p>如果让 $w,b$ 都为0，那么 $a^{[l+1]} &#x3D; a{^{[l]}}$ ，学习恒等函数对残差块来说很简单。也就是说，虽然加上一个残差块（两层神经网络），效率也不逊色于更简单的神经网络。并且残差块添加的位置也不影响网络的表现。</p>
<p>在不伤害性能的基础上，如果残差块的隐藏单元学习到一些有用信息，那么就能比恒等函数表现得更好。</p>
<p>而对于plain神经网络来说，就算是学习恒等函数的参数都很困难，因此很多层最后的表现变差了。</p>
<p>另外，ResNet使用same卷积，保证 $z^{[l+2]}$ 和 $a^{[l]}$ 有相同的维度，可以相加。如果输入和输出维度不一样，就再增加一个矩阵 $w_s$ 。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/137.png' width="80%" height="80%"/ loading="lazy">

<p>几个之前提到的细节：</p>
<ul>
<li>使用3×3 same卷积，保证 $z^{[l+2]}$ 和 $a^{[l]}$ 有相同的维度，可以相加。</li>
<li>pool-like 层，进行 &#x2F;2 降维操作。</li>
<li>CONV-CONV-CONV-POOL 交替进行的结构。</li>
</ul>
<h4 id="2-5-网络中的网络-x2F-1×1卷积"><a href="#2-5-网络中的网络-x2F-1×1卷积" class="headerlink" title="2.5 网络中的网络 &#x2F; 1×1卷积"></a>2.5 网络中的网络 &#x2F; 1×1卷积</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/138.png' width="80%" height="80%"/ loading="lazy">

<p>1×1卷积添加了非线性函数，可以让网络学习更复杂的函数。</p>
<p>1×1卷积对单通道作用不大，但对于多通道，可以把所有通道相同位置的数输出成一个数（对应位置相乘 -&gt; 相加 -&gt; ReLU）。如果filter数量不止一个，可以输出多个通道。</p>
<p>论文名字叫 network in network ，这种方法也可以称为1×1卷积。论文中的架构没有得到广泛使用，但这种方法利用到了之后的Inception等模型上。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/139.png' width="80%" height="80%"/ loading="lazy">

<p>作用如上图。POOL的作用是压缩 $n_H,n_W$，而 1×1 卷积可以压缩 $n_C$，减少信道数量来简化计算。当然让信道数量保持不变或者增加也可以。</p>
<h4 id="2-6-Inception-模块简介、"><a href="#2-6-Inception-模块简介、" class="headerlink" title="2.6 Inception 模块简介、"></a>2.6 Inception 模块简介、</h4><p>Inception模块：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/140.png' width="80%" height="80%"/ loading="lazy">

<p>Inception：不需要人来决定使用什么规格的filter、是否使用POOL。网络结构更复杂但表现更好。</p>
<p>如图，Inception模块输入某个量，经过不同的处理，输出将这些结果叠加起来。</p>
<p>Inception网络不需要人为决定使用哪个fitler，或是否需要池化，而是由网络自行决定这些参数。我们可以给网络添加这些参数的所有可能的值，然后把这些输出连接起来，让网络学习他需要什么样的参数。</p>
<p>为了维持所有的维度相同，对卷积要使用filter卷积，对池化要使用padding（比较特殊的POOL）。</p>
<p>巨大的运算量：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/141.png' width="80%" height="80%"/ loading="lazy">

<p>Inception模块的问题是参数多，计算成本高。以 5×5卷积的一部分为例：需要 5×5×192 filter，对于输出的每个数都要做 filter 规格次数的乘法，也就是一共要做 $(28×28×32) * (5×5×192) ≈ 120 M$ 次乘法。</p>
<p>改进方法：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/142.png' width="80%" height="80%"/ loading="lazy">

<p>使用 1×1 卷积得到相同规格的输出，通过压缩成较小的中间形态（瓶颈层bottleneck layer）。</p>
<p>一共要做 $(28×28×16) * (1×1×192)  + (28×28×32) * (5×5×16) ≈ 2.4 M + 10 M &#x3D; 12.4 M$ 次运算，乘法计算的成本大约变为原来的十分之一。</p>
<p>通过合理构建瓶颈层，既可以显著缩小表示层的规模，又不会降低网络性能，从而大量节省计算成本。</p>
<h4 id="2-7-Inception-网络-x2F-GoogLeNet（Inception-network）"><a href="#2-7-Inception-网络-x2F-GoogLeNet（Inception-network）" class="headerlink" title="2.7 Inception 网络 &#x2F; GoogLeNet（Inception network）"></a>2.7 Inception 网络 &#x2F; GoogLeNet（Inception network）</h4><p>Inception模块：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/143.png' width="80%" height="80%"/ loading="lazy">



<p>Inception网络：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/144.png' width="80%" height="80%"/ loading="lazy">

<p>红圈：由Inception模块重复堆叠而来，有些max-pooling层，来改变长和宽。</p>
<p>绿圈：一些分支，通过一些隐藏层，做一个softmax分类。它确保了即使是隐藏单元和中间层，也参与了特征运算，也能进行预测图片的分类。并且防止网络过拟合。</p>
<p>其他：也有变体把 Inception 和 ResNet 结合起来。可以看Inception的后续论文。</p>
<h4 id="2-8-使用开源的实现方案"><a href="#2-8-使用开源的实现方案" class="headerlink" title="2.8 使用开源的实现方案"></a>2.8 使用开源的实现方案</h4><p>很多神经网络难以复现，因为一些超参数的细节调整会影响性能。</p>
<p>先从使用开源的实现开始。</p>
<h4 id="2-9-迁移学习（Transfer-Learning）"><a href="#2-9-迁移学习（Transfer-Learning）" class="headerlink" title="2.9 迁移学习（Transfer Learning）"></a>2.9 迁移学习（Transfer Learning）</h4><p>用迁移学习把公共数据集的知识迁移到我们自己的问题上。</p>
<p>在做一个计算机视觉的应用时，相比于从头训练权重、随机初始化，可以<strong>下载开源的、别人已经训练好的网络结构的权重，作为我们模型的初始化</strong>。</p>
<p>以猫咪分类问题为例，我们使用预训练的 ImageNet 模型，将最后分类改为 softmax 分类这是猫是Tigger，还是Misty，还是都不是。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/145.png' width="80%" height="80%"/ loading="lazy">

<p><strong>图上</strong>：把前面预训练的模型当作冻结的，<strong>只训练跟我们的 softmax 层有关的参数</strong>。</p>
<ul>
<li><p>或许可以设置<code>trainableParameter = 0​</code>或<code>freeze=1</code>这样的参数，指定不训练特定层的权重。</p>
</li>
<li><p>可以把前面冻结的模型看作一个函数，输入一张图片，输出一个特征向量。只训练后面的softmax层，用这个特征向量来做预测。因此可以<strong>提前计算训练集中所有样本的这一层的激活值</strong>，然后存到硬盘里，在此之上训练softmax层。这样就不用每次遍历数据集重新计算这一层的激活值了。</p>
</li>
</ul>
<p><strong>图中</strong>：如果模型特别大，可以freeze一部分模型，然后<strong>训练后面的模型</strong>。也可以freeze一部分模型，把后面的模型进行修改。</p>
<ul>
<li>规律：<strong>数据集越大，需要冻结的层数越少，需要进行训练的层数越多</strong>。</li>
</ul>
<p><strong>图下</strong>：如果有特别多的数据，就用开源的网络和它的权重当作参数的初始化，然后<strong>训练整个网络</strong>。</p>
<p>其他：计算机视觉问题中，迁移学习特别常用。除非有一个极其大的数据集，才从头开始训练所有东西。</p>
<h4 id="2-10-数据增强（Data-augmentation）"><a href="#2-10-数据增强（Data-augmentation）" class="headerlink" title="2.10 数据增强（Data augmentation）"></a>2.10 数据增强（Data augmentation）</h4><p>计算机视觉方面的主要问题是没有办法得到充足的数据。当训练模型时，不管是从别人预训练的模型进行迁移学习，还是从源代码开始训练模型，数据增强会经常有所帮助。</p>
<p>数据增强的方法：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/146.png' width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/147.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li>镜像翻转、随即裁剪（保留主体）、旋转、剪切、局部弯曲 等。也可以组合起来用，但因为太复杂，实际上用的很少。</li>
<li>色彩转换，进行RGB的调整，一般是根据某种概率分布来决定改变的值。<ul>
<li>*对RGB不同的采样方式：使用PCA（见机器学习网课笔记）。在AlexNet的论文中，成为“PCA color augmentation”，比如我们的图片呈紫色（红蓝多，绿少），那么PCA颜色增强算法会对红蓝有大的增减幅度，对绿的变化相对少，以此使总体的颜色保持一致。</li>
</ul>
</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/148.png' width="80%" height="80%"/ loading="lazy">

<p>如果数据集比较大，常用的方法是设置单个thread，串行读取数据、进行数据增强。</p>
<ul>
<li><p>thread A：<strong>从硬盘读数据并数据增强</strong>。CPU有一个thread不停地从硬盘中读取数据，同时进行变形或颜色转换形成新的图像，从而构成一个batch或者mini-batch的数据；</p>
</li>
<li><p>thread B：<strong>训练</strong>。这些数据被传递给其他thread（可能是CPU或GPU），进行模型的训练。</p>
</li>
</ul>
<p><strong>以上两个thread可以并行实现。</strong></p>
<p>其他：数据增强也有一些超参数，比如如何进行颜色变化等。方法依然是学习开源的实现。</p>
<h4 id="2-11-计算机视觉现状"><a href="#2-11-计算机视觉现状" class="headerlink" title="2.11 计算机视觉现状"></a>2.11 计算机视觉现状</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/149.png' width="80%" height="80%"/ loading="lazy">

<p>从数据少到数据多的应用场景：目标检测，图像识别，语音识别</p>
<p>数据很多时：不需要精心设计模型，可以使用简单的算法、较少的人工。</p>
<p>数据很少：进行更多手工工程，进行迁移学习。</p>
<p>知识的两种来源：标签；手工工程。如果没有很多标签，就用更多的手工。</p>
<p><strong>计算机视觉问题通常数据相对较少，更多依赖于手工工程，并且设计比较复杂的网络架构，有比较复杂的超参数选择问题</strong>。可以说：<strong>计算机视觉问题建立在小数据系统</strong>。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/150.png' width="80%" height="80%"/ loading="lazy">

<p>在benchmarks（基准测试）&#x2F;竞赛上取得好的表现的方法：</p>
<ul>
<li>集成<ul>
<li>当构思好神经网络之后，独立训练几个网络，并对它们的输出求平均。</li>
<li>集成需要保留多个网络，对内存有比较大的占用。</li>
</ul>
</li>
<li>multi-crop<ul>
<li>这是一种将数据增强扩展到测试集的方法。以10-crop为例，把测试集的图片分为（中心裁剪、四角裁剪、镜像中心裁剪、镜像四角裁剪）十张图片，分别对它们进行分类，并对输出求平均。</li>
<li>multi-crop只需要保留一个网络，不会占用太多内存，但会让运行时间变慢。</li>
</ul>
</li>
</ul>
<p>需要注意，<strong>这是在基准测试和竞赛中使用的方法，不要在真实生产场景下使用这些方法，因为会让运行时间变慢。</strong></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/151.png' width="80%" height="80%"/ loading="lazy">

<p>由于计算机视觉问题建立在小数据系统，其他人已经完成了大量的网络架构的手工工程；并且一个神经网络在一个视觉问题上很有效，通常也会解决其他视觉问题。</p>
<p>所以想建立一个使用的系统，最好先从其他人的神经网络架构入手。尤其是开源系统，会包含超参数设置等细节问题。<strong>最好使用预训练的模型，在我们自己的数据集上进行调优</strong>。</p>
<h3 id="第三周-目标检测（Object-detection）"><a href="#第三周-目标检测（Object-detection）" class="headerlink" title="第三周 目标检测（Object detection）"></a>第三周 目标检测（Object detection）</h3><p>(1) 能完成 classification 的卷积神经网络。我们希望找到 location</p>
<p>(2) 结合滑动窗口进行 detection（滑动窗口+CNN），可以找到 location 了。但计算量大</p>
<p>(3) 滑动窗口的卷积实现（overfeat），计算简单了。但滑动窗口精度不高</p>
<p>(4) YOLO算法，划分单元格，每个格子预测中点在格子内的物体和物体的边框。可以预测物体的精确边框了。但单个物体有多个边框</p>
<p>(5) mon-max supression，单个物体只有一个边框了。但一个格子只能检测一个对象</p>
<p>(6) anchor boxes，一个格子可以检测多个对象了。</p>
<h4 id="3-1-目标定位（Object-localization）"><a href="#3-1-目标定位（Object-localization）" class="headerlink" title="3.1 目标定位（Object localization）"></a>3.1 目标定位（Object localization）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/152.png' width="80%" height="80%"/ loading="lazy">

<p>detection问题中，图片可以包含多个对象，甚至是多个不同分类的对象。</p>
<p>classification 的思路可以帮助学习 classification with localization 问题；classification with localization 的思路又有助于学习 detection 问题。</p>
<p>我们从 classification with localization 问题开始。</p>
<p>classification with localization pipeline：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/153.png' width="80%" height="80%"/ loading="lazy">

<p>图片输入ConvNet，输出一个4分类的softmax + 一个定位坐标。</p>
<p>标签 y 定义如下：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/154.png' width="80%" height="80%"/ loading="lazy">

<p>假定图片中最多出现一个对象。输出 y ：</p>
<ul>
<li>P_c：是否有物体（没有就是第四类background）</li>
<li>bx, by, bh, bw：物体框</li>
<li>c_1, c_2, c_3：是哪一类物体</li>
</ul>
<p>图右侧有两个例子，”?” 是不需要关心的数。</p>
<p>损失函数定义见图左。当 $y_1&#x3D;1$，使用 squared erorr；当 $y_1&#x3D;0$，不许考虑其他元素，只看 P_c 的准确度。</p>
<p>也可以对不同部分使用不同的损失函数，比如对分类部分 c_1, c_2, c_3 使用对数，对 bx, by, bh, bw 使用平方误差，对 P_c 使用逻辑回归损失函数。全用 squared error 也是可以的。</p>
<h4 id="3-2-特征点检测（Landmark-detection）"><a href="#3-2-特征点检测（Landmark-detection）" class="headerlink" title="3.2 特征点检测（Landmark detection）"></a>3.2 特征点检测（Landmark detection）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/155.png' width="80%" height="80%"/ loading="lazy">

<p>以面部识别为例，选定脸部特征点的个数，并生成包含这些特征点的标签训练集，就可以利用神经网络输出脸部特征点的位置。</p>
<p>检测关键点也是数据集图形效果的一个关键构造模块，有了关键点，我们可以进行各种处理，比如扭曲、头戴皇冠的AR等等。</p>
<p>为了得到这样的效果，我们需要一个带有关键点的数据集，这个数据集是人工标注的。举个例子：如果有64个关键点，并且采用图中上方的标签结构，就需要一个129维的标签y。</p>
<p>对人的姿态检测问题，也可以建立关键点。</p>
<h4 id="3-3-目标检测（Object-detection）"><a href="#3-3-目标检测（Object-detection）" class="headerlink" title="3.3 目标检测（Object detection）"></a>3.3 目标检测（Object detection）</h4><p>有了上两节的目标定位和特征点检测，可以通过滑动窗口构建目标检测系统了。</p>
<p>step 1：训练一个ConvNet，输入切割好的图片，输出是否是一辆汽车，y&#x3D;0或1.</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/156.png' width="80%" height="80%"/ loading="lazy">

<p>step 2：选定一个特定大小的窗口，将窗口在图片中滑动，把每个切片输入ConvNet进行识别。一次滑动结束后，使用更大的窗口重复上述操作。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/157.png' width="80%" height="80%"/ loading="lazy">

<p>不管汽车在图片中的哪里，总有一个窗口能让汽车被识别出来。</p>
<p>滑动窗口的一个大问题是计算开销。如果步幅很大，会让输入ConvNet的窗口切片减少，但粗粒度可能会影响性能。如果采用小粒度或小步幅，传递给ConvNet的窗口切片会特别多，计算成本很高。</p>
<p>这个问题已经有了比较好的解决方法，在下节讲。</p>
<h4 id="3-4-滑动窗口的卷积实现（Convolutional-implementation-of-sliding-windows）"><a href="#3-4-滑动窗口的卷积实现（Convolutional-implementation-of-sliding-windows）" class="headerlink" title="3.4 滑动窗口的卷积实现（Convolutional implementation of sliding windows）"></a>3.4 滑动窗口的卷积实现（Convolutional implementation of sliding windows）</h4><p>首先知道怎样把全连接层转换成卷积层：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/158.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>为了代替将 5×5×16 展开成 400 维的全连接层：进行 400 个 5×5×16 卷积核的卷积，得到 1×1×400 的输出层。 1×1×400 的输出是上一层  5×5×16 激活值经过某个线性函数的输出结果。</p>
</li>
<li><p>为了代替 400 到 400 的全连接层：进行 400 个 1×1 卷积核的卷积。</p>
</li>
<li><p>为了代替 400 到 softmax分类的全连接层：进行 4 个 1×1 卷积核的卷积接一个softmax函数，最终得到 1×1×4 的输出层。</p>
</li>
</ul>
<p>滑动窗口的卷积实现：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/159.png' width="80%" height="80%"/ loading="lazy">

<p>我们有一个 14×14×3 作为输入的ConvNet。如果在 16×16×3 的大图像做滑动窗口，会划分为四部分、进行四次卷积。结果发现，这四次卷积操作中的很多计算都是重复的。</p>
<p>直接对大图进行相同的卷积操作。现在输出层为 2×2×400 而不是 1×1×400。这四次卷积分别对应输出层 2×2 的四个角。（绿色区域的卷积是其中一个例子）</p>
<p>通过把滑动窗口的很多次卷积作为一张图片输入给ConvNet进行计算，其中的公有区域可以共享很多计算。</p>
<p>同理，对 28×28×3 的图片直接进行卷积，相当于做 8×8 次步长为 2 （因为MAX POOL是2×2）的卷积，并且把结果按滑动窗口的顺序排列起来。</p>
<p>总结：对大图直接进行卷积，一次得到所有预测值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/160.png' width="80%" height="80%"/ loading="lazy">

<p>这个算法效率高，但仍然存在一个缺点：边界框的位置可能不够准确，由于步长的存在，滑动窗口可能不能很好的框选住物体。在下节解决这个问题。</p>
<p>其他：这个思路也被R-CNN借鉴，从而诞生了Fast R-cNN算法。</p>
<h4 id="3-5-Bounding-Box-预测-YOLO-算法"><a href="#3-5-Bounding-Box-预测-YOLO-算法" class="headerlink" title="3.5 Bounding Box 预测 - YOLO 算法"></a>3.5 Bounding Box 预测 - YOLO 算法</h4><p>更精准的边界框预测算法：YOLO算法。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/161.png' width="80%" height="80%"/ loading="lazy">

<p>把图片分成若干格，对每一格应用之前讲过的 classification with localization 算法。对于里面有车的格子，预测出绿色、黄色的标签；对于其他格子，预测出来紫色的标签。<strong>每个单元格负责预测中点位于该格子内的物体和物体的边界框。</strong></p>
<p>以图中 3×3 格子，长度为 8 的向量，总的输出尺寸是 3×3×8。</p>
<p>如果我们训练这样一个神经网络：输入为 100×100×3 的图片，经过一个ConvNet，最终映射到 3×3×8的输出。通过反向传播训练这个网络，使其能将任意输入x映射到这类输出向量y。这个神经网络可以输出精确的边界框。</p>
<p>细节补充：</p>
<ul>
<li><p>根据物体的中心点划分它所在的格子，即使对象可以跨越多个格子，也只会被分配到一个格子。</p>
</li>
<li><p>在实践中可以使用更精细的格子划分，比如 19×19，降低不同物体中心位于同一个格子的概率。</p>
</li>
<li><p>在YOLO中也使用滑动窗口的卷积实现，不会分别计算每个格子经过ConvNet的输出。这加速了YOLO算法的运行，实际上它的运行速度很快，可以达到实时识别。</p>
</li>
</ul>
<p>表示 bounding box 的约定：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/162.png' width="80%" height="80%"/ loading="lazy">

<p>bx, by, bh, bw 单位是相对格子尺度的比例。</p>
<ul>
<li>bx，by 必须在 0 到 1 之间。</li>
<li>bw，bh 可以大于1，因为车的尺寸可能比一个格子大。</li>
</ul>
<p>其他：YOLO论文比较难懂，一些细节很难理解。</p>
<h4 id="3-6-交并比（Intersection-over-union）"><a href="#3-6-交并比（Intersection-over-union）" class="headerlink" title="3.6 交并比（Intersection over union）"></a>3.6 交并比（Intersection over union）</h4><p>如何判断 object detection 算法运作良好？定义IoU的概念。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/163.png' width="80%" height="80%"/ loading="lazy">

<p>实际的 bounding box 是红色，我们预测的是紫色。IoU 计算它们交集（intersection）和并集（union）的比值，并跟一个阈值进行对比。</p>
<h4 id="3-7-非极大值抑制（Non-max-suppression）"><a href="#3-7-非极大值抑制（Non-max-suppression）" class="headerlink" title="3.7 非极大值抑制（Non-max suppression）"></a>3.7 非极大值抑制（Non-max suppression）</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/164.png' width="80%" height="80%"/ loading="lazy">

<p>分格子进行检测后，不仅中心点的格子会认为自己含有某个物体，周围的格子也会认为自己检测出了这个物体。导致很多格子的 P_c 值会比较高（图中的数字）。导致对同一个对象做出多次检测。</p>
<p><strong>非极大值抑制</strong>：首先看概率最大的一个，标记这里检测出了车（图中高亮）；然后逐一检查剩下的矩形，对所有与这个bounding box有很高交并比（IoU）的其他bounding box的输出抑制。</p>
<p>通过非极大值抑制，确保算法对每个对象只检测一次。</p>
<p>算法步骤：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/165.png' width="80%" height="80%"/ loading="lazy">



<h4 id="3-8-Anchor-Boxes"><a href="#3-8-Anchor-Boxes" class="headerlink" title="3.8 Anchor Boxes"></a>3.8 Anchor Boxes</h4><p>现在的问题：每个格子只能检测出一个对象。如何能让一个格子检测多个（中点位于格子内的）对象？</p>
<p>方法：设置几个anchor box。以两个为例：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/166.png' width="80%" height="80%"/ loading="lazy">

<p>如果有两个anchor box，标签变为16维，输出从 3×3×8 变为 3×3×16。</p>
<p>文字描述：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/167.png' width="80%" height="80%"/ loading="lazy">

<p>检测到的物体，寻找跟哪个 anchor box 的交并比更高，然后填入 $y$ 相应的位置。</p>
<p>一个具体例子：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/168.png' width="80%" height="80%"/ loading="lazy">

<p>如果指定 anchor box 1 大约是行人形状，anchor box 2 大约是汽车形状，同时有人和车、只有车的 $y$ 如图右侧所示。</p>
<p>异常情况：</p>
<ul>
<li><p>如果一个格子中有三个对象，但只设置了两个 anchor box</p>
</li>
<li><p>同一个格子的几个 anchor box 形状相似</p>
</li>
</ul>
<p>这两种情况发生时，算法没有好的解决方法，需要引入一些打破僵局的默认手段专门处理这些情况。</p>
<p>其他：</p>
<ul>
<li>anchor box 要处理的格子有多个对象的中点问题出现的很少（361个格子很难重复），但设立 anchor box 的好处在于能让学习算法更具有对数据集的针对性，尤其是数据集中有一些很瘦很高的对象，比如行人，或者汽车这样很宽的对象。</li>
<li>如何选择 anchor box 的形状？<ul>
<li>一般是手工指定。选择 5~10 个，涵盖想要检测的对象的各种形状。</li>
<li>另一个更高级的方法：使用 k-means ，将两类对象形状聚类，来选择最具代表性的 anchor box。</li>
</ul>
</li>
</ul>
<h4 id="3-9-整合起来：YOLO-算法"><a href="#3-9-整合起来：YOLO-算法" class="headerlink" title="3.9 整合起来：YOLO 算法"></a>3.9 整合起来：YOLO 算法</h4><p><strong>训练模型</strong>：训练一个卷积神经网络，输入图片，输出相应的标签 $y$。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/169.png' width="80%" height="80%"/ loading="lazy">



<p><strong>进行预测</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/170.png' width="80%" height="80%"/ loading="lazy">



<p><strong>进行非最大值抑制</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/171.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>对于每个格子，都有两个 anchor box 预测的结果。有些 bounding box可以超出所在格子的宽高。  </p>
</li>
<li><p>丢掉概率很小（P_c）的预测。</p>
</li>
<li><p>对每个类别，单独运行非最大值抑制。分别找到独立的行人、汽车、摩托。</p>
</li>
</ul>
<h4 id="3-10-候选区域（Region-proposals-）"><a href="#3-10-候选区域（Region-proposals-）" class="headerlink" title="3.10 *候选区域（Region proposals ）"></a>3.10 *候选区域（Region proposals ）</h4><p>在目标检测领域论文中的常见算法，在少数窗口上运行CNN分类器。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/172.png' width="80%" height="80%"/ loading="lazy">

<p>有些滑动窗口并没有进行识别的价值。因此 R-CNN 算法尝试选出一些区域，在这些区域上运行CNN分类器是有意义的。</p>
<p>运行图像分割算法，找到一些色块（如2000个），在色块上放置边界框，然后跑CNN分类器查看结果。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/173.png' width="80%" height="80%"/ loading="lazy">

<p>可以看出，R-CNN运行比较慢。</p>
<p>basic R-CNN使用某种算法求出候选区域，然后对每个候选区域跑一下CNN分类器，每个区域输出一个标签+边界框。（得到的边界框更精确，而不是色块的边界框）</p>
<p>Fast R-CNN：滑动窗口的卷积实现。加速了R-CNN。</p>
<p>得到预选区域的聚类步骤（propose regions）仍然比较缓慢。</p>
<p>Faster R-CNN：使用CNN而不是传统图像分割算法，来获得候选区域色块。</p>
<p>其他：大多数更快的R-CNN算法实现还是比YOLO慢很多。因为R-CNN需要两步，先得出预选区域，然后进行识别；而YOLO是 you only look once。</p>
<h3 id="第四周-特殊应用：人脸识别和风格迁移"><a href="#第四周-特殊应用：人脸识别和风格迁移" class="headerlink" title="第四周 特殊应用：人脸识别和风格迁移"></a>第四周 特殊应用：人脸识别和风格迁移</h3><h4 id="4-1-什么是人脸识别？"><a href="#4-1-什么是人脸识别？" class="headerlink" title="4.1 什么是人脸识别？"></a>4.1 什么是人脸识别？</h4><p>识别人脸+活体检测。</p>
<p>一些术语：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/174.png' width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>人脸验证（verification）：输入图片和身份，验证图片和身份是否对应。一对一。</p>
</li>
<li><p>人脸识别（recognition）：输入图片，识别出身份。一对多。</p>
</li>
</ul>
<p>先构建verification系统。如果表现够好，就将其用在recognition系统上。</p>
<h4 id="4-2-One-Shot学习（One-shot-learning）"><a href="#4-2-One-Shot学习（One-shot-learning）" class="headerlink" title="4.2 One-Shot学习（One-shot learning）"></a>4.2 One-Shot学习（One-shot learning）</h4><p>需要通过单张图片或单个人脸样例，就可以识别这个人的身份。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/175.png' width="80%" height="80%"/ loading="lazy">

<p>如果使用图中画出的结构，训练CNN进行softmax识别，那么如果新加入一个人，就要修改输出数量并且重新训练CNN。</p>
<p>需要做到one-shot：只通过一个样本进行学习。</p>
<p>具体来说：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/176.png' width="80%" height="80%"/ loading="lazy">

<p>不训练CNN，而是学习similarity函数 $d(img1,1mg2)$ 。这个函数输入两张图片，输出<strong>差异值</strong>。</p>
<p>用输入图片跟数据库中的图片计算差异值，跟阈值比较，进行判断。</p>
<p>添加新的图片，也可以正常工作。</p>
<h4 id="4-3-Siamese-网络（Siamese-network）"><a href="#4-3-Siamese-网络（Siamese-network）" class="headerlink" title="4.3 Siamese 网络（Siamese network）"></a>4.3 Siamese 网络（Siamese network）</h4><p>想要学习similarity函数 $d(img1,1mg2)$ ，输入两张图片，输出它们的差异度或相似度。</p>
<p>实现的一个方式就是用 Siamese 网络，思路是：<strong>对于不同的输入运行相同的CNN，然后比较它们的输出</strong>。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/177.png' width="80%" height="80%"/ loading="lazy">

<p>有一个网络，输入一张图片，可以得到一个128维的向量。如果把两张图片 $x^{(1)},x^{(2)}$ 输入同样的网络，可以得到输出向量 $f(x^{(1)}),f(x^{(2)})$ 。</p>
<p>将 $d(x^{(1)},x^{(2)})$ 定义为两张图片编码之差的范数：</p>
<p>$$d(x^{(1)},x^{(2)})&#x3D;||f(x^{(1)})-f(x^{(2)})||^2_2$$ </p>
<p>从训练层面上：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/178.png' width="80%" height="80%"/ loading="lazy">

<p>训练神经网络参数，目标是：相同人物的图片，输出相似；不同任务的图片，输出相差较大。</p>
<p>下节：有了训练目标，具体定义怎样的损失函数？</p>
<h4 id="4-4-Triplet-损失"><a href="#4-4-Triplet-损失" class="headerlink" title="4.4 Triplet 损失"></a>4.4 Triplet 损失</h4><p>想让神经网络学习到比较好的图片编码，方法之一是定义三元组损失函数（triplet loss），然后应用梯度下降。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/179.png' width="80%" height="80%"/ loading="lazy">

<p>同时看三张图片：样本图片Anchor、同样身份的图片Positive、不同身份的图片Negative。</p>
<p>我们想要：</p>
<p>$$||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 ≤ 0   $$ </p>
<p>为了确保不学习到全为0，来满足方程的编码方式：设置超参数 $\alpha$ ，作为margin。</p>
<p>$$||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + \alpha ≤ 0   $$ </p>
<p>也就是要求<strong>不同身份的编码差异度要比相同身份的编码差异度大很多</strong>。</p>
<p>triplet loss 定义如下：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/180.png' width="80%" height="80%"/ loading="lazy">

<p>$$L(A,P,N) &#x3D; max(||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + \alpha ,0   )$$ </p>
<p>$$J&#x3D;\sum^m_{i&#x3D;1}L(A^{(i)},P^{(i)},N^{(i)}) $$ </p>
<p>使用max函数，达到让第一项小于等于0的效果，而且不关心比0小多少。</p>
<p>注意：为了定义<strong>三元组的数据集</strong>，需要成对的A、P，也就是<strong>需要同一个人的多张照片</strong>。这也是图中举例的数据集，1k人有10k图片的原因。在训练结束后，就达到了one-shot的目的，可以<strong>给某个人的一张照片进行识别</strong>。</p>
<p>数据集的问题：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/181.png' width="80%" height="80%"/ loading="lazy">

<p>如果随机选择A、P、N，那么约束条件 $d(A,p)+\alpha ≤ d(A,N)$ 很容易满足，因为随机选择的图片 A和P 的差异比 A和N 的差异小很多。网络并不能从中学习到什么。</p>
<p>我们需要<strong>尽可能选择“难以识别”的三元组，组合成数据集</strong>。</p>
<p>总览：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/182.png' width="80%" height="80%"/ loading="lazy">

<p>制作三元组数据集，使用三元组loss，让模型输出一个图片的编码。</p>
<p>其他：如今已经有很多公司使用特别多的数据、训练了大型的模型，并开源了模型参数。所以相比于从头开始，可以<strong>下载别人的预训练模型</strong>。同时也要了解这些模型的训练方法。在下一节讲Siamese的变体，以及如何训练这些模型。</p>
<h4 id="4-5-面部验证与二分类（Face-verification-and-binary-classification）"><a href="#4-5-面部验证与二分类（Face-verification-and-binary-classification）" class="headerlink" title="4.5 面部验证与二分类（Face verification and binary classification）"></a>4.5 面部验证与二分类（Face verification and binary classification）</h4><p>除了triplet loss，也可以把verification当作一个二分类问题。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/183.png' width="80%" height="80%"/ loading="lazy">

<p>使用siamese的架构，两张图片分别计算编码，然后输入一个逻辑回归单元，进行预测，相同的身份输出1，不同的身份输出0。</p>
<p>注意：</p>
<ul>
<li>siamese架构，上下两个网络是相同的。</li>
<li>输入是两张图片，输出一个0或1。</li>
<li>假设上面是每次要识别的新图片，下面是数据集的图片，可以把数据集图片的编码都保存下来，不需要每次都经过网络，也不需要保存原图片。</li>
</ul>
<p>总览：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/184.png' width="80%" height="80%"/ loading="lazy">

<p>创建二元组数据集。</p>
<h4 id="4-6-什么是神经风格迁移？（neural-style-transfer）"><a href="#4-6-什么是神经风格迁移？（neural-style-transfer）" class="headerlink" title="4.6 什么是神经风格迁移？（neural style transfer）"></a>4.6 什么是神经风格迁移？（neural style transfer）</h4><p>把 Style 迁移到 Content 上。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/185.png' width="80%" height="80%"/ loading="lazy">

<p>要完成这个任务，需要组合不同深度、不同层的卷积神经网络的中间值。</p>
<h4 id="4-7-深度卷积网络在学习什么？"><a href="#4-7-深度卷积网络在学习什么？" class="headerlink" title="4.7 深度卷积网络在学习什么？"></a>4.7 深度卷积网络在学习什么？</h4><p>直观理解：卷积网络中深度较大的层在做什么。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/186.png' width="80%" height="80%"/ loading="lazy">

<p>可视化 Layer 1 的9个最大程度激活的隐藏units。可以看到，每个units在理解一些比较简单的信息，比如颜色、边缘。</p>
<p>随着层数加大，网络理解一些高维、完整的特征。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/187.png' width="80%" height="80%"/ loading="lazy">

<p>可以联想“感受野”的概念。</p>
<h4 id="4-8-代价函数"><a href="#4-8-代价函数" class="headerlink" title="4.8 代价函数"></a>4.8 代价函数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/188.png' width="80%" height="80%"/ loading="lazy">

<p>$$J(G) &#x3D; \alpha J_{content}(C, G) + \beta J_{style}(S,G)  $$  </p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/189.png' width="80%" height="80%"/ loading="lazy">

<p>获得生成图片G的步骤：</p>
<ul>
<li><p>随机初始化G</p>
</li>
<li><p>梯度下降，在最小化损失函数的过程中更新G</p>
</li>
</ul>
<h4 id="4-9-内容代价函数"><a href="#4-9-内容代价函数" class="headerlink" title="4.9 内容代价函数"></a>4.9 内容代价函数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/190.png' width="80%" height="80%"/ loading="lazy">

<p>用预训练卷积神经网络的隐藏层 $l$ 来计算内容损失，通常选择 $l$ 在网络的中间层，不太浅也不太深。</p>
<p>就像本章前面所讲，我们用这张图片的编码来计算内容损失。</p>
<p>$$J_{content}(C,G) &#x3D; \frac{1}{2}||a^{(l)(C)}- a^{(l)(G)}||^2  $$</p>
<h4 id="4-10-风格代价函数"><a href="#4-10-风格代价函数" class="headerlink" title="4.10 风格代价函数"></a>4.10 风格代价函数</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/191.png' width="80%" height="80%"/ loading="lazy">

<p>把图片的风格定义为：$l$ 层中各个通道之间激活项的相关性系数。</p>
<p>如何计算这个系数？</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/192.png' width="80%" height="80%"/ loading="lazy">

<p>比如我们有红色、黄色两个channel的激活值，如上图左下方所示，红色channel学习垂直纹理，黄色channel学习橘红色，什么时候两个通道有较高的相关性呢？</p>
<ul>
<li>图片中出现垂直纹理的地方，有很大概率是橘红色的，就说这两个channel有相关性。</li>
<li>图片中有垂直纹理的地方，很小概率是橘红色的，就说这两个channel没有相关性。</li>
</ul>
<p>相关系数度量的就是这个概率。</p>
<p>在生成图像G中，测量channel之间的相关系数，与S的相关系数做对比。这样就能测量生成图像G的风格和输入图像S的相似程度了。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/193.png' width="80%" height="80%"/ loading="lazy">

<p>对于G和S，分别计算一个风格矩阵。</p>
<ul>
<li>用第 $l$ 层来测量风格，$a^{[l]}_{i,j,k} $ 是 $l$ 层中 $(i,j,k)$ 位置的激活值。$i,j,k$ 分别是高度、宽度、channel。</li>
<li>$G^{(l)(S)}$ 是第 $l$ 层的风格矩阵，规格是 $n_c^{(l)} \times n_c^{(l)}$。<ul>
<li>$S$ 的风格矩阵：$G_{kk’}^{(l)(S)}&#x3D;\sum^{n^{(l)}_H}_{i&#x3D;1}\sum^{n^{(l)}_W}_{j&#x3D;1}a_{ijk}^{(l)(S)}a_{ijk’}^{(l)(S)}  $ </li>
<li>$G$ 的风格矩阵：$G_{kk’}^{(l)(G)}&#x3D;\sum^{n^{(l)}_H}_{i&#x3D;1}\sum^{n^{(l)}_W}_{j&#x3D;1}a_{ijk}^{(l)(G)}a_{ijk’}^{(l)(G)}  $ </li>
<li>用 $G$ 这个字母来表示是因为在线性代数中这也叫 “gram matrix”</li>
<li>做的事情：遍历图中各个高度和宽度，将 $k$ 和 $k’$ 通道中对应位置的激活项相乘并求和。如果两个通道对应的激活值 $a$ 相关，$G$ 就大。</li>
</ul>
</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/194.png' width="80%" height="80%"/ loading="lazy">

<p>最终的风格损失函数：</p>
<p>$$ J_{style}^{(l)}(S,G) &#x3D; ||G^{(l)(S)} - G^{(l)(G)}  ||^2_F    $$</p>
<p>也可以定义神经网络所有层的风格损失函数：</p>
<p>$$ J_{style}(S,G)&#x3D;\sum _l \lambda^{(l)} J_{style}^{(l)}(S,G)   $$</p>
<h4 id="4-11-图像的一维和三维扩展"><a href="#4-11-图像的一维和三维扩展" class="headerlink" title="4.11 图像的一维和三维扩展"></a>4.11 图像的一维和三维扩展</h4><p>从2D图像到1D心电图：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/195.png' width="80%" height="80%"/ loading="lazy">

<p>用一个 1×5 filter 卷积 1×14 原数据，得到 1×10 的输出。</p>
<p>多通道filters同理。用32个 5×16 filter 卷积 10×16 原数据，得到 6×32 的输出。</p>
<p>当然也有RNN等专门处理序列化数据的网络。</p>
<p>3D数据举例：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/196.png' width="80%" height="80%"/ loading="lazy">

<p>CT扫描，若干的切片。</p>
<p>3D数据的卷积如下：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/197.png' width="80%" height="80%"/ loading="lazy">





<h2 id="第五课-序列模型-Sequence-Models"><a href="#第五课-序列模型-Sequence-Models" class="headerlink" title="第五课 序列模型(Sequence Models)"></a>第五课 序列模型(Sequence Models)</h2><p>暂时空缺</p>
<h1 id="第四部分：Tensorflow-2-0-笔记"><a href="#第四部分：Tensorflow-2-0-笔记" class="headerlink" title="第四部分：Tensorflow 2.0 笔记"></a>第四部分：Tensorflow 2.0 笔记</h1><p>整理自北大曹建老师网课，见<a href="https://www.bilibili.com/video/BV1B7411L7Qt">此处</a></p>
<h3 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1  基本概念"></a>1  基本概念</h3><h4 id="1-1-张量-Tensor"><a href="#1-1-张量-Tensor" class="headerlink" title="1.1 张量 Tensor"></a>1.1 张量 Tensor</h4><p>Tensor：多维数组（列表）。阶：张量的维数。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf1.png'  width="80%" height="80%"/ loading="lazy">

<p>数据类型：</p>
<ul>
<li>整型、浮点型：<code>tf.int32</code> <code>tf.float32</code> <code>tf.float64</code> </li>
<li>bool型：<code>tf.constant([True, False])</code> </li>
<li>string型：<code>tf.constant(&quot;Hello, world.&quot;)</code></li>
</ul>
<p>创建张量：</p>
<ul>
<li><p>z直接创建：<code>tf.constant(张量内容, dtype=数据类型)</code></p>
</li>
<li><p>将numpy转换为Tensor：<code>tf.convert_to_tensor(原数据名, dtype=数据类型)</code></p>
</li>
<li><p>特殊张量：<code>tf.zeros(维度)</code> <code>tf.ones(维度)</code> <code>tf.fill(维度，指定值)</code>，如<code>tf.fill([3, 2, 4], 10)</code></p>
</li>
</ul>
<blockquote>
<p>维度的表示：向量[ , , ]的逗号隔开每个维度的元素个数。有几个逗号就代表tensor有+1维。</p>
</blockquote>
<ul>
<li>随机张量：<ul>
<li>正态分布：<code>tf.random.normal(维度, mean=均值, stddev=标准差)</code></li>
<li>截断式正态分布：<code>tf.random.truncated_normal(维度, mean=均值, stddev=被侦测)</code></li>
<li>均匀分布随机数：<code>tf.random.uniform(维度, minval=最小值, maxval=最大值)</code></li>
</ul>
</li>
</ul>
<blockquote>
<p>区间左闭右开</p>
</blockquote>
<h4 id="1-2-常用函数"><a href="#1-2-常用函数" class="headerlink" title="1.2 常用函数"></a>1.2 常用函数</h4><ul>
<li><p><code>tf.cast(原张量名, dtype=数据类型)</code> ，强制类型转换</p>
</li>
<li><p><code>tf.reduce_min(张量名)</code> <code>tf.reduce_max(张量名)</code> ，计算张量维度上的最值</p>
</li>
<li><p>axis：在一个二位张量或数组中，可以通过axis控制执行维度。</p>
<ul>
<li>axis&#x3D;0 代表跨第一个维度（跨行，经度，纵向操作，down）</li>
<li>axis&#x3D;1 代表跨第二个维度（跨列，纬度，横向操作，across）</li>
</ul>
</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf2.png'  width="80%" height="80%"/ loading="lazy">

<ul>
<li><p><code>tf.Variable()</code> ，标记为“可训练”。被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练的参数。</p>
</li>
<li><p>如：<code>w = tf.Variable(tf.random.normal([2, 2], mean=0, stddev=1))</code></p>
</li>
<li><p>数学运算：</p>
<ul>
<li>四则运算：<code>tf.add</code> ，<code>tf.subtract</code> ，<code>tf.multiply</code> ，<code>tf.divide</code>，参数为两个张量，必须维度相同。</li>
<li>指数运算：<code>tf.square</code> ，<code>tf.pow</code> ，<code>tf.sqrt</code></li>
<li>矩阵乘法：<code>tf.matmul</code></li>
</ul>
</li>
<li><p><code>tf.data.Dataset.from_tensor_slices((输入特征，标签))</code>，把特征值和标签配对。Numpy和Tensor格式都可用该语句读入数据</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">features <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">17</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
labels <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_tensor_slices<span class="token punctuation">(</span><span class="token punctuation">(</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<ul>
<li>在with结构中，使用GradientTape，实现某个函数对指定参数的求导运算。with结构记录计算过程，gradient求出张量的梯度</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
	w <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
	loss <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
grad <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> w<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>grad<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>梯度是 2*w&#x3D;6.0，运行结果：<code>tf.Tensor(6.0, shape=(), dtype=float32)</code></p>
</li>
<li><p><code>enumerate()</code>，索引，返回索引和元素：</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">seq <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'one'</span><span class="token punctuation">,</span> <span class="token string">'two'</span><span class="token punctuation">,</span> <span class="token string">'three'</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> element <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> element<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<pre class="language-markdown" data-language="markdown"><code class="language-markdown">0 one
1 two
2 three</code></pre>
</li>
<li><p><code>tf.one_hot(带转换数据, depth=几分类)</code>，转换成one-hot编码：</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">calsses <span class="token operator">=</span> <span class="token number">3</span>
labels <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span> <span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> tf<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> depth<span class="token operator">=</span>classes<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<pre class="language-markdown" data-language="markdown"><code class="language-markdown">([[0. 1. 0.]
  [1. 0. 0.]
  [0. 0. 1.]], shape=(3, 3), dtype=float32)</code></pre>
</li>
<li><p><code>tf.nn.softmax(张量)</code>，$Softmax(y_i)&#x3D;\frac{e^{y_i}}{\sum_{j&#x3D;0}^ne^{y_i}}$，使输出符合概率分布</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">y <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.01</span><span class="token punctuation">,</span> <span class="token number">2.01</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.66</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y_pro <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y_pro<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<pre class="language-markdown" data-language="markdown"><code class="language-markdown">tf.Tensor([0.22598174 0.69583046 0.0481878 ], shape=(3,), dtype=float32)</code></pre>
</li>
<li><p><code>w.assign_sub(w要自减的值) </code>，参数自更新</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">w <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
w<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<p><code>&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=int32, numpy=3&gt;</code></p>
</li>
<li><p><code>tf.argmax(张量名, axis=操作轴)</code>，返回张量沿指定维度最大值的索引</p>
</li>
</ul>
<h3 id="2-优化方法"><a href="#2-优化方法" class="headerlink" title="2 优化方法"></a>2 优化方法</h3><h4 id="2-1-一些函数"><a href="#2-1-一些函数" class="headerlink" title="2.1 一些函数"></a>2.1 一些函数</h4><ul>
<li><code>tf.where(条件语句, A, B)</code>，条件为真返回A，条件为假返回B</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> tf<span class="token punctuation">.</span>where<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>greater<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<p><code>tf.Tensor([1, 2, 3, 4, 5], shape=(5,), dtype=int32)</code></p>
</li>
<li><p><code>np.random.RandomState.rand(维度)</code>，返回指定维度的 [0, 1] 的随机数。维度为空则返回一个标量</p>
</li>
<li><p><code>np.vstack(数组1, 数组2)</code>，将两个数组按垂直方向叠加</p>
</li>
<li><p>构建网格坐标点：</p>
<ul>
<li><p><code>np.mgrid[起始值:结束值:步长, 起始值:结束值:步长, ...]</code></p>
</li>
<li><p><code>x.ravel()</code>，把x变为一维数组，把变量x拉直</p>
</li>
<li><p><code>np.c_[数组1, 数组2, ...]</code>，使返回的间隔数值点配对</p>
</li>
</ul>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">x<span class="token punctuation">,</span> y <span class="token operator">=</span> np<span class="token punctuation">.</span>mgrid<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">:</span><span class="token number">0.5</span><span class="token punctuation">]</span>
grid <span class="token operator">=</span> np<span class="token punctuation">.</span>c_<span class="token punctuation">[</span>x<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x:"</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"y:"</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"grid:"</span><span class="token punctuation">,</span> grid<span class="token punctuation">)</span></code></pre>

<ul>
<li><p>运行结果：</p>
<pre class="language-markdown" data-language="markdown"><code class="language-markdown">x:[[1. 1. 1. 1.]
   [2. 2. 2. 2.]]
y:[[2.  2.5 3.  3.5]
   [2.  2.5 3.  3.5]]
grid:
 [[1.  2. ]
  [1.  2.5]
  [1.  3. ]
  [1.  3.5]
  [2.  2. ]
  [2.  2.5]
  [2.  3. ]
  [2.  3.5]]</code></pre></li>
</ul>
<h4 id="2-2-指数衰减学习率"><a href="#2-2-指数衰减学习率" class="headerlink" title="2. 2 指数衰减学习率"></a>2. 2 指数衰减学习率</h4><pre class="language-none"><code class="language-none">LR_BASE &#x3D; 0.2
LR_DECAY &#x3D; 0.99
LR_STEP &#x3D; 1
for epoch in range(epoch):
lr &#x3D; LR_BASE * LR_DECAY ** (epoch &#x2F; LR_STEP)</code></pre>

<h4 id="2-3-激活函数"><a href="#2-3-激活函数" class="headerlink" title="2.3 激活函数"></a>2.3 激活函数</h4><ul>
<li><code>tf.nn.sigmoid(x)</code>，sigmoid的导数为 [0, 0.25] 的小数，多层神经网络链式求导时，如果连续相乘会造成梯度消失</li>
<li><code>tf.math.tanh(x)</code>，输出是0均值了，但依然存在梯度消失、幂运算问题</li>
<li><code>tf.nn.relu(x)</code>，解决了梯度消失问题，并且计算速度快；输出非0均值，收敛慢，并且存在dead relu问题<ul>
<li>dead relu是经过relu函数的负数特征过多导致的，可以合理参数初始化、设置小的学习率</li>
</ul>
</li>
<li><code>tf.nn.leaky_relu(x)</code>，解决了dead relu问题</li>
</ul>
<p>建议：首选relu函数；学习率设置较小值；输入特征标准化；初始化参数中心化</p>
<h4 id="2-4-损失函数"><a href="#2-4-损失函数" class="headerlink" title="2.4 损失函数"></a>2.4 损失函数</h4><ul>
<li><p>均方误差mse：$MSE(y_,y) &#x3D; \frac{\sum_{i&#x3D;1}^n(y-y_)^2}{n}$ </p>
<ul>
<li><code>loss_mse = tf.reduce_mean(tf.square(y_ - y))</code></li>
</ul>
</li>
<li><p>交叉熵损失函数ce：$H(y_, y) &#x3D; -\Sigma y_*lny$ </p>
<ul>
<li><code>loss_ce = tf.losses.categorical_crossentropy(y_, y)</code> </li>
<li>softmax与交叉熵结合，输出先进行softmax，再计算y与y_的交叉熵损失函数<ul>
<li><code>tf.nn.softmax_cross_entropy_with_logits(y_, y)</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2-5-缓解过拟合"><a href="#2-5-缓解过拟合" class="headerlink" title="2.5 缓解过拟合"></a>2.5 缓解过拟合</h4><ul>
<li>欠拟合的解决方法：增加模型复杂度，增加特征，增加参数；减小正则化系数</li>
<li>过拟合的解决方法：数据清洗；增大数据集；增大正则化参数</li>
</ul>
<p>正则化：</p>
<ul>
<li><p><code>loss = loss(y与y_)+ REGULARIZER *loss(w)​</code></p>
<ul>
<li><p>$loss_{L1}(w) &#x3D; \sum_i|w_i|$ ，L1正则化会让参数变为0，减少参数数量，降低复杂度</p>
</li>
<li><p>$loss_{L2}(w) &#x3D; \sum_i|w_i^2|$ ，L2正则化会使参数接近0但不为0，降低复杂度</p>
</li>
</ul>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">loss_mse <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>y_train <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">)</span>
loss_regularization <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
loss_regularization<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>l2_loss<span class="token punctuation">(</span>w1<span class="token punctuation">)</span><span class="token punctuation">)</span>
loss_regularizaiton<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>l2_loss<span class="token punctuation">(</span>w2<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
求和，例：
x = tf.constant(([1, 1, 1], [1, 1, 1]))
tf.reduce_sum(x)
>>>6
'''</span>
loss_regularization <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>loss_regularization<span class="token punctuation">)</span>
loss <span class="token operator">=</span> loss_mse <span class="token operator">+</span> <span class="token number">0.03</span> <span class="token operator">*</span> loss_regularization</code></pre>

<h4 id="2-6-优化器"><a href="#2-6-优化器" class="headerlink" title="2.6 优化器"></a>2.6 优化器</h4><p>待优化参数$w$，损失函数$loss$，学习率$lr$，每次迭代一个$batch$，$t$表示当前$batch$迭代的总次数：</p>
<ol>
<li>计算 $t$ 时刻损失函数关于参数的梯度 $g_t &#x3D; \nabla loss &#x3D; \frac{\partial loss}{\partial w_t}$ </li>
<li>计算 $t$ 时刻一阶动量 $m_t$ 和二阶动量 $V_t$ </li>
<li>计算 $t$ 时刻下降梯度 $\eta_t &#x3D; lr \sdot m_t&#x2F;\sqrt{V_t} $ </li>
<li>计算 $t+1$ 时刻参数 $w_{t+1} &#x3D; w_t - \eta_t &#x3D; w_t - lr \sdot m_t &#x2F; \sqrt{V_t}$</li>
</ol>
<h5 id="几种优化器"><a href="#几种优化器" class="headerlink" title="几种优化器"></a>几种优化器</h5><p>SGD：随机梯度下降（无momentum）</p>
<ul>
<li><p>$m_t &#x3D; g_t \qquad V_t &#x3D; 1$ </p>
</li>
<li><p>$w_{t+1} &#x3D; w_t - lr \sdot \frac{\partial loss}{\partial w_t} $</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">w1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>

<p>SGDM：含momentum的SGD</p>
<ul>
<li><p>$m_t &#x3D; \beta \sdot m_{t-1}+(1-\beta)\sdot g_t \qquad V_t &#x3D; 1$ </p>
</li>
<li><p>$w_{t+1} &#x3D; w_t - lr \sdot (\beta \sdot m_{t-1}+(1-\beta)\sdot g_t) $</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">m_w <span class="token operator">=</span> beta <span class="token operator">*</span> m_w <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta<span class="token punctuation">)</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
m_b <span class="token operator">=</span> beta <span class="token operator">*</span> m_b <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta<span class="token punctuation">)</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
w1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> m_w<span class="token punctuation">)</span>
b1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> m_b<span class="token punctuation">)</span></code></pre>

<p>Adagrad：在SGD（无momentum）的基础上增加二阶动量</p>
<ul>
<li><p>$m_t &#x3D; g_t \qquad V_t &#x3D; \sum^t_{\tau&#x3D;1}g_\tau^2$ </p>
</li>
<li><p>$w_{t+1} &#x3D; w_t - lr \sdot g_t&#x2F;(\sqrt{\sum^t_{\tau&#x3D;1}g_\tau^2})$</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">v_w<span class="token punctuation">,</span> v_b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
<span class="token comment"># adagrad</span>
v_w <span class="token operator">+=</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
v_b <span class="token operator">+=</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
w1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_w<span class="token punctuation">)</span><span class="token punctuation">)</span>
b1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_b<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<p>RMSProp：在SGD（无momentum）的基础上增加二阶动量</p>
<ul>
<li><p>$m_t &#x3D; g_t \qquad V_t &#x3D; \beta \sdot V_{t-1}+(1-\beta) \sdot g_t^2$ </p>
</li>
<li><p>$w_{t+1} &#x3D; w_t - lr \sdot g_t&#x2F;(\sqrt{\beta \sdot V_{t-1}+(1-\beta) \sdot g_t^2})$</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">v_w<span class="token punctuation">,</span> v_b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
beta <span class="token operator">=</span> <span class="token number">0.9</span>
<span class="token comment"># rmsprop</span>
v_w <span class="token operator">=</span> beta <span class="token operator">*</span> v_w <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
v_b <span class="token operator">=</span> beta <span class="token operator">*</span> v_b <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
w1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_w<span class="token punctuation">)</span><span class="token punctuation">)</span>
b1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_b<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<p>Adam：同时结合SGDM一阶动量和RMSProp二阶动量</p>
<ul>
<li><p>$m_t &#x3D; \beta_1 \sdot m_{t-1}+(1-\beta_1)\sdot g_t \qquad V_t &#x3D; \beta_2 \sdot V_{step-1}+(1-\beta_2) \sdot g_t^2$ </p>
</li>
<li><p>修正一阶动量的偏差 $\widehat{m_t} &#x3D; \frac{m_t}{1-\beta_1^t}$ </p>
</li>
<li><p>修正二阶动量的偏差 $\widehat{V_t} &#x3D; \frac{V_t}{1-\beta_2^t}$ </p>
</li>
<li><p>$w_{t+1} &#x3D; w_t - lr \sdot \frac{m_t}{1-\beta_1^t}&#x2F;\sqrt{\frac{V_t}{1-\beta_2^t}}$</p>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">m_w<span class="token punctuation">,</span> m_b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
v_w<span class="token punctuation">,</span> v_b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
beta1<span class="token punctuation">,</span> beta2 <span class="token operator">=</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span>
delta_w<span class="token punctuation">,</span> delta_b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
global_step <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># adam</span>
m_w <span class="token operator">=</span> beta1 <span class="token operator">*</span> m_w <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta1<span class="token punctuation">)</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
m_b <span class="token operator">=</span> beta1 <span class="token operator">*</span> m_b <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta1<span class="token punctuation">)</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
v_w <span class="token operator">=</span> beta2 <span class="token operator">*</span> v_w <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta2<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
v_b <span class="token operator">=</span> beta2 <span class="token operator">*</span> v_b <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> beta2<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

m_w_correction <span class="token operator">=</span> m_w <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>beta1<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>global_step<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
m_b_correction <span class="token operator">=</span> m_b <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>beta1<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>global_step<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
v_w_correction <span class="token operator">=</span> v_w <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>beta2<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>global_step<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
v_b_correction <span class="token operator">=</span> v_b <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>beta2<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>global_step<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

w1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> m_w_correction <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_w_correction<span class="token punctuation">)</span><span class="token punctuation">)</span>
b1<span class="token punctuation">.</span>assign_sub<span class="token punctuation">(</span>lr <span class="token operator">*</span> m_b_correction <span class="token operator">/</span> tf<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>v_b_correction<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>





<h3 id="3-神经网络搭建八股"><a href="#3-神经网络搭建八股" class="headerlink" title="3 神经网络搭建八股"></a>3 神经网络搭建八股</h3><h4 id="3-1-使用Sequential搭建神经网络"><a href="#3-1-使用Sequential搭建神经网络" class="headerlink" title="3.1 使用Sequential搭建神经网络"></a>3.1 使用Sequential搭建神经网络</h4><p>tf.keras搭建神经网络六步法（使用Sequential）：</p>
<ol>
<li><p>import</p>
</li>
<li><p>train, test：划分数据集</p>
</li>
<li><p>model &#x3D; tf.keras.models.Sequential：逐层描述网络结构，前向传播</p>
</li>
<li><p>model.compile：配置训练方法，选择优化器、损失函数、评测指标</p>
</li>
<li><p>model.fit：执行训练过程</p>
</li>
<li><p>model.summary：打印网络的结构和参数统计</p>
</li>
</ol>
<h5 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h5><p><code>model = tf.keras.models.Sequential([网络结构])</code> </p>
<ul>
<li>拉直层：<code>tf.keras.layers.Flatten()</code> </li>
<li>全连接层：<code>tf.keras.layers.Dense(神经元个数, activation=&quot;激活函数&quot;, kernel_regularizer=哪种正则化)</code> <ul>
<li>激活函数：<code>relu</code>, <code>softmax</code>, <code>sigmoid</code>, <code>tanh</code> </li>
<li>正则化：<code>tf.keras.regularizers.l1()</code>, <code>tf.keras.regularizers.l2()</code></li>
</ul>
</li>
<li>卷积层：<code>tf.keras.layers.Conv2D(fitlers=卷积核个数, kernel_size=卷积核尺寸, strides=步长, padding=&quot;valid&quot; or &quot;same&quot;) </code></li>
<li>LSTM层：<code>tf.keras.layers.LSTM()</code></li>
</ul>
<h5 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h5><p><code>model.compile(optimizer=优化器, loss=损失函数, metrics=[&quot;准确率&quot;])</code> </p>
<ul>
<li>optimizer 可选：<ul>
<li><code>&#39;sgd&#39;</code> or <code>tf.keras.optimizers.SGD(lr=学习率, momentum=动量)</code> </li>
<li><code>&#39;adagrad&#39;</code> or <code>tf.keras.optimizers.Adagrad(lr=学习率)</code> </li>
<li><code>&#39;adadelta&#39;</code> or <code>tf.keras.optimizers.Adadelta(lr=学习率)</code> </li>
<li><code>&#39;adam&#39;</code> or <code>tf.keras.optimizers.Adam(lr=学习率, beta_1=0.9, beta_2=0.999)</code></li>
</ul>
</li>
<li>loss 可选： <ul>
<li><code>&#39;mse&#39;</code> or <code>tf.keras.losses.MeanSquaredError()</code> </li>
<li><code>&#39;sparse_categorical_crossentropy&#39;</code> or <code>tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)</code> ，神经网络预测前没有经过概率分布则是True，经过概率分布就是False</li>
</ul>
</li>
<li>metrics 可选：<ul>
<li><code>accuracy</code>：y_ 和 y 都是数值，如 y_&#x3D;[1]  y&#x3D;[1]</li>
<li><code>categorical_accuracy</code>：y_ 和 y 都是独热码，如 y_&#x3D;[0, 1, 0]  y&#x3D;[0.256, 0.695, 0.048]</li>
<li><code>sparse_categorical_accuracy</code>：y_ 是数值，y 是独热码，如 y_&#x3D;[1]  y&#x3D;[0.256, 0.695, 0.048]</li>
</ul>
</li>
</ul>
<h5 id="fit"><a href="#fit" class="headerlink" title="fit"></a>fit</h5><pre class="language-python" data-language="python"><code class="language-python">model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>训练集特征<span class="token punctuation">,</span> 训练集标签<span class="token punctuation">,</span> 
batch_size<span class="token operator">=</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token punctuation">,</span> 
validation_data<span class="token operator">=</span><span class="token punctuation">(</span>测试集特征<span class="token punctuation">,</span> 测试集标签<span class="token punctuation">)</span> <span class="token keyword">or</span> validation_split<span class="token operator">=</span>从训练集划分多少比例给测试集<span class="token punctuation">,</span> validation_freq<span class="token operator">=</span>多少epoch测试一次<span class="token punctuation">)</span></code></pre>

<h5 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h5><p><code>model.summary()</code></p>
<h5 id="举例：鸢尾花识别"><a href="#举例：鸢尾花识别" class="headerlink" title="举例：鸢尾花识别"></a>举例：鸢尾花识别</h5><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 1. import</span>
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># 2. train, test</span>
x_train <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data
y_train <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>target

np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>y_train<span class="token punctuation">)</span>
tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span>set_seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>

<span class="token comment"># 3. Sequential</span>
model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 4. compile</span>
model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 5. fit</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> validation_split<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>

<span class="token comment"># 6. summary</span>
model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>



<h4 id="3-2-使用class搭建神经网络"><a href="#3-2-使用class搭建神经网络" class="headerlink" title="3.2  使用class搭建神经网络"></a>3.2  使用class搭建神经网络</h4><p>Sequential支持搭建上层输入是下层输出的神经网络，如果有跳连，可以用class搭建。</p>
<p>tf.keras搭建神经网络六步法（使用class）：</p>
<ol>
<li><p>import</p>
</li>
<li><p>train, test</p>
</li>
<li><p><strong>class MyModel(Model) model&#x3D;MyModel</strong></p>
</li>
<li><p>model.compile</p>
</li>
<li><p>model.fit</p>
</li>
<li><p>model.summary</p>
</li>
</ol>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token builtin">super</span><span class="token punctuation">(</span>MyModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
		定义网络结构块
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    	调用网络结构块，实现前向传播
    	<span class="token keyword">return</span> y
model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<p>以鸢尾花分类的网络为例：</p>
 <pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 1. import</span>
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras <span class="token keyword">import</span> Model
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># 2. train, test</span>
x_train <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data
y_train <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>target

np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>y_train<span class="token punctuation">)</span>
tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span>set_seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>

<span class="token comment"># 3. class</span>
<span class="token keyword">class</span> <span class="token class-name">IrisModel</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token builtin">super</span><span class="token punctuation">(</span>IrisModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>d1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>regularizers<span class="token punctuation">.</span>l2<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    	y <span class="token operator">=</span> self<span class="token punctuation">.</span>d1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    	<span class="token keyword">return</span> y
    
model <span class="token operator">=</span> IrisModel<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 4. compile</span>
model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 5. fit</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> validation_split<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>

<span class="token comment"># 6. summary</span>
model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>



<h4 id="3-3-MNIST-数据集"><a href="#3-3-MNIST-数据集" class="headerlink" title="3.3 MNIST 数据集"></a>3.3 MNIST 数据集</h4><p>MNIST数据集有 7 万张 28*28 像素的手写数字，其中 6 万张用于训练，1 万张用于测试。</p>
<h5 id="Sequential-1"><a href="#Sequential-1" class="headerlink" title="Sequential"></a>Sequential</h5><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

mnist <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>mnist
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
x_train<span class="token punctuation">,</span> x_test <span class="token operator">=</span> x_train <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> x_test <span class="token operator">/</span> <span class="token number">255.0</span>  <span class="token comment"># 把[0, 255]变为[0, 1]，输入特征值小更易于神经网络吸收</span>

model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span> 
              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<h5 id="class"><a href="#class" class="headerlink" title="class"></a>class</h5><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense<span class="token punctuation">,</span> Flatten
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras <span class="token keyword">import</span> Model

mnist <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>mnist
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
x_train<span class="token punctuation">,</span> x_test <span class="token operator">=</span> x_train <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> x_test <span class="token operator">/</span> <span class="token number">255.0</span>

<span class="token keyword">class</span> <span class="token class-name">MnistModel</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MnistModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d2 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
	<span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>d2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y

model <span class="token operator">=</span> MnistModel<span class="token punctuation">(</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span> 
              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>





<h3 id="4-八股扩展"><a href="#4-八股扩展" class="headerlink" title="4 八股扩展"></a>4 八股扩展</h3><p>自制数据集、数据增强、断点续训（实时存取模型）、参数提取（把参数存入文本）、acc&#x2F;loss可视化、应用程序。</p>
<h4 id="4-1-自制数据集"><a href="#4-1-自制数据集" class="headerlink" title="4.1 自制数据集"></a>4.1 自制数据集</h4><p>读写文件、建立数据集的操作，详见代码。</p>
<h4 id="4-2-数据增强"><a href="#4-2-数据增强" class="headerlink" title="4.2 数据增强"></a>4.2 数据增强</h4><pre class="language-python" data-language="python"><code class="language-python">image_gen_train <span class="token operator">=</span> ImageDataGenerator<span class="token punctuation">(</span>
	rescale <span class="token operator">=</span> 所有数据将乘以该值<span class="token punctuation">,</span> 
    rotation_range <span class="token operator">=</span> 随机旋转角度<span class="token punctuation">,</span> 
    width_shift_range <span class="token operator">=</span> 随机宽度偏移量<span class="token punctuation">,</span> 
    height_shift_range <span class="token operator">=</span> 随机高度偏移量<span class="token punctuation">,</span> 
    horizontal_flip <span class="token operator">=</span> 是否随机水平翻转<span class="token punctuation">,</span> 
    zoom_range <span class="token operator">=</span> 随即缩放 
<span class="token punctuation">)</span>
image_gen_train<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span></code></pre>

<p>x_train要求是四维数据，需要进行reshape，最后一个是通道数量：<code>x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)</code> </p>
<p>model.fit步骤变为：<code>model.fit(image_gen_train.flow(x_train, y_train, batch_size=32))</code> </p>
<p>数据增强的效果需要在实际应用程序中体会。</p>
<h4 id="4-3-断点续训"><a href="#4-3-断点续训" class="headerlink" title="4.3 断点续训"></a>4.3 断点续训</h4><h5 id="读取模型"><a href="#读取模型" class="headerlink" title="读取模型"></a>读取模型</h5><p><code>load_weights(路径) </code></p>
<p>保存模型时，会自动生成.index索引表文件。如果路径中已经有保存好的模型，就直接加载模型参数：</p>
<pre class="language-python" data-language="python"><code class="language-python">checkpoint_save_path <span class="token operator">=</span> <span class="token string">"./checkpoint/mnist.ckpt"</span>
<span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>checkpoint_save_path <span class="token operator">+</span> <span class="token string">'.index'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'---load the model---'</span><span class="token punctuation">)</span>
	model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>checkpoint_save_path<span class="token punctuation">)</span></code></pre>

<h5 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h5><pre class="language-python" data-language="python"><code class="language-python">cp_callback <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>ModelCheckpoint<span class="token punctuation">(</span>
    filepath<span class="token operator">=</span>checkpoint_save_path<span class="token punctuation">,</span> <span class="token comment"># 文件存储路径</span>
    save_weights_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># 是否只保留模型参数</span>
    save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>	<span class="token comment"># 是否只保留最优结果</span>

history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> 
                   	validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_data<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> 
                    callbacks<span class="token operator">=</span><span class="token punctuation">[</span>cp_callback<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 模型训练时加入 callbacks 选项，记录到 history 中</span></code></pre>



<h4 id="4-4-参数提取"><a href="#4-4-参数提取" class="headerlink" title="4.4 参数提取"></a>4.4 参数提取</h4><p><code>model.trainable_variables</code> 返回模型中可训练参数。</p>
<p><code>np.set_printoptions(threshold=超过多少省略显示)</code> </p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">)</span>
<span class="token builtin">file</span> <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'./weights.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> v <span class="token keyword">in</span> model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">:</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>name<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
<span class="token builtin">file</span><span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>



<h4 id="4-5-acc-x2F-loss-曲线"><a href="#4-5-acc-x2F-loss-曲线" class="headerlink" title="4.5 acc&#x2F;loss 曲线"></a>4.5 acc&#x2F;loss 曲线</h4><p>在 model.fit 执行训练过程的同时，同步记录了：</p>
<ul>
<li>训练集loss：<code>loss </code> </li>
<li>测试集loss：<code>val_loss </code> </li>
<li>训练集准确率：<code>sparse_categorical_accuracy </code> </li>
<li>测试集准确率：<code>val_sparse_categorical_accuracy</code></li>
</ul>
<p>可用 history.history 提取出来。</p>
<pre class="language-python" data-language="python"><code class="language-python">history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> 
                   	validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_data<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> 
                    callbacks<span class="token operator">=</span><span class="token punctuation">[</span>cp_callback<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>

<pre class="language-python" data-language="python"><code class="language-python">acc <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span>
val_acc <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_sparse_categorical_accuracy'</span><span class="token punctuation">]</span>
loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span>
val_loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_loss'</span><span class="token punctuation">]</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>acc<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Training Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>val_acc<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Validation Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training and Validation Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Training Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>val_loss<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Validation Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training and Validation Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>



<h4 id="4-6-模型应用程序"><a href="#4-6-模型应用程序" class="headerlink" title="4.6 模型应用程序"></a>4.6 模型应用程序</h4><p><code>predict(输入特征, batch_size=)</code> 返回前向传播计算结果。</p>
<p>分以下几步：</p>
<ul>
<li>复现模型的前向传播Sequential</li>
<li>加载模型参数</li>
<li>进行预测</li>
</ul>
<p>附手写数字识别任务的代码：</p>
<p><strong>A. 模型训练和保存</strong></p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> os
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt


mnist <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>mnist
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
x_train<span class="token punctuation">,</span> x_test <span class="token operator">=</span> x_train <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> x_test <span class="token operator">/</span> <span class="token number">255.0</span>  <span class="token comment"># 把[0, 255]变为[0, 1]，输入特征值小更易于神经网络吸收</span>

model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>
              loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 查看有没有可以加载的模型</span>
checkpoint_save_path <span class="token operator">=</span> <span class="token string">"./checkpoint/mnist.ckpt"</span>
<span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>checkpoint_save_path <span class="token operator">+</span> <span class="token string">'.index'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'-------------load the model-----------------'</span><span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>checkpoint_save_path<span class="token punctuation">)</span>

cp_callback <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>ModelCheckpoint<span class="token punctuation">(</span>filepath<span class="token operator">=</span>checkpoint_save_path<span class="token punctuation">,</span>
                                                 save_weights_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                                 save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>


history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>
          validation_data<span class="token operator">=</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">,</span> validation_freq<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
          callbacks<span class="token operator">=</span><span class="token punctuation">[</span>cp_callback<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 通过调用回调函数，保存模型</span>

model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token comment"># 参数写入txt</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">)</span>
<span class="token builtin">file</span> <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'./weights.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> v <span class="token keyword">in</span> model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">:</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>name<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
    <span class="token builtin">file</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
<span class="token builtin">file</span><span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token comment"># acc/loss可视化</span>
acc <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'sparse_categorical_accuracy'</span><span class="token punctuation">]</span>
val_acc <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_sparse_categorical_accuracy'</span><span class="token punctuation">]</span>
loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span>
val_loss <span class="token operator">=</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">'val_loss'</span><span class="token punctuation">]</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>acc<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Training Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>val_acc<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Validation Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training and Validation Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Training Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>val_loss<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Validation Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training and Validation Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>



<p><strong>B. 模型加载和预测</strong></p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

model_save_path <span class="token operator">=</span> <span class="token string">'./checkpoint/mnist.ckpt'</span>

<span class="token comment"># 复现前向传播</span>
model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 加载模型</span>
model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>model_save_path<span class="token punctuation">)</span>


preNum <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">(</span><span class="token string">"执行几次识别任务:"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>preNum<span class="token punctuation">)</span><span class="token punctuation">:</span>
    image_path <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">(</span><span class="token string">"图片文件名:"</span><span class="token punctuation">)</span>
    img <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>image_path<span class="token punctuation">)</span>
    img <span class="token operator">=</span> img<span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Image<span class="token punctuation">.</span>ANTIALIAS<span class="token punctuation">)</span>  <span class="token comment"># 转化为28*28</span>
    img_arr <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>img<span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'L'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 转化为灰度图片</span>

    <span class="token comment"># 白底黑字，转换成黑底白字，并降噪</span>
    <span class="token comment"># img_arr = 255 - img_arr</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> img_arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">200</span><span class="token punctuation">:</span>
                img_arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">255</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                img_arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>

    img_arr <span class="token operator">=</span> img_arr <span class="token operator">/</span> <span class="token number">255.0</span>  <span class="token comment"># 归一化</span>

    <span class="token comment"># 神经网络训练时，都是按照batch送入网络，所以给img_arr前面添加一个维度</span>
    <span class="token comment"># img_arr:(28, 28)</span>
    <span class="token comment"># x_predict:(1, 28, 28)</span>
    x_predict <span class="token operator">=</span> img_arr<span class="token punctuation">[</span>tf<span class="token punctuation">.</span>newaxis<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>

    result <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_predict<span class="token punctuation">)</span>   <span class="token comment"># 前向传播</span>
    pred <span class="token operator">=</span> tf<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>result<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># 输出最大概率值的索引</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>
    tf<span class="token punctuation">.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"预测数字："</span><span class="token punctuation">,</span> pred<span class="token punctuation">,</span> <span class="token string">"\n"</span><span class="token punctuation">)</span></code></pre>





<h3 id="5-卷积神经网络"><a href="#5-卷积神经网络" class="headerlink" title="5 卷积神经网络"></a>5 卷积神经网络</h3><p>实际问题中，图片尺寸大、通道多，直接输入全连接层不现实，需要进行特征的提取，再输入到全连接层。卷积计算是提取特征的常用、有效的方法。</p>
<h4 id="5-1-卷积概念"><a href="#5-1-卷积概念" class="headerlink" title="5.1 卷积概念"></a>5.1 卷积概念</h4><h5 id="卷积计算"><a href="#卷积计算" class="headerlink" title="卷积计算"></a>卷积计算</h5><p>输入特征图的channel数决定了卷积核的channel数；卷积核的个数决定输出特征图的channel数。</p>
<p>如果觉得某层特征提取能力差，可以在这一层多用几个卷积核。</p>
<p>可以根据下图记住参数的个数。每个格子是一个w，每个卷积核有一个b。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf3.png'  width="80%" height="80%"/ loading="lazy">

<p>当卷积核是多channel的，也是执行全部相乘、求和的计算过程。</p>
<p>当有多个卷积核，把每个卷积核得到的特征图叠加起来，形成多channel的输出特征图。</p>
<h5 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h5><p>感受野的概念如图：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf4.png'  width="80%" height="80%"/ loading="lazy">

<p>用两层 3*3 卷积核，和一层 5*5 卷积核，最终得到的单个输出像素的感受野都是 5，我们说<strong>这两种卷积方法的特征提取能力是一样的</strong>。</p>
<p>此时，考虑它们的带训练参数个数和计算量，在不同的卷积方法中进行选择：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf5.png'  width="80%" height="80%"/ loading="lazy">

<p>这也是现在神经网络经常使用两层 3*3 卷积核替换一层 5*5 卷积核的原因。</p>
<h4 id="5-2-tensorflow卷积层的实现"><a href="#5-2-tensorflow卷积层的实现" class="headerlink" title="5.2 tensorflow卷积层的实现"></a>5.2 tensorflow卷积层的实现</h4><h5 id="A-padading-全零填充"><a href="#A-padading-全零填充" class="headerlink" title="A. padading-全零填充"></a>A. padading-全零填充</h5><p>全零填充在输入图片周围补0，使卷积计算的输出保持跟输入相同的尺寸。</p>
<ul>
<li><p><code>padding=&#39;SAME&#39;</code>：使用全零填充</p>
</li>
<li><p><code>padding=&#39;VALID&#39;</code>：不使用全零填充</p>
</li>
</ul>
<h5 id="B-Conv2D-描述卷积层"><a href="#B-Conv2D-描述卷积层" class="headerlink" title="B. Conv2D-描述卷积层"></a>B. Conv2D-描述卷积层</h5><pre class="language-python" data-language="python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>
    filters<span class="token operator">=</span>卷积核个数
    kernel_size<span class="token operator">=</span>卷积核尺寸<span class="token punctuation">,</span> 
    strides<span class="token operator">=</span>滑动步长<span class="token punctuation">,</span> 
    padding<span class="token operator">=</span><span class="token string">"same"</span> <span class="token keyword">or</span> <span class="token string">"valid"</span><span class="token punctuation">,</span> 
    activation<span class="token operator">=</span><span class="token string">"relu"</span> <span class="token keyword">or</span> <span class="token string">"softmax"</span> <span class="token keyword">or</span> <span class="token string">"tanh"</span> <span class="token keyword">or</span> <span class="token string">"sigmoid"</span><span class="token punctuation">,</span>  <span class="token comment"># 如果有BN，此处不写</span>
    input_shape<span class="token operator">=</span><span class="token punctuation">(</span>高<span class="token punctuation">,</span> 宽<span class="token punctuation">,</span> 通道数<span class="token punctuation">)</span> <span class="token comment"># 输入特征图的维度，可以省略</span>
<span class="token punctuation">)</span></code></pre>

<p>kernel_size、strides等，如果横纵相同，就写一个长整数；否则写 <code>(纵向, 横向)</code> </p>
<p>卷积层三种不同的传参方法：</p>
<pre class="language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
    Conv2D<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    MaxPool2D<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    
    Conv2D<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
    MaxPool2D<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    
	Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
    MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    
    Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>



<h5 id="C-BN-批标准化"><a href="#C-BN-批标准化" class="headerlink" title="C. BN-批标准化"></a>C. BN-批标准化</h5><p>神经网络对0附近的数据更敏感，梯度更大。</p>
<ul>
<li>标准化：使数据符合0均值，1标准差的分布</li>
<li>批标准化：对一batch的数据做标准化处理</li>
</ul>
<p>$$ {H_i^k}^\prime  &#x3D; \frac{H_i^k - \mu^k_{batch}}{\sigma^k_{batch}} $$</p>
<p><strong>BN常用在卷积操作和激活操作之间。</strong> </p>
<p>均值、标准差都是针对第 k 个卷积核生成的 batch 张（同channel）图中的所有像素点。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf6.png'  width="80%" height="80%"/ loading="lazy">

<p>这种简单的标准化，使输入数据集中在激活函数的线性区域。</p>
<p>因此在BN操作中，为每个像素 $H$ 引入<strong>两个可训练参数</strong>，缩放因子 $\gamma$ 和偏移因子 $\beta$，标准正态分布后的输入数据通过这两个因子，优化了特征数据分布的宽窄、偏移量（如图），<strong>保证了网络的非线性表达力</strong>。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf7.png'  width="80%" height="80%"/ loading="lazy">

<pre class="language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
	Conv2D<span class="token punctuation">(</span>fitlers<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>



<h5 id="D-Pooling-池化"><a href="#D-Pooling-池化" class="headerlink" title="D. Pooling-池化"></a>D. Pooling-池化</h5><p>用于减少特征的数量。</p>
<ul>
<li>最大池化：提取图片纹理</li>
<li>均值池化：保留背景特征</li>
</ul>
<p><code>padding=&#39;same&#39;</code> 时，<strong>不可整除的会填充成可整除，而不是不改变输出的尺寸</strong> </p>
<pre class="language-python" data-language="python"><code class="language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>MaxPool2D<span class="token punctuation">(</span>
	pool_size<span class="token operator">=</span>池化核尺寸<span class="token punctuation">,</span> 
    strides<span class="token operator">=</span>池化步长<span class="token punctuation">,</span>  <span class="token comment"># 默认跟 pool_size 相等</span>
    padding<span class="token operator">=</span><span class="token string">'valid'</span> <span class="token keyword">or</span> <span class="token string">'same'</span> <span class="token comment"># 是否使用全零填充的池化</span>
<span class="token punctuation">)</span>
tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>AveragePooling2D<span class="token punctuation">(</span>
	pool_size<span class="token operator">=</span>池化核尺寸<span class="token punctuation">,</span> 
    strides<span class="token operator">=</span>池化步长<span class="token punctuation">,</span> 
    padding<span class="token operator">=</span><span class="token string">'valid'</span> <span class="token keyword">or</span> <span class="token string">'same'</span> 
<span class="token punctuation">)</span></code></pre>

<p>例子见之前的代码。 </p>
<h5 id="E-Dropout-舍弃"><a href="#E-Dropout-舍弃" class="headerlink" title="E. Dropout-舍弃"></a>E. Dropout-舍弃</h5><p>为了防止过拟合，按照一定概率，把一部分神经元从社交网络中暂时舍弃。使用神经网络时，被舍弃的神经元恢复连接。</p>
<p><code>tf.keras.layers.Dropout(舍弃概率)</code> </p>
<p>例子见之前的代码。 </p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>卷积神经网络：借助卷积核提取特征，然后送入全连接网络。</p>
<p>特征提取分四步：</p>
<ul>
<li>Conv2D</li>
<li>BN</li>
<li>Activation</li>
<li>Pooling</li>
</ul>
<p>Q：卷积是什么？</p>
<p>A：卷积就是特征提取器，就是CBAPD。</p>
<pre class="language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
	Conv2D<span class="token punctuation">(</span>fitlers<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
	Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>



<h4 id="5-3-卷积神经网络搭建示例"><a href="#5-3-卷积神经网络搭建示例" class="headerlink" title="5.3 卷积神经网络搭建示例"></a>5.3 卷积神经网络搭建示例</h4><h5 id="Cifar-10-数据集"><a href="#Cifar-10-数据集" class="headerlink" title="Cifar 10 数据集"></a>Cifar 10 数据集</h5><ul>
<li>5万张 32*32 像素的 10 分类彩色图片和标签，作为训练集 </li>
<li>1万张 32*32 像素的 10 分类彩色图片和标签，作为训练集</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">cifar10 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>cifar10
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span> <span class="token operator">=</span> cifar10<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<h5 id="搭建示例"><a href="#搭建示例" class="headerlink" title="搭建示例"></a>搭建示例</h5><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Baseline</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Baseline<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b1 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a1 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p1 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d1 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d2 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f2 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>f2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y</code></pre>



<h4 id="5-4-LeNet"><a href="#5-4-LeNet" class="headerlink" title="5.4 LeNet"></a>5.4 LeNet</h4><p><strong>在统计卷积神经网络层数时，一般只统计卷积计算层和全连接计算层。</strong> </p>
<p>LeNet的五层结构如图：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf8.png'  width="80%" height="80%"/ loading="lazy">

<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LeNet5</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LeNet5<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p1 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c2 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p2 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f2 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f3 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>f3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y</code></pre>



<h4 id="5-5-AlexNet"><a href="#5-5-AlexNet" class="headerlink" title="5.5 AlexNet"></a>5.5 AlexNet</h4><p>AlexNet 的八层结构：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf9.png'  width="80%" height="80%"/ loading="lazy">

<p>（图中有些数不准确，详见代码。）</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf10.png'  width="80%" height="80%"/ loading="lazy">

<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">AlexNet8</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>AlexNet8<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b1 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a1 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p1 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c2 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b2 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a2 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p2 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c3 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>\
                         activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c4 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>\
                         activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c5 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span>\
                         activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p3 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>f1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d1 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>f2 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d2 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>f3 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>f3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y</code></pre>



<h4 id="5-6-VGGNet"><a href="#5-6-VGGNet" class="headerlink" title="5.6 VGGNet"></a>5.6 VGGNet</h4><p>使用小尺寸卷积核，在减少参数的同时提高了识别准确率。</p>
<p>VGG网络结构规整，适合硬件加速。</p>
<p>（图中有些数不准确，详见代码。）</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf11.png'  width="80%" height="80%"/ loading="lazy">

<p>卷积核个数 64 –&gt; 128 –&gt; 256 –&gt; 512，因为越靠后，特征图尺寸越小，通过增加通道，增加了特征图的深度，保持了信息的承载能力。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">VGG16</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>VGG16<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b1 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a1 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c2 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b2 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a2 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p1 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>， strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d1 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c3 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b3 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a3 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c4 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b4 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a4 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p2 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>， strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d2 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c5 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b5 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a5 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c6 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b6 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a6 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c7 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b7 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a7 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p3 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>， strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d3 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c8 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b8 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a8 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c9 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b9 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a9 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c10 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b10 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a10 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p4 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>， strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d4 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c11 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b11 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a11 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c12 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b12 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a12 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c13 <span class="token operator">=</span> Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b13 <span class="token operator">=</span> BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a13 <span class="token operator">=</span> Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p5 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>， strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d5 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d6 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>f2 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d7 <span class="token operator">=</span> Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>f3 <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c6<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b6<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a6<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c7<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b7<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a7<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c8<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b8<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a8<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c9<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b9<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a9<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c10<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b10<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a10<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c11<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b11<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a11<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c12<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b12<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a12<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c13<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>b13<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>a13<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d6<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>f2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>d7<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        y <span class="token operator">=</span> self<span class="token punctuation">.</span>f3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y</code></pre>



<h4 id="5-7-Inception-Net"><a href="#5-7-Inception-Net" class="headerlink" title="5.7 Inception Net"></a>5.7 Inception Net</h4><p>在同一层网络内使用不同尺寸的卷积核，提升了模型感知力。</p>
<p>单个Inception结构块如图：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf12.png'  width="80%" height="80%"/ loading="lazy">

<p>卷积连接器Filter concatenation将四个分支按深度方向堆叠在一起。</p>
<p>由于卷积操作都是 C-B-A 的结构，将这种三步卷积操作定义为一个类，减少代码长度：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ConvBNRelu</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ConvBNRelu<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
            Conv2D<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token punctuation">,</span> strides<span class="token operator">=</span>strides<span class="token punctuation">,</span> padding<span class="token operator">=</span>padding<span class="token punctuation">)</span><span class="token punctuation">,</span> 
            BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
            Activation<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span>
	<span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x</code></pre>

<p>然后，Inception结构块可以这样实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">InceptionBlk</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ch<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>InceptionBlk<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ch <span class="token operator">=</span> ch
        self<span class="token punctuation">.</span>strides <span class="token operator">=</span> strides
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> strides<span class="token operator">=</span>strides<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c2_1 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> strides<span class="token operator">=</span>strides<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c2_2 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>c3_1 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> strides<span class="token operator">=</span>strides<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c3_2 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>p4_1 <span class="token operator">=</span> MaxPool2D<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>c4_2 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> kernelsz<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> strides<span class="token operator">=</span>strides<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x1 <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x2_1 <span class="token operator">=</span> self<span class="token punctuation">.</span>c2_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x2_2 <span class="token operator">=</span> self<span class="token punctuation">.</span>c2_2<span class="token punctuation">(</span>x2_1<span class="token punctuation">)</span>
        x3_1 <span class="token operator">=</span> self<span class="token punctuation">.</span>c3_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x3_2 <span class="token operator">=</span> self<span class="token punctuation">.</span>c3_2<span class="token punctuation">(</span>x3_1<span class="token punctuation">)</span>
        x4_1 <span class="token operator">=</span> self<span class="token punctuation">.</span>p4_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x4_2 <span class="token operator">=</span> self<span class="token punctuation">.</span>c4_2<span class="token punctuation">(</span>x4_1<span class="token punctuation">)</span>
        <span class="token comment"># 堆叠不同分支的输出</span>
        x <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2_2<span class="token punctuation">,</span> x3_2<span class="token punctuation">,</span> x4_2<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x</code></pre>



<p>精简InceptionNet：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/nndl/tf13.png'  width="80%" height="80%"/ loading="lazy">

<p>每个 block 由两个 InceptionBlk 组成，第一个的卷积 strides&#x3D;1，第二个的 strides&#x3D;2.</p>
<p>这使 block_0 的输出特征图尺寸减半，因此我们把输出特征图深度加深为2倍，保持信息承载能力。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Inception10</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_blocks<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> init_ch<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Inception10<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> init_ch
        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> init_ch
        self<span class="token punctuation">.</span>num_blocks <span class="token operator">=</span> num_blocks
        self<span class="token punctuation">.</span>init_ch <span class="token operator">=</span> init_ch
        self<span class="token punctuation">.</span>c1 <span class="token operator">=</span> ConvBNRelu<span class="token punctuation">(</span>init_ch<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>blocks <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> block_id <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">for</span> layer_id <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> layer_id <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    block <span class="token operator">=</span> InceptionBlk<span class="token punctuation">(</span>self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    block <span class="token operator">=</span> InceptionBlk<span class="token punctuation">(</span>self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>blocks<span class="token punctuation">.</span>add<span class="token punctuation">(</span>block<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>out_channels <span class="token operator">*=</span> <span class="token number">2</span>
        self<span class="token punctuation">.</span>p1 <span class="token operator">=</span> GlobalAveragePooling2D<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f1 <span class="token operator">=</span> Dense<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>blocks<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>p1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>f1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y

   model <span class="token operator">=</span> Inception10<span class="token punctuation">(</span>num_blocks<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span></code></pre>



<h4 id="5-8-ResNet"><a href="#5-8-ResNet" class="headerlink" title="5.8 ResNet"></a>5.8 ResNet</h4><p>提出层间残差跳连，引入前方信息，缓解梯度消失，可以让网络层数增加。</p>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>课程笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>年轻人的第一次求职（实习生）</title>
    <url>/2022/02/05/job-1/</url>
    <content><![CDATA[<h4 id="0-写在前面"><a href="#0-写在前面" class="headerlink" title="0 写在前面"></a>0 写在前面</h4><p>正常是：从2022年的3月开始，报名各个公司的2022暑期实习项目。我原本的计划也是：寒假多多准备（回来就是躺着了），3月在学校混一下科研工作（一直在混）、认真求职。</p>
<p>但2021年下半年我参加了<a href="https://gameinstitute.qq.com/open-courses/2021/prog">腾讯游戏客户端开发公开课</a>，在课程结束时，可以自愿进入面试环节，据hr说这算是暑假实习的提前批。因此我抱着反正不亏的心态投了简历，并经历了相对曲折的面试流程，最终到达了Good End。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/3.png" style="zoom: 67%;" / loading="lazy">

<p>收到offer的时候已经是2022年1月28日了，满脑子只有一个想法——累。因此我就直接接受offer、并进入入职流程了。这样，少了多方比较的选择过程，多了两个月的空闲期。</p>
<p>当然，能取得这个结果也是运气好，进入的项目也是我很感兴趣的（就算拿到多个offer我大概率也选这个），在后文会提到。再加上室友也通过了北极光工作室的所有流程，很巧我们都在上海实习，或许能相互帮助一下。总之是接受了这个offer了，实习就不挑了，秋招再多方考虑吧。</p>
<p>由于这是我的第一次求职尝试，对于心态、知识都是不小的考验，因此在这里做比较完整的记录。除了记下被问到的、能回忆起的问题，会更加偏向于写我在整个过程中的感受，以及对于求职的一些经验教训。</p>
<h4 id="1-客户端开发公开课"><a href="#1-客户端开发公开课" class="headerlink" title="1 客户端开发公开课"></a>1 客户端开发公开课</h4><p>总体来说，课上教的东西不多，但对我是一个很好的契机。不仅用UE4做了些开发，并且也借着几次面试的机会，边面边学、补了一些专业基础知识。</p>
<ul>
<li><p>在8月底、9月初有公开课的选拔环节，需要投简历，然后参加一次笔试＋一次面试</p>
<ul>
<li>当时我还没开始准备求职的相关内容。笔试除了编程题基本上没有会的😅，面试问了很多基础知识我都没答上来，具体的问题已经记不清了，反正C++基础（都没怎么答上来）、数学题（判断点在扇形内）、算法（快排）都有</li>
<li>最终依然过了，进入了天美工作室班</li>
<li>我们组面试阵容很豪华，但应该也没很严格地筛人。后面听说还补招了</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/4.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>在10~12月完成了课程，并提交了三次作业，<a href="https://github.com/ACBGZM/UE4Course">在这里</a>。平时有各种事情，都是临近交作业的一周从头开始学＋做，大约都花了一周左右吧</p>
<ul>
<li>课程更多在讲游戏客户端开发的各个模块的基础知识，比如物理、网络同步、AI等，顺带简单讲了UE4如何实现相应的功能。要想完成作业需要自己找点东西来看</li>
<li>我是只以完成作业为目标学的UE4。在B站看过几个同学传的作业视频，对UE4做了一些研究并做出了很酷炫的效果，真是相当优秀，比如其他班的 <a href="https://www.bilibili.com/video/BV1GP4y1G7S5">ARPG作业</a></li>
</ul>
</li>
</ul>
<h4 id="2-投简历-2021-x2F-12-x2F-23"><a href="#2-投简历-2021-x2F-12-x2F-23" class="headerlink" title="2 投简历 2021&#x2F;12&#x2F;23"></a>2 投简历 2021&#x2F;12&#x2F;23</h4><p>12月底最后一次作业结束之后，就可以报名面试了。主要填意向工作地（我哪里都行，上海填在第一个）、感兴趣的游戏类型（我填的RPG、动作）。然后HR会给每个人推到最匹配的项目组。</p>
<p>之前报名公开课交过简历，这次把公开课当作一个项目加上去了。其他的就沿用之前，并没有写得很讲究，简简单单准备一下就投了。简历其实还是应该多斟酌一下的</p>
<ul>
<li>项目上，也没参加过什么大型项目，就把乱七八糟的游戏相关经历都写上了<ul>
<li>本科做的Unity小游戏，JS游戏，WebGL也写了一笔。这些应该没必要（没被问），下次删了就好</li>
<li>去年6月，由于实在不想做科研，就花了近2周，跟着教程做了一个Unity2D横板过关游戏，<a href="https://share.weiyun.com/L3TA8NCK">演示视频</a> </li>
<li>本次公开课的作业，<a href="https://github.com/ACBGZM/UE4Course">项目地址</a>、<a href="https://share.weiyun.com/KV7C4B8O">演示视频</a></li>
</ul>
</li>
<li>技能上，由于怕被挑毛病，用词很谨慎，有点矫枉过正了<ul>
<li>语言连“了解”都没敢写，直接写的“有使用 C++、Python、C#、JavaScript 进行项目开发的经历”</li>
<li>听说选拔的时候室友写OS被问了，我就没提具体的课，就一句“熟悉计算机专业基础知识”</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/5.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>我投简历的时候室友一面结束、回家放假了。他们那边说的是“在年前肯定全弄完”，当时我们就认为在2021年肯定就全都完事了。</p>
<h4 id="3-一面-2022-x2F-1-x2F-4-下午"><a href="#3-一面-2022-x2F-1-x2F-4-下午" class="headerlink" title="3 一面 2022&#x2F;1&#x2F;4 下午"></a>3 一面 2022&#x2F;1&#x2F;4 下午</h4><h5 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h5><p>投完简历就是圣诞节那几天。我整个12月都在做一个很晦气的大型杂活，刚好也快到了交差的日子，身心俱疲。因此想着“一周后就全结束了，到时候好好放假”，很努力地一边做杂活、一边准备面试（甚至有几天早上10点就到实验室开工了）。</p>
<p>其实也是头一回准备求职的相关内容，主要是看各种的C++、UE4的基础。由于时间很短，重点看了一些临时抱佛脚的资料。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/6.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>临近元旦假期，群里很多同学都被面了好几次了，我却什么消息都没收到。已经是三而竭的状态了。在12月30号收到约面试的电话，我想在年前的这两天赶紧面完完事，但对面说面试官全都出差了，于是就约在了假期回来的第一天。那假期肯定也过不好了。</p>
<p>心里实在是疲惫，准备得比较休闲。开始看一些其他的基础，比如网络、OS等主要是看了几个不错的视频。比如 <a href="https://www.bilibili.com/video/BV124411k7uV">从设备看计算机网络</a>，这位UP主的视频很有意思。其他的收藏夹没同步到新电脑。</p>
<h5 id="面试（下午-35min）"><a href="#面试（下午-35min）" class="headerlink" title="面试（下午 35min）"></a>面试（下午 35min）</h5><p>面试官给人沉稳、擅长交流的感觉。节奏很快。问的很多，很全面，各种知识基本上会问到不会为止。跟选拔面试的内容有点像。有一个本子作为题库，我一开始吭哧瘪肚面试官就翻本子找下一题。</p>
<ul>
<li>C++<ul>
<li>struct内存对齐，加一个函数、虚函数会怎么变</li>
<li>虚函数相关一路问，问到vptr存在对象的哪里（我不会就下一个了）</li>
<li>构造、析构的顺序，析构函数和虚函数</li>
<li>（以上是围绕多态来问）</li>
<li>new&#x2F;delete，malloc&#x2F;free的区别（我当时还没仔细去看这个，都在说使用，没说实现）</li>
<li>delete[]在做什么，一个数组delete和delete[]的区别（这里我答错了）</li>
<li>（另一条线，内存）</li>
</ul>
</li>
<li>UE4 C++<ul>
<li>讲讲UE4的C++<ul>
<li>从UObject类到后面各种Gameplay的组成，以及一些特性都提了一下，但我也是知道名字而已，我作业是蓝图做的没用C++</li>
</ul>
</li>
<li>动态功能的内存管理、GC具体怎么做，反射是怎么实现的<ul>
<li>不会，只说了肯定跟RTTI之类的动态特性有关</li>
</ul>
</li>
<li>UE4的三个智能指针<ul>
<li>有一个不认识的，面试官说那个是UE4自创的、C++里没有。把认出来的两个说了一下</li>
</ul>
</li>
<li>UE4场景管理如何做<ul>
<li>刚开始我说做了几个level，后面感觉可能在问场景内的很多东西如何管理（场景管理这个词确实就是这个意思），就说或许会用树存</li>
</ul>
</li>
<li>具体什么数据结构<ul>
<li>我猜了个红黑树，因为各方面性能均衡；又想到可能会跟数据库一样遍历，就改成B+树，但没说我的思路</li>
</ul>
</li>
</ul>
</li>
<li>项目<ul>
<li>做了什么模块，对哪个模块比较熟悉，细说<ul>
<li>我答物理和AI，不知道哪里提了句射线，被问了个射线的问题没答上来</li>
</ul>
</li>
<li>物理模块是如何实现的，核心是<ul>
<li>不会</li>
</ul>
</li>
<li>AI模块实现了什么，怎么实现的，AI敌人之间是否有交互<ul>
<li>脑子有点乱，不知道怎么讲到角色动画去了，后面又说回来了</li>
</ul>
</li>
</ul>
</li>
<li>数学，跟选拔面试一样先问有没有图形学基础，如果没有就问数学题<ul>
<li>3D空间的一个平面如何表示，方程是什么，如何用数据结构存<ul>
<li>我是真忘了，拉扯了很久（或者说面试官试图提示我），但我一直在说点＋法向量。脑子里在想2D空间直线的点法式方程。但有点懵没仔细想。其实就是 $Ax+By+Cz+D&#x3D;0$ <ul>
<li>如果以点法式想下去，点 $\bold {p_0}(p_x, p_y, p_z)$、法向量 $\vec n&#x3D;(a,b,c)$ </li>
<li>平面上的所有点满足 $(\bold p-\bold {p_0})\cdot\vec n&#x3D;0$ ，即 $(x-p_x,y-p_y,z-p_z)\cdot(a,b,c)&#x3D;0$ </li>
<li>$ax+by+cz-(ap_x+bp_y+cp_z)&#x3D;0$</li>
</ul>
</li>
<li>面试官说点＋法向量确实可以，但如果你知道平面的方程会有更好的方法</li>
</ul>
</li>
</ul>
</li>
<li>算法<ul>
<li>快排的思路，复杂度，稳定性<ul>
<li>递归栈要logn的空间，这个我想起来了。稳定性我说不稳定，面试官说确定吗？我沉默了一会，打算思考一下，但脑子里还是3D空间平面的事情……可以说脑子已经不动了</li>
</ul>
</li>
</ul>
</li>
<li>设计模式<ul>
<li>是否了解，用过什么<ul>
<li>我本科上过课，并且这门课是我们学院一个年轻的副院长教的，讲得很好，所有的实验也做了，记得就是用C#写的。但水平原因，平时用不上，已经全忘了。只说了Unity2D游戏的SoundManager用了单例模式</li>
</ul>
</li>
<li>你觉得UE4引擎本身用了哪些设计模式<ul>
<li>没说话</li>
</ul>
</li>
</ul>
</li>
<li>反问环节<ul>
<li>首先面试官自我介绍了一下<ul>
<li>得知1月15号前会确定最终的人选，也就是整个流程会结束</li>
<li>对面是天美T1工作室，ACM项目组，Gameplay组负责校招的。下设3C组、AI组还有什么的</li>
<li>各种形容词可以说buff拉满了，上海、3A、ACT、开放世界、在研、UE4</li>
<li>面试完了脑子开始转了，我想 ACM 不会是 assassin’s creed mobile 吧？后面一查<a href="https://www.zhihu.com/question/436704377">还真是</a></li>
</ul>
</li>
<li>对我怎么评价<ul>
<li>实话说，C++、算法基础还不错，游戏、图形学不太行，建议找本书从0看</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="4-笔试-2022-x2F-1-x2F-8"><a href="#4-笔试-2022-x2F-1-x2F-8" class="headerlink" title="4 笔试 2022&#x2F;1&#x2F;8"></a>4 笔试 2022&#x2F;1&#x2F;8</h4><h5 id="准备-1"><a href="#准备-1" class="headerlink" title="准备"></a>准备</h5><p>面试完心情比较放松。第二天（1月5号）临近中午，正在床上躺尸呢，接到了一面面试官的电话，说要给我加一个笔试（3h、编程题），当时我说1月7号，后面怕要开组会就改到1月8号上午。</p>
<p>在1月5号又收到另一个电话，要约二面，但听我要加笔试，就说后面再约了。</p>
<p>我不喜欢做题，平时没有刷题习惯，偶尔兴致上来了就Leetcode刷一刷，反正从来不会坚持下去。也不太清楚考什么类型的题，就把牛客的 <a href="https://www.nowcoder.com/exam/oj?tab=%E8%AF%AD%E6%B3%95%E7%AF%87&topicId=225">C++语法篇</a> 做完了，根本没难度。过程中有点麻木的感觉，好像是为了找个事做而在做这些简单题。</p>
<p>此外，也在Leetcode开了个剑指Offer的题单，把之前一直不怎么清楚的DFS、动态规划思路都看了一下，做了几个题找找感觉。</p>
<h5 id="笔试（上午9-00-3h）"><a href="#笔试（上午9-00-3h）" class="headerlink" title="笔试（上午9:00 3h）"></a>笔试（上午9:00 3h）</h5><p>比想象的简单多了。3h就三道题。题目是英文的，不会给测试用例，但写个函数即可。要求不能用任何STL、库方法等，可以说白准备了</p>
<ul>
<li>两个垂直于坐标轴的矩形，判断相交<ul>
<li>很巧，我在本科用JS做过一个简陋的打砖块游戏（<a href="https://github.com/ACBGZM/web-learning">在这</a>），弹的东西跟砖块都是矩形。当时写得很复杂（因为还要考虑弹出去的方向），这次写的思路比较清晰，从h、v两个方向，判断两个矩形中点的距离是否超过两个矩形高&#x2F;宽和的一半</li>
</ul>
</li>
<li>树的层次遍历<ul>
<li>需要自己实现队列。我直接讨巧，在栈上新建静态数组、用两个指针指向队头和对位，省的再写队列操作、或者手动管理内存了</li>
</ul>
</li>
<li>自己实现atoi<ul>
<li>用一个布尔值来记录数字是否开始，未开始时可以是正负号、空格等字符，开始了就只能是数字</li>
<li>后面发现是<a href="https://leetcode-cn.com/problems/string-to-integer-atoi/">Leetcode 8</a></li>
</ul>
</li>
</ul>
<p>或许除了算法，还会综合考察代码风格之类的？总之我在我的水平上注意了一下，比如 <code>static_cast</code> 等新特性，在哪里开辟空间之类的</p>
<h4 id="5-二面-2022-x2F-1-x2F-12"><a href="#5-二面-2022-x2F-1-x2F-12" class="headerlink" title="5 二面 2022&#x2F;1&#x2F;12"></a>5 二面 2022&#x2F;1&#x2F;12</h4><h5 id="准备-2"><a href="#准备-2" class="headerlink" title="准备"></a>准备</h5><p>从一面结束（1月4号），我每天查几次流程的状态，一直是“初试”。后面就懒得查了，也不准备了，在学校呆了最后几天，学期末有一些乱七八糟的事情。10、11号坐了两天火车，结果12号上午11点接到电话，问我当天晚上8点有没有空二面。</p>
<p>打完电话查了一下，流程到了“复试”了。</p>
<p>有点慌，稍微看了下之前做的笔记准备了一下。也看了一下一面被问到的问题，主要是多态、内存、场景管理。</p>
<h5 id="面试（晚上20-00-45min）"><a href="#面试（晚上20-00-45min）" class="headerlink" title="面试（晚上20:00 45min）"></a>面试（晚上20:00 45min）</h5><p>这次的面试气压比较低，节奏也慢，遇到不会的就是我一个人沉默。我沉默的时候面试官也闭麦在联络别的事情。我觉得可能有连续沉默两分钟的情况。</p>
<p>晚上脑子不清楚，有时虽然在沉默，也没在思考。比较惨烈。</p>
<p>被问到的问题，可以说在会与不会之间吧，或许只知道一个概念就随口说出来了、但会被问到比较具体。</p>
<ul>
<li>自我介绍，校内科研</li>
<li>游戏开发经历，unity2D游戏的简介，有没有敌人AI、如何交互，子弹如何判断杀死敌人</li>
<li>从子弹问到碰撞具体怎么实现<ul>
<li>碰撞怎样计算，以子弹发射出去为例，在哪个坐标系算<ul>
<li>其实Unity里就打几个勾完事了，但面试官会往算法问</li>
<li>我记得一点分离轴定理（当时不知道叫这个名字），大致说了一下，面试官会一直往细问，一直被问到大于多少小于多少</li>
</ul>
</li>
</ul>
</li>
<li>一个场景中有海量怪物，如何查找离主角最近的10个怪物<ul>
<li>如果主角是在不断运动的怎么办，怪物也是运动的怎么办，怪物移动得很快怎么办，有的快有的慢怎么办<ul>
<li>也是一路往细问。一面完看了一下场景管理，就一上来说用八叉树等存找起来方便一些。结果面试官会一直问具体怎么查</li>
<li>没仔细看八叉树，就自己瞎说了一些。主角在八叉树中也有一个位置，如果不动就往上往下找，移动得快就首先尽量找之前一个状态离我比较近的节点下的怪物，可以以速度最快的怪物移动的范围做衡量，如果超过这个能移动的最大范围就没必要找了</li>
<li>不清楚说的对不对，或者到底应不应该说八叉树</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>每个题都感觉面试官是在引导我往正确的方向走，但我真的不太会，就在反复说八叉树啥的。这个题答了很久</p>
</blockquote>
<ul>
<li>海量的数据，如何查找 &gt;m &lt;n 的全部数据<ul>
<li>如果m和n每次在变怎么办，之类的<ul>
<li>结束上个题我脑子有点乱。听完题一下子联想到选拔面试问的怎么查最大的m个数，当时被面试官提醒快排的思路，因此我直接就往快排想了（惨案的开始），先用m快排、再在结果里用n快排（其实我想说的是一趟快排操作）；后面又不知道怎么回事说堆排序之类的</li>
<li>如果每次在变，我想了一下说先变成有序的会方便一些，当然也要考虑这些数是怎么存的</li>
</ul>
</li>
<li>不管怎样，给出一个完整的存、查方案，不能假定数字不是很多<ul>
<li>又不知道为什么说了一会哈希表，刚开始说取余数、拉链法，被提醒后发现不对。就把某个大小区间的数存到一起，然后对于m和n去找，其实我描述的是类似桶排序的思路</li>
</ul>
</li>
<li>为什么不用普通链表存？<ul>
<li>我说不能把有相关性的数存到一起，又被问为什么不能，问我哈希表到底存了什么</li>
</ul>
</li>
<li>如果就是普通链表存、有序数组，怎么找？<ul>
<li>看我越跑越远，已经提示到这份上了。然而我先说了一句“链表肯定不能随机找、或二分找，因此。。。”，马上又要瞎说了，面试官赶紧给我打断了，问为什么不能二分找？</li>
<li>我答如果是跳表，是接近二分的，普通链表没法很快捷地二分找。面试官问怎么就不能二分？问完估计看我一时半会说不对，就马上下一题了<ul>
<li>其实我脑子里的二分跟logn是等价的，因为链表每个节点不能在O(1)获得，所以我脑子里的二分跟普通链表是完全没关系的，因此这里会说跳表才能二分。但“二分”作为一个查找思路，或许不一定要立刻能找到中间位置。链表慢慢next过去也是可以的，只要总体思路是在二分即可</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>这个题如果是有序的，其实直接用链表存、用二分的思路查就好了，给定m和n再从上一状态二分查找新位置即可（估计也会被追问到底怎么查找新位置）。我听到这个题首先在想之前遇到的找最大m个数的题，因此脱口而出快排，然后就在这个方向上一直想，甚至后面又在想哈希表什么的。后面越错越慌。其实是面试官一直在提醒了。</p>
<p>二面的面试官会根据我每一句陈述问出“为什么”，而且话比较少，基本上就这仨字，压力有点大。可能是在提醒、或者纠正我到正确的思路上吧。对于我这种喜欢随口一说的人，是一个宝贵的经验：<u>尽量保证每句话都经得起解释</u>。</p>
</blockquote>
<ul>
<li>海量的怪物坐标，怎样合理地存储<ul>
<li>不考虑场景、遍历之类的操作，如何存100万条3个数组成的大型数组<ul>
<li>我以为是场景管理，又在说比较合理或许还是用树存，如果要求遍历比较方便（类似于数据库），可以用B树B+树存。面试官补充说不考虑场景、遍历之类的操作，就是海量的坐标，问你如何压缩</li>
<li>沉默后说没什么想法，如果都是3个数那或许就存到字符串里面</li>
</ul>
</li>
<li>具体怎么存？如果就是100万个vector&lt;vector&lt;float&gt;(3)&gt;，如何压缩一下？<ul>
<li>实在没想法，我慢慢说思路：数组占的空间基本是固定的，有多少条肯定是固定的，只能从每条的空间入手，每个float 4字节，我要从字节的角度上去压缩吗？</li>
</ul>
</li>
<li>随便怎么压缩<ul>
<li>我竟然说把进制提升一下，让float短一点，这里基本上是瞎扯了。后面又反应过来这对它的长度没什么改变</li>
</ul>
</li>
<li>下一题吧，你之后可以去了解一下</li>
</ul>
</li>
</ul>
<blockquote>
<p>沉默的10分钟，这个题我至今不知道该怎么回答。全程没怎么说话，每个问题或许都要沉默个1分钟，脑子也不转，当时心里觉得就没戏了。面试官好像也在闭了麦在联络什么事情</p>
</blockquote>
<ul>
<li>一个复合结构的数组，存结构体本身、存指针，哪一个更快？<ul>
<li>（是本身吧？存本身可以直接到相应的空间找，而存指针要到相应的地址上去找）</li>
<li>那如果数据结构很大？</li>
<li>（是由于很大、加载不到内存里，要发生换页之类的吗？）</li>
<li>也没那么大。下一题吧</li>
</ul>
</li>
</ul>
<blockquote>
<p>当时脑子不太转了，总在想“结构体、指针”，面试官估计也看我没答道点上就直接下一题了。现在想，可能重点是“海量、数组”。众所周知vector是经常扩容的，扩容的时候会把原来数组的所有东西都拷贝一份新的。如果存的是结构体，就会调用拷贝构造函数。</p>
<p>面试官问数据结构很大，可能就是在提醒这一点。</p>
</blockquote>
<ul>
<li>一个10000的for循环、一个100的for循环，怎么遍历？<ul>
<li>（怎么遍历是指？除了从头到尾遍历的其他方法吗？）</li>
<li>哪个在里面、哪个在外面，怎么样更快（知识盲区）</li>
<li>没事，你自己试一下就知道了。</li>
<li>我说好的我记一下。<ul>
<li>简单说，首先尽量不要多层嵌套，然后如果嵌套，内部的不要经常跳出</li>
<li><a href="https://blog.csdn.net/xiaohuh421/article/details/44056189">可以看看这个</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>又是一听不知道问啥的，确认了一下发现纯纯的知识盲区，赶紧下一题吧</p>
<p>由于我面试经验少，只要不说得很明白、我就不理解问的什么，需要确认一下。就像“你回答一下A吧”，但我不知道“A”是什么。如果面试经验多一些，可能会发现都是些老题吧</p>
</blockquote>
<ul>
<li><p>UE4 的 TMap 和 C++ STL map 的区别</p>
<ul>
<li>（我没用过TMap，它是哈希表吗？C++的map是红黑树。TMap是否对应unordered_map？）</li>
<li>对的，它是哈希表。各有什么特点？</li>
</ul>
</li>
<li><p>设计模式了解多少？</p>
<ul>
<li>（本科上过课，但忘得差不多了，Unity游戏里用过单例模式去维护sound manager）</li>
<li>单例模式的特点？</li>
<li>什么时候用单例模式？</li>
</ul>
</li>
<li><p>反问</p>
<ul>
<li>（对于校招，基础更重要、还是关于这类游戏问题更重要？）</li>
<li>基础，数据结构和算法。对于游戏方面则是自己知道的越多越好</li>
<li>（对于这种怎样准备？）</li>
<li>数据结构和算法啊。（我说确实）</li>
</ul>
</li>
</ul>
<h5 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h5><p>问的竟然全是我最怕的数据结构和算法。</p>
<p>这次面完是真的累了。刚坐两天火车回家，第二天火急火燎面试，面的也一塌糊涂，没有一道题是答得很好的。</p>
<p>之前说15号就确定完最后人选了，我观望了两天没消息心里就放弃了。心里有种解脱了的感觉。当时构思这篇文章的题目是《找工作真难啊！记一次白忙活》。</p>
<h4 id="6-三面-2022-x2F-1-x2F-23"><a href="#6-三面-2022-x2F-1-x2F-23" class="headerlink" title="6 三面 2022&#x2F;1&#x2F;23"></a>6 三面 2022&#x2F;1&#x2F;23</h4><h5 id="准备-3"><a href="#准备-3" class="headerlink" title="准备"></a>准备</h5><p>二面完，尤其是15号之后我就完全进入休假模式了。在亚服开了个号快乐原神开荒，看看直播，看看lpl比赛。没再看面试的东西。</p>
<p>20号突然收到个邮件，提醒我接受面试。看了下系统，这次是系统上约了“复试”的面试了（之前都是电话约的，系统上没有）。刚开始有想放弃的念头，不想再回到抗压状态了。但还是想着善始善终吧。</p>
<p>剩下两天就休闲准备了。回顾了一下笔记，补了补一二面的知识盲区，又查了查面经。</p>
<ul>
<li>在C++的多态和内存两条线上又多看了一点。这里看了<a href="https://www.bilibili.com/video/BV15g4y1a7F3">一个很不错的视频</a>，涵盖了从使用到实现的很多知识</li>
<li>之前总被问AI模块，就看了一下导航网格的实现原理，顺带看了一眼A*</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/7.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="面试（中午11-30-35min）"><a href="#面试（中午11-30-35min）" class="headerlink" title="面试（中午11:30 35min）"></a>面试（中午11:30 35min）</h5><p>真是别开生面，面试官晚来了10分钟，然后麦坏了调了一会，最终还是以他打字、我说话的方式面的。单口相声，我怕安静的时候会尬，就尽量多说了点话，但心里又怕太罗嗦、跟二面一样瞎说。</p>
<p>没想到，面试都是在问软问题，像自己的理解、感悟、动机之类的。</p>
<ul>
<li>自我介绍，学校学习情况和经历</li>
<li>项目<ul>
<li>Unity2D游戏的开发思路，技术方案，负责什么；UE4是课程作业，Unity是作业吗，做小游戏的动机是什么；UE4的AI方面，框架、设计之类的</li>
<li>两个项目技术上进步了吗，有什么感悟</li>
</ul>
</li>
<li>我浅薄的理解<ul>
<li>以后的技术发展方向？喜欢的游戏品类？<ul>
<li>就我最真诚的想法，还真没有很长远的方向（因为真的了解的不多，也不知道真实工作是什么样子），就说近期想多多学习基础，补一些基础且必要的知识</li>
<li>说了下去年刚玩的死亡搁浅、泰坦陨落2，其实可能都不是纯种ACT。P4G肯定更不算，就没说。忘了说手游（原神）了</li>
</ul>
</li>
<li>现阶段对角色、AI、场景，哪些方向有技术研究或思考吗？可能怎么实现，或者这方面了解的技术<ul>
<li>还真没有（又是只知道名词），想了一会说了说启发式的游戏AI，行为树之类，和深度学习（去年看过一篇CHI20的论文）、强化学习控制的AI，稍微锐评了一下</li>
<li>也说了几句角色动画，状态机和BlendSpace，蒙太奇，还有从几个主干骨骼单独控制上半身的旋转等，提了一下IK</li>
</ul>
</li>
</ul>
</li>
<li>聊聊，反问<ul>
<li>项目里分AI、3C、战斗、场景、UI等小组</li>
<li>反问环节，我问“实习生会上手哪些工作呢？会负责哪些事情，或者会不会负责事情？”<ul>
<li>得知了一些培养方案</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="结束-1"><a href="#结束-1" class="headerlink" title="结束"></a>结束</h5><p>三面倒是在知识上没什么难度，各种问题我也懒得揣测背后的深意了，都是很真诚地回答。结束后也没有很轻松的感觉。自己一个人说话，心里还是没底。</p>
<p>上网查了一下，有的三面是业务面，也有的是总监面，也就是业务面通过了、给领导看看你是不是正常人，这样的性质。</p>
<p>室友已经全流程走完了，相应的我对后面的事情也心里有数了。</p>
<h4 id="7-hr面试-2022-x2F-1-x2F-25"><a href="#7-hr面试-2022-x2F-1-x2F-25" class="headerlink" title="7 hr面试 2022&#x2F;1&#x2F;25"></a>7 hr面试 2022&#x2F;1&#x2F;25</h4><h5 id="面试（中午-20min）"><a href="#面试（中午-20min）" class="headerlink" title="面试（中午 20min）"></a>面试（中午 20min）</h5><p>上午11点多吧，直接打了个电话来（上海的电话），确认我方便就开始面了。没有事前约或者通知</p>
<ul>
<li>为什么想进入游戏行业<ul>
<li>我可能比较自然，本科接触过一点Unity，也做过Web之类的，现在又在做CV，但找工作还是自然想找游戏相关的工作<ul>
<li>关于Web，前端是原生三件套，后端是SSM、Spring Boot之类的，让我坚定了不做Web、更不再碰Java的想法</li>
<li>关于ML算法就更没热情了，科研到底是在干什么呢。实验室就像debuff，一个人找到算法岗的工作，那得是很优秀的人了，我显然不行</li>
<li>（这两点并没有说出来）</li>
</ul>
</li>
<li>用创作表达自己，或用技术帮助到优秀内容的创作，对我来说都是好事（在<a href="https://acbgzm.github.io/2021/02/14/summary-2020/">去年的年终总结</a>说了）</li>
</ul>
</li>
<li>本科经历，考研的情况，择校考虑，平时成绩</li>
<li>平时爱好</li>
<li>自认为性格好、不好的方面</li>
<li>当晚还要做个综合测试，1h</li>
</ul>
<p>依然是讲的很真诚。</p>
<h4 id="8-后续"><a href="#8-后续" class="headerlink" title="8 后续"></a>8 后续</h4><ul>
<li><p>1&#x2F;27 收到深圳hr总部的电话，告知通过</p>
</li>
<li><p>1&#x2F;28 邮箱收到正式实习offer，以及入职指引</p>
<ul>
<li>会要办银行卡什么的，有途径从深圳办</li>
</ul>
</li>
<li><p>1&#x2F;30 出门剃头拍照片</p>
<ul>
<li>入职指引有个7天的限制，打电话问了一下说可以后面再激活。但还是想过年前把事情都结束吧</li>
<li>入职填的6月初，希望顺利</li>
</ul>
</li>
<li><p>1&#x2F;31 欢度除夕（其实又是奔波）</p>
</li>
</ul>
<h4 id="9-一些感想"><a href="#9-一些感想" class="headerlink" title="9 一些感想"></a>9 一些感想</h4><h5 id="运气"><a href="#运气" class="headerlink" title="运气"></a>运气</h5><p>我深知自己不是平时一直很优秀，或者“先飞”早早准备的类型。基础都忘了，算法也没怎么刷。对于游戏开发，我既不懂引擎（包括图形学基础和各种图形API），也没有对Gameplay做很多实践、或参加比赛（一般是跟着教程做，随用随查）。</p>
<p>这次运气确实很好</p>
<ul>
<li>前面说过，是根据填的意向工作地、感兴趣游戏类型，被推到不同的项目，这一个过程的运气成分可能还挺大的<ul>
<li>不是项目组看简历捞人，而是直接推到各个项目组，显然不同项目组有不同的岗位数量、考核方法</li>
</ul>
</li>
<li>流程内的运气也挺重要的<ul>
<li>我经历的周期比较长，估计也是在sort中不靠前吧，最后才把我捞起来了</li>
</ul>
</li>
</ul>
<p>回想至今经历过的比较正式的面试，也就是直升高中面试（唯一一次线下面的）、研究生复试、这次的实习面试。我不是擅长与人交流的人，每次也没觉得自己表现得很好。但这三次面试都通过了，或许面试也是我的运气加成时刻吧。</p>
<h5 id="心态"><a href="#心态" class="headerlink" title="心态"></a>心态</h5><p>到这里，我可能写得有点矫情：正常面试流程，没必要如临大敌；会就是会、不会就是不会，没必要内心戏这么多。</p>
<ul>
<li><p>为什么我这么累？</p>
<ul>
<li>一方面是我偏向串行、并想单一的事情较多；另一方面我的11、12月实在有点无缝衔接了，各种事情碰在一起，讲完组会赶作业、赶完作业做烂活、做完烂活就面试，身心俱疲</li>
<li>在我的<a href="https://acbgzm.github.io/2022/01/26/misc-1/">这篇博文里稍微记录了一下</a>，虽然也有点矫情</li>
</ul>
</li>
<li><p>为什么好像把这件事看得很重，本来不是试一试的心态吗？</p>
<ul>
<li>首先，我对这种场合会本能地紧张（准备时会有点紧张，真开始了就没事了），倒是习惯了。比起不紧张，我更希望面试的时候脑子会动</li>
<li>随着流程进行下去，确实会有所期待</li>
<li>场外因素：正好放假，无所事事，就算是玩，也会去想这唯一的正事。如果在正常的求职时间，多投几家公司，就不会把一家公司的过程、结果看得很重了，整个时间也会更充实吧</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>工作</tag>
        <tag>生活小记</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode记录 (1)</title>
    <url>/2020/04/16/leetcode-1/</url>
    <content><![CDATA[<p>算法太差了，打算学习一下。leetcode一点点做，先从简单题开始，重点放在熟悉STL用法。</p>
<span id="more"></span>

<p>需要回看的题目：<a href="#4-1365-%E6%9C%89%E5%A4%9A%E5%B0%91%E5%B0%8F%E4%BA%8E%E5%BD%93%E5%89%8D%E6%95%B0%E5%AD%97%E7%9A%84%E6%95%B0%E5%AD%97-httpshttpsleetcode-cncomproblemshow-many-numbers-are-smaller-than-the-current-number%E9%9A%BE%E5%BA%A6%E7%AE%80%E5%8D%95">4</a>  <a href="#16-%E9%9D%A2%E8%AF%95%E9%A2%9855---i-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6-httpsleetcode-cncomproblemser-cha-shu-de-shen-du-lcof%E9%9A%BE%E5%BA%A6%E7%AE%80%E5%8D%95">16</a>  <a href="#20-1323-6-%E5%92%8C-9-%E7%BB%84%E6%88%90%E7%9A%84%E6%9C%80%E5%A4%A7%E6%95%B0%E5%AD%97-httpsleetcode-cncomproblemsmaximum-69-number%E9%9A%BE%E5%BA%A6%E7%AE%80%E5%8D%95">20</a></p>
<hr>
<p>写完更新：记录了20题，虽然leetcode都标为简单题，但我感到难度差异很大。练手的目的应该是达到了，但记录起来效率比较低。以后只记录有必要回顾的题，每篇记录10题。</p>
<hr>
<h3 id="1-面试题58-II-左旋转字符串-难度：简单"><a href="#1-面试题58-II-左旋转字符串-难度：简单" class="headerlink" title="1 面试题58 - II. 左旋转字符串  难度：简单"></a>1 <a href="https://leetcode-cn.com/problems/zuo-xuan-zhuan-zi-fu-chuan-lcof/">面试题58 - II. 左旋转字符串  </a>难度：简单</h3><p>我的土味解法：三次reverse</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    void reverse(string&amp; s,int begin, int length)&#123;
        for(int i&#x3D;0;i&lt;length&#x2F;2;i++)&#123;
            char temp&#x3D;s[begin+i];
            s[begin+i]&#x3D;s[begin+length-1-i];
            s[begin+length-1-i]&#x3D;temp;
        &#125;
    &#125;
    string reverseLeftWords(string s, int n) &#123;
        int l&#x3D;s.length();
        reverse(s,0,l);
        reverse(s,0,l-n);
        reverse(s,l-n,n);
        return s;
    &#125;
&#125;;</code></pre>

<blockquote>
<p>执行用时 :8 ms, 在所有 C++ 提交中击败了64.43% 的用户<br>内存消耗 :7.1 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<p><strong><a href="https://leetcode-cn.com/problems/zuo-xuan-zhuan-zi-fu-chuan-lcof/solution/yi-xing-jiu-gou-liao-by-yzwz-2/">网友高级解法（作者：yzwz）</a></strong></p>
<p>一行代码即可，把两个字符串拼起来，中间截取一部分</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    string reverseLeftWords(string s, int n) &#123;
        return (s+s).substr(n,s.size());
    &#125;
&#125;;</code></pre>
<hr>
<h3 id="2-LCP-1-猜数字-难度：简单"><a href="#2-LCP-1-猜数字-难度：简单" class="headerlink" title="2 LCP 1. 猜数字  难度：简单"></a>2 <a href="https://leetcode-cn.com/problems/guess-numbers/">LCP 1. 猜数字  </a>难度：简单</h3><p>也太简单了，怎么跟平时在leetcode看到的简单题不一样。。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int game(vector&lt;int&gt;&amp; guess, vector&lt;int&gt;&amp; answer) &#123;
        int count&#x3D;0;
        for(int i&#x3D;0;i&lt;3;i++)&#123;
            if(guess[i]&#x3D;&#x3D;answer[i]) count++;
        &#125;
        return count;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :0 ms, 在所有 C++ 提交中击败了100.00% 的用户<br>内存消耗 :6.2 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<p><strong><a href="https://leetcode-cn.com/problems/guess-numbers/solution/cyi-xing-dai-ma-yi-huo-shuang-bai-by-lucky_dog/">网友解法（作者：Lucky_dog）</a></strong></p>
<p>一行异或。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int game(vector&lt;int&gt;&amp; guess, vector&lt;int&gt;&amp; answer) &#123;
        return !(guess[0]^answer[0]) + !(guess[1]^answer[1]) + !(guess[2]^answer[2]);
    &#125;
&#125;;</code></pre>
<hr>
<h3 id="3-1342-将数字变成-0-的操作次数-难度：简单"><a href="#3-1342-将数字变成-0-的操作次数-难度：简单" class="headerlink" title="3 1342. 将数字变成 0 的操作次数  难度：简单"></a>3 <a href="https://leetcode-cn.com/problems/number-of-steps-to-reduce-a-number-to-zero/">1342. 将数字变成 0 的操作次数  </a>难度：简单</h3><p>很简单。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int numberOfSteps (int num) &#123;
        int count&#x3D;0;
        while(num)&#123;
            if(num%2&#x3D;&#x3D;0) num&#x2F;&#x3D;2;
            else num--;
            count++;
        &#125;
        return count;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :0 ms, 在所有 C++ 提交中击败了100.00% 的用户<br>内存消耗 :6 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="4-1365-有多少小于当前数字的数字-难度：简单"><a href="#4-1365-有多少小于当前数字的数字-难度：简单" class="headerlink" title="4 1365. 有多少小于当前数字的数字  难度：简单"></a>4 <a href="%5Bhttps://%5D(https://leetcode-cn.com/problems/how-many-numbers-are-smaller-than-the-current-number/)">1365. 有多少小于当前数字的数字  </a>难度：简单</h3><pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    vector&lt;int&gt; smallerNumbersThanCurrent(vector&lt;int&gt;&amp; nums) &#123;
        vector&lt;int&gt; result;
        int length&#x3D;nums.size();
        for(int i&#x3D;0;i&lt;length;i++)&#123;
            int count&#x3D;0;
            for(int j&#x3D;0;j&lt;length;j++)&#123;
                if(nums[j]&lt;nums[i]) count++;
            &#125;
            result.push_back(count);
        &#125;
        return result;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :52 ms, 在所有 C++ 提交中击败了13.57% 的用户<br>内存消耗 :7 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<p><strong><a href="https://leetcode-cn.com/problems/how-many-numbers-are-smaller-than-the-current-number/solution/you-duo-shao-xiao-yu-dang-qian-shu-zi-de-shu-zi-to/">网友解法（作者：huwt）</a></strong></p>
<ul>
<li>学到了vector定义时可以用参数<code>vector&lt;int&gt; num(nums.size(),0);</code>跟数组差不多。</li>
<li>我忽略了题目条件：所有数不大于100。遍历第一遍存放每个数出现次数；第二遍从小到大累加即可。</li>
</ul>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    vector&lt;int&gt; smallerNumbersThanCurrent(vector&lt;int&gt;&amp; nums) &#123;
        int arr[101];
        memset(arr, 0, sizeof(arr));
        &#x2F;* 初始化计数桶 *&#x2F;
        for (auto i : nums) &#123;
            arr[i] ++;
        &#125;
        &#x2F;* 累加处理计数桶，使得 arr[i] 表示比 i 小的数字的个数 *&#x2F;
        int cnt &#x3D; 0;
        for (int i &#x3D; 0; i &lt;&#x3D; 100; i ++) &#123;
            int temp &#x3D; arr[i];
            arr[i] &#x3D; cnt;
            cnt +&#x3D; temp;
        &#125;
        vector&lt;int&gt; ret;
        &#x2F;* 遍历 nums，取出对应桶 arr[i] 里的结果即可 *&#x2F;
        for (int i : nums) &#123;
            ret.push_back(arr[i]);
        &#125;
        return ret;
    &#125;
&#125;;</code></pre>

<p><strong>改进我的解法</strong></p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    vector&lt;int&gt; smallerNumbersThanCurrent(vector&lt;int&gt;&amp; nums) &#123;
        vector&lt;int&gt; result;
        int a[102]&#x3D;&#123;0&#125;;
        for(int i&#x3D;0;i&lt;nums.size();i++)&#123;
            a[nums[i]]++;
        &#125;
        int count&#x3D;0;
        for(int i&#x3D;0;i&lt;101;i++)&#123;
            int temp &#x3D; a[i];
            a[i] &#x3D; count;   &#x2F;*严格小于*&#x2F;
            count +&#x3D; temp;  &#x2F;*把等于的加上*&#x2F;
        &#125;

        for(int i&#x3D;0;i&lt;nums.size();i++)&#123;
            result.push_back(a[nums[i]]);
        &#125;
        
        return result;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :8 ms, 在所有 C++ 提交中击败了88.68% 的用户<br>内存消耗 :7.3 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="5-1281-整数的各位积和之差-难度：简单"><a href="#5-1281-整数的各位积和之差-难度：简单" class="headerlink" title="5 1281. 整数的各位积和之差    难度：简单"></a>5 <a href="https://leetcode-cn.com/problems/subtract-the-product-and-sum-of-digits-of-an-integer/">1281. 整数的各位积和之差    </a>难度：简单</h3><p>简单模拟</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int subtractProductAndSum(int n) &#123;
        int a&#x3D;1,b&#x3D;0;
        while(n)&#123;
            int t&#x3D;n%10;
            a*&#x3D;t;
            b+&#x3D;t;
            n&#x2F;&#x3D;10;
        &#125;
        return a-b;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :4 ms, 在所有 C++ 提交中击败了53.31% 的用户<br>内存消耗 :5.9 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="6-题目名称-难度：简单"><a href="#6-题目名称-难度：简单" class="headerlink" title="6 题目名称  难度：简单"></a>6 <a href="https://">题目名称  </a>难度：简单</h3><p>依然是简单模拟，不用stl会快一点。<br>Dev C++编译to_string()出错，原因是不支持C++ 11的一些特性。需要<a href="https://blog.csdn.net/qq_43492703/article/details/91042595">设置环境</a>。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int findNumbers(vector&lt;int&gt;&amp; nums) &#123;
        int count&#x3D;0;
        string s;
        for(vector&lt;int&gt;::iterator it&#x3D;nums.begin();it!&#x3D;nums.end();it++)&#123;
            s &#x3D; to_string(*it);
            if(s.size()%2&#x3D;&#x3D;0)&#123;
                count++;
            &#125;
        &#125;
        return count;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :16 ms, 在所有 C++ 提交中击败了16.81% 的用户<br>内存消耗 :9.7 MB, 在所有 C++ 提交中击败了5.14%的用户</p>
</blockquote>
<hr>
<h3 id="7-771-宝石与石头-难度：简单"><a href="#7-771-宝石与石头-难度：简单" class="headerlink" title="7 771. 宝石与石头  难度：简单"></a>7 <a href="https://leetcode-cn.com/problems/jewels-and-stones/">771. 宝石与石头  </a>难度：简单</h3><p>可以直接二重循环，也可以用stl。我用的hash。其实直接string.find(c)就可以。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int numJewelsInStones(string J, string S) &#123;
        int count&#x3D;0;
        bool hash[52]&#x3D;&#123;0&#125;;
        for(int i&#x3D;0;i&lt;J.size();i++)&#123;
            if(J[i]&gt;&#x3D;&#39;A&#39;&amp;&amp;J[i]&lt;&#x3D;&#39;Z&#39;) hash[J[i]-&#39;A&#39;]&#x3D;1;
            else if(J[i]&gt;&#x3D;&#39;a&#39;&amp;&amp;J[i]&lt;&#x3D;&#39;z&#39;) hash[J[i]-&#39;a&#39;+26]&#x3D;1;
        &#125;
        for(int i&#x3D;0;i&lt;S.size();i++)&#123;
            if(S[i]&gt;&#x3D;&#39;A&#39;&amp;&amp;S[i]&lt;&#x3D;&#39;Z&#39;)&#123;
                if(hash[S[i]-&#39;A&#39;]) count++;
            &#125; 
            else if(S[i]&gt;&#x3D;&#39;a&#39;&amp;&amp;S[i]&lt;&#x3D;&#39;z&#39;)&#123;
                if(hash[S[i]-&#39;a&#39;+26]) count++;
            &#125;
        &#125;
        return count;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :4 ms, 在所有 C++ 提交中击败了76.45% 的用户<br>内存消耗 :6.3 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<p><strong><a href="https://leetcode-cn.com/problems/jewels-and-stones/solution/shu-zu-he-unordered_mapde-bi-jiao-by-jacder/">网友解法（作者：jacder）</a></strong></p>
<p>关于上面提到的各种容器，<a href="https://leetcode-cn.com/problems/jewels-and-stones/solution/shu-zu-he-unordered_mapde-bi-jiao-by-jacder/">可以看看这篇解法。</a>代码就不贴了。</p>
<hr>
<h3 id="8-1389-按既定顺序创建目标数组-难度：简单"><a href="#8-1389-按既定顺序创建目标数组-难度：简单" class="headerlink" title="8 1389. 按既定顺序创建目标数组  难度：简单"></a>8 <a href="https://leetcode-cn.com/problems/create-target-array-in-the-given-order/">1389. 按既定顺序创建目标数组  </a>难度：简单</h3><p>注意vector的insert()用法，<strong>第一个参数是迭代器，第二个参数是要存储的元素值。</strong></p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    vector&lt;int&gt; createTargetArray(vector&lt;int&gt;&amp; nums, vector&lt;int&gt;&amp; index) &#123;
        vector&lt;int&gt; target;
        for(int i&#x3D;0;i&lt;nums.size();i++)&#123;
            target.insert(target.begin()+index[i], nums[i]);
        &#125;
        return target;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :4 ms, 在所有 C++ 提交中击败了66.73% 的用户<br>内存消耗 :8.5 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="9-1108-IP-地址无效化-难度：简单"><a href="#9-1108-IP-地址无效化-难度：简单" class="headerlink" title="9 1108. IP 地址无效化  难度：简单"></a>9 <a href="https://leetcode-cn.com/problems/defanging-an-ip-address/">1108. IP 地址无效化  </a>难度：简单</h3><p>很简单</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    string defangIPaddr(string address) &#123;
        string s;
        for(int i&#x3D;0;i&lt;address.size();i++)&#123;
            if(address[i]&#x3D;&#x3D;&#39;.&#39;) s+&#x3D;&quot;[.]&quot;;
            else s+&#x3D;address[i];
        &#125;
        return s;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :0 ms, 在所有 C++ 提交中击败了100.00% 的用户<br>内存消耗 :6 MB, 在所有 C++ 提交中击败了100.00%的用户 </p>
</blockquote>
<hr>
<h3 id="10-1313-解压缩编码列表-难度：简单"><a href="#10-1313-解压缩编码列表-难度：简单" class="headerlink" title="10 1313. 解压缩编码列表  难度：简单"></a>10 <a href="https://leetcode-cn.com/problems/decompress-run-length-encoded-list/">1313. 解压缩编码列表  </a>难度：简单</h3><p>很简单</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    vector&lt;int&gt; decompressRLElist(vector&lt;int&gt;&amp; nums)&#123;
        vector&lt;int&gt; vec;
        for(int i&#x3D;1;i&lt;nums.size();i+&#x3D;2)&#123;
            for(int j&#x3D;0;j&lt;nums[i-1];j++) vec.push_back(nums[i]);
        &#125;
        return vec;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :8 ms, 在所有 C++ 提交中击败了65.87% 的用户<br>内存消耗 :10.1 MB, 在所有 C++ 提交中击败了100.00%的用户 </p>
</blockquote>
<p>看了一下题解，可以使用vector的insert()函数，参数分别是：位置，个数，值</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">for(vector&lt;int&gt;::iterator iter &#x3D; nums.begin(); iter !&#x3D; nums.end(); iter +&#x3D; 2) &#123;
    vec.insert(vec.end(), *it, *(it + 1));
&#125;</code></pre>

<hr>
<h3 id="11-1266-访问所有点的最小时间-难度：简单"><a href="#11-1266-访问所有点的最小时间-难度：简单" class="headerlink" title="11 1266. 访问所有点的最小时间  难度：简单"></a>11 <a href="https://leetcode-cn.com/problems/minimum-time-visiting-all-points/">1266. 访问所有点的最小时间  </a>难度：简单</h3><p>数学题，我的想法是找到相差最大的维度即可。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int minTimeToVisitAllPoints(vector&lt;vector&lt;int&gt;&gt;&amp; points) &#123;
        int t&#x3D;0;
        for(int i&#x3D;0;i&lt;points.size()-1;i++)&#123;
            int a&#x3D;abs(points[i][0]-points[i+1][0]);
            int b&#x3D;abs(points[i][1]-points[i+1][1]);
            int c&#x3D;a&gt;b?a:b;
            t+&#x3D;c;
        &#125;
        return t;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :8 ms, 在所有 C++ 提交中击败了89.64% 的用户<br>内存消耗 :9.6 MB, 在所有 C++ 提交中击败了100.00%的用户 </p>
</blockquote>
<hr>
<h3 id="12-1290-二进制链表转整数-难度：简单"><a href="#12-1290-二进制链表转整数-难度：简单" class="headerlink" title="12 1290. 二进制链表转整数  难度：简单"></a>12 <a href="https://leetcode-cn.com/problems/convert-binary-number-in-a-linked-list-to-integer/">1290. 二进制链表转整数  </a>难度：简单</h3><p>基本的链表操作题，进制转换题目往权值考虑。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int getDecimalValue(ListNode* head) &#123;
        int sum&#x3D;0;
        ListNode* p&#x3D;head;
        while(p!&#x3D;NULL)&#123;
            sum&#x3D;sum*2+p-&gt;val;
            p&#x3D;p-&gt;next;
        &#125;
        return sum;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :4 ms, 在所有 C++ 提交中击败了68.52% 的用户<br>内存消耗 :8.1 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="13-237-删除链表中的节点-难度：简单"><a href="#13-237-删除链表中的节点-难度：简单" class="headerlink" title="13 237. 删除链表中的节点  难度：简单"></a>13 <a href="https://leetcode-cn.com/problems/delete-node-in-a-linked-list/">237. 删除链表中的节点  </a>难度：简单</h3><p>阅读理解题。就给了一个参数，意思是不删除节点，而是换掉节点存储的数的值。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    void deleteNode(ListNode* node) &#123;
        node-&gt;val&#x3D;node-&gt;next-&gt;val;
        node-&gt;next&#x3D;node-&gt;next-&gt;next;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :20 ms, 在所有 C++ 提交中击败了11.62% 的用户<br>内存消耗 :7.8 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="14-面试题-02-03-删除中间节点-难度：简单"><a href="#14-面试题-02-03-删除中间节点-难度：简单" class="headerlink" title="14 面试题 02.03. 删除中间节点  难度：简单"></a>14 <a href="https://leetcode-cn.com/problems/delete-middle-node-lcci/">面试题 02.03. 删除中间节点  </a>难度：简单</h3><p>跟上一题一样。这次把不要的节点删了。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    void deleteNode(ListNode* node) &#123;
        ListNode* x&#x3D;node-&gt;next;
        node-&gt;val&#x3D;x-&gt;val;
        node-&gt;next&#x3D;x-&gt;next;
        delete x;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :28 ms, 在所有 C++ 提交中击败了11.97% 的用户<br>内存消耗 :7.8 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="15-面试题-02-02-返回倒数第-k-个节点-难度：简单"><a href="#15-面试题-02-02-返回倒数第-k-个节点-难度：简单" class="headerlink" title="15 面试题 02.02. 返回倒数第 k 个节点  难度：简单"></a>15 <a href="https://leetcode-cn.com/problems/kth-node-from-end-of-list-lcci/">面试题 02.02. 返回倒数第 k 个节点  </a>难度：简单</h3><p>双指针</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int kthToLast(ListNode* head, int k) &#123;
        ListNode* a&#x3D;head;
        ListNode* b&#x3D;head;
        for(int i&#x3D;0;i&lt;k-1;i++) a&#x3D;a-&gt;next;
        while(a-&gt;next!&#x3D;NULL)&#123;
            a&#x3D;a-&gt;next;
            b&#x3D;b-&gt;next;
        &#125;
        return b-&gt;val;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :4 ms, 在所有 C++ 提交中击败了75.90% 的用户<br>内存消耗 :10.4 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="16-面试题55-I-二叉树的深度-难度：简单"><a href="#16-面试题55-I-二叉树的深度-难度：简单" class="headerlink" title="16 面试题55 - I. 二叉树的深度  难度：简单"></a>16 <a href="https://leetcode-cn.com/problems/er-cha-shu-de-shen-du-lcof/">面试题55 - I. 二叉树的深度  </a>难度：简单</h3><p>最基本的二叉树操作，后序遍历</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">&#x2F;**
 * Definition for a binary tree node.
 * struct TreeNode &#123;
 *     int val;
 *     TreeNode *left;
 *     TreeNode *right;
 *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;
 * &#125;;
 *&#x2F;
class Solution &#123;
public:
    int maxDepth(TreeNode* root) &#123;
        if(root &#x3D;&#x3D; NULL) return 0;
        int lDepth &#x3D; 1, rDepth &#x3D; 1;
        lDepth +&#x3D; maxDepth(root-&gt;left);
        rDepth +&#x3D; maxDepth(root-&gt;right);
        return lDepth&gt;rDepth?lDepth:rDepth;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :16 ms, 在所有 C++ 提交中击败了47.13% 的用户<br>内存消耗 :19 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="17-面试题17-打印从1到最大的n位数-难度：简单"><a href="#17-面试题17-打印从1到最大的n位数-难度：简单" class="headerlink" title="17 面试题17. 打印从1到最大的n位数  难度：简单"></a>17 <a href="https://leetcode-cn.com/problems/da-yin-cong-1dao-zui-da-de-nwei-shu-lcof/">面试题17. 打印从1到最大的n位数  </a>难度：简单</h3><p>简单题</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    vector&lt;int&gt; printNumbers(int n) &#123;
        vector&lt;int&gt; vec;
        int max &#x3D; 0; 
        while(n--)&#123;
            max &#x3D; max*10 + 9;
        &#125;
        for(int i&#x3D;1;i&lt;&#x3D;max;i++)&#123;
            vec.push_back(i);
        &#125;
        return vec;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :4 ms, 在所有 C++ 提交中击败了98.14% 的用户<br>内存消耗 :11.4 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="18-709-转换成小写字母-难度：简单"><a href="#18-709-转换成小写字母-难度：简单" class="headerlink" title="18 709. 转换成小写字母  难度：简单"></a>18 <a href="https://leetcode-cn.com/problems/to-lower-case/">709. 转换成小写字母  </a>难度：简单</h3><p>简单题</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    string toLowerCase(string str) &#123;
        for(int i&#x3D;0;i&lt;str.length();i++)&#123;
            if(str[i]&gt;&#x3D;&#39;A&#39;&amp;&amp;str[i]&lt;&#x3D;&#39;Z&#39;)&#123;
                str[i] -&#x3D; (&#39;Z&#39; - &#39;z&#39;);
            &#125;
        &#125;
        return str;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :0 ms, 在所有 C++ 提交中击败了100.00% 的用户<br>内存消耗 :6.3 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="19-1323-6-和-9-组成的最大数字-难度：简单"><a href="#19-1323-6-和-9-组成的最大数字-难度：简单" class="headerlink" title="19 1323. 6 和 9 组成的最大数字  难度：简单"></a>19 <a href="https://leetcode-cn.com/problems/maximum-69-number/">1323. 6 和 9 组成的最大数字  </a>难度：简单</h3><p>题解里学到一个函数<code>string s = to_string(num);</code></p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int maximum69Number (int num) &#123;
        vector&lt;int&gt; a;
        while(num!&#x3D;0)&#123;
            a.push_back(num%10);
            num &#x2F;&#x3D; 10;
        &#125;
        for(int i&#x3D;a.size()-1;i&gt;&#x3D;0;i--)&#123;
            if(a[i]&#x3D;&#x3D;6)&#123;
                a[i]&#x3D;9;
                break;
            &#125;
        &#125;
        int r&#x3D;0;
        for(int i&#x3D;a.size()-1;i&gt;&#x3D;0;i--)&#123;
            r &#x3D; r*10 + a[i];
        &#125;
        return r;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :0 ms, 在所有 C++ 提交中击败了100.00% 的用户<br>内存消耗 :6 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="20-1323-6-和-9-组成的最大数字-难度：简单"><a href="#20-1323-6-和-9-组成的最大数字-难度：简单" class="headerlink" title="20 1323. 6 和 9 组成的最大数字  难度：简单"></a>20 <a href="https://leetcode-cn.com/problems/maximum-69-number/">1323. 6 和 9 组成的最大数字  </a>难度：简单</h3><p>没想出来好解法，还是笨办法模拟。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int oddCells(int n, int m, vector&lt;vector&lt;int&gt;&gt;&amp; indices) &#123;
        vector&lt;vector&lt;int&gt;&gt; a(n, vector&lt;int&gt; (m, 0));
        int count &#x3D; 0;
        for(int i&#x3D;0;i&lt;indices.size();i++)&#123;
            for(int j&#x3D;0;j&lt;m;j++)&#123;
                a[indices[i][0]][j]++;
            &#125;
            for(int j&#x3D;0;j&lt;n;j++)&#123;
                a[j][indices[i][1]]++;
            &#125;
        &#125;
        for(int i&#x3D;0;i&lt;n;i++)&#123;
            for(int j&#x3D;0;j&lt;m;j++)&#123;
                if(a[i][j]%2&#x3D;&#x3D;1) count++;
            &#125;
        &#125;
        return count;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :8 ms, 在所有 C++ 提交中击败了51.03% 的用户<br>内存消耗 :8.1 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<p><a href="https://leetcode-cn.com/problems/cells-with-odd-values-in-a-matrix/solution/ti-jie-1252-qi-shu-zhi-dan-yuan-ge-de-shu-mu-by-ze/">这篇题解写了几种思路  </a>作者：zerotrac2</p>
<hr>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode记录 (2)</title>
    <url>/2020/04/22/leetcode-2/</url>
    <content><![CDATA[<p>leetcode刷题笔记</p>
<span id="more"></span>

<h3 id="21-面试题13-机器人的运动范围-难度：中等"><a href="#21-面试题13-机器人的运动范围-难度：中等" class="headerlink" title="21 面试题13. 机器人的运动范围  难度：中等"></a>21 <a href="https://leetcode-cn.com/problems/ji-qi-ren-de-yun-dong-fan-wei-lcof/">面试题13. 机器人的运动范围  </a>难度：中等</h3><p>奇奇怪怪bug：先检验x和y是否越界，再检验visited[x][y]是否访问。否则会出现诡异的越界错误。</p>
<p>注意vector二维数组的定义和初始化写法。</p>
<p>dfs思想，多加练习。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int movingCount(int m, int n, int k) &#123;
        vector&lt;vector&lt;int&gt; &gt; visited (m, vector&lt;int&gt; (n, 0));
        return dfs(0, 0, m, n, k, visited);
    &#125;
    int dfs(int x, int y, int m, int n, int k, vector&lt;vector&lt;int&gt; &gt;&amp; visited)&#123;
        if(x&gt;&#x3D;m || y&gt;&#x3D;n ||visited[x][y] &#x3D;&#x3D; 1 ||  (x&#x2F;10 + x%10 + y&#x2F;10 + y%10) &gt; k)&#123;
            return 0;
        &#125;
        visited[x][y] &#x3D; 1;
        return 1 + dfs(x+1, y, m, n, k, visited) + dfs(x, y+1, m, n, k, visited);
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :0 ms, 在所有 C++ 提交中击败了100.00% 的用户<br>内存消耗 :6.9 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="22-151-翻转字符串里的单词-难度：中等"><a href="#22-151-翻转字符串里的单词-难度：中等" class="headerlink" title="22 151. 翻转字符串里的单词  难度：中等"></a>22 <a href="https://leetcode-cn.com/problems/reverse-words-in-a-string/">151. 翻转字符串里的单词  </a>难度：中等</h3><p>我的方法：vector存储string，先后面加个空格就不用最后特殊判断了，用bool控制忽略连续的空格。</p>
<p>因为库用的比较多，应该很费时。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    string reverseWords(string s) &#123;
        s+&#x3D;&#39; &#39;;
        vector&lt;string&gt; vec;
        int t&#x3D;0;
        bool space&#x3D;false;
        string temp;
        while(t&lt;s.size())&#123;
            if(s[t]!&#x3D;&#39; &#39;)&#123;
                temp+&#x3D;s[t];
                space&#x3D;true;
            &#125;
            else if(s[t]&#x3D;&#x3D;&#39; &#39;)&#123;
                if(space&#x3D;&#x3D;true)&#123;
                    vec.push_back(temp);
                    temp.clear();
                    space&#x3D;false;	
                &#125;
            &#125;
            t++;
        &#125;
        string s1;
        for(int i&#x3D;vec.size()-1;i&gt;&#x3D;0;i--)&#123;
            if(i!&#x3D;0)&#123;
                s1+&#x3D;vec[i];
                s1+&#x3D;&#39; &#39;;
            &#125;
            else&#123;
                s1+&#x3D;vec[i];
            &#125;
        &#125;
        return s1;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :24 ms, 在所有 C++ 提交中击败了18.68% 的用户<br>内存消耗 :8.7 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="23-445-两数相加-II-难度：中等"><a href="#23-445-两数相加-II-难度：中等" class="headerlink" title="23 445. 两数相加 II  难度：中等"></a>23 <a href="https://leetcode-cn.com/problems/add-two-numbers-ii/">445. 两数相加 II  </a>难度：中等</h3><p>用的<strong>栈存储</strong>两个链表，<strong>头插法</strong>建立新链表。</p>
<p>很多其他思路可以看看题解区，反转链表，转字符串，也有原地操作的，等等。</p>
<p>坑：判断循环结束时，两栈空但有进位，不能结束循环。</p>
<p>定义节点的时候别忘了用构造函数初始化一下。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">&#x2F;**
 * Definition for singly-linked list.
 * struct ListNode &#123;
 *     int val;
 *     ListNode *next;
 *     ListNode(int x) : val(x), next(NULL) &#123;&#125;
 * &#125;;
 *&#x2F;
class Solution &#123;
public:
    ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) &#123;
        stack&lt;int&gt; s1, s2;
        while(l1!&#x3D;NULL)&#123;
            s1.push(l1-&gt;val);
            l1&#x3D;l1-&gt;next;
        &#125;
        while(l2!&#x3D;NULL)&#123;
            s2.push(l2-&gt;val);
            l2&#x3D;l2-&gt;next;
        &#125;
        int carry&#x3D;0,sum&#x3D;0;
        ListNode* head &#x3D; new ListNode(0);
        while(!s1.empty() || !s2.empty() || carry!&#x3D;0)&#123;
            int a &#x3D; s1.empty()?0:s1.top();
            int b &#x3D; s2.empty()?0:s2.top();
            sum &#x3D; a + b + carry;
            ListNode* l &#x3D; new ListNode(sum%10);
            carry&#x3D;sum&#x2F;10;

            l-&gt;next &#x3D; head-&gt;next;
            head-&gt;next &#x3D; l;
            
            if(!s1.empty()) s1.pop();
            if(!s2.empty()) s2.pop();        
        &#125;
        return head-&gt;next;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :64 ms, 在所有 C++ 提交中击败了12.70% 的用户<br>内存消耗 :73.8 MB, 在所有 C++ 提交中击败了11.11%的用户</p>
</blockquote>
<hr>
<h3 id="24-11-盛最多水的容器-难度：中等"><a href="#24-11-盛最多水的容器-难度：中等" class="headerlink" title="24 11. 盛最多水的容器  难度：中等"></a>24 <a href="https://leetcode-cn.com/problems/container-with-most-water/">11. 盛最多水的容器  </a>难度：中等</h3><p>一看题目就知道有好算法，然后怒模拟：</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int maxArea(vector&lt;int&gt;&amp; height) &#123;
        int m_a &#x3D; 0;
        int l &#x3D; height.size();
        for(int i&#x3D;0;i&lt;l-1;i++)&#123;
            for(int j&#x3D;i+1;j&lt;l;j++)&#123;
                int area &#x3D; (j-i)*min(height[i], height[j]);
                m_a &#x3D; area&gt;m_a?area:m_a;
            &#125;
        &#125;
        return m_a;
    &#125;
&#125;;</code></pre>

<p>果然超出时间限制了！^ ^</p>
<p>根据题解，使用双指针：</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int maxArea(vector&lt;int&gt;&amp; height) &#123;
        int max &#x3D; 0;
        int head &#x3D; 0, rear &#x3D; height.size()-1;
        while(head&lt;rear)&#123;
            int area &#x3D; (rear - head) * min(height[head], height[rear]);
            if(area&gt;max) max&#x3D;area;

            if(height[head]&gt;height[rear]) rear--;
            else head++;
        &#125;
        return max;
    &#125;
&#125;;</code></pre>
<blockquote>
<p>执行用时 :16 ms, 在所有 C++ 提交中击败了85.84% 的用户<br>内存消耗 :7.5 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<p>做到现在，99%的题依然感觉毫无思路，只能模拟。算法能力到底是见得太多–&gt;看什么都熟悉–&gt;立刻就知道往哪方面想，还是能凭空想出来好的解法？我暂且蒙在鼓里</p>
<hr>
<h3 id="25-206-反转链表-难度：中等"><a href="#25-206-反转链表-难度：中等" class="headerlink" title="25 206. 反转链表  难度：中等"></a>25 <a href="https://leetcode-cn.com/problems/reverse-linked-list/">206. 反转链表  </a>难度：中等</h3><p>反转链表，双指针。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">&#x2F;**
 * Definition for singly-linked list.
 * struct ListNode &#123;
 *     int val;
 *     ListNode *next;
 *     ListNode(int x) : val(x), next(NULL) &#123;&#125;
 * &#125;;
 *&#x2F;
class Solution &#123;
public:
    ListNode* reverseList(ListNode* head) &#123;
        ListNode *p &#x3D; head, *q &#x3D; NULL;
        while(p!&#x3D;NULL)&#123;
            ListNode* temp &#x3D; p-&gt;next;
            p-&gt;next &#x3D; q;
            q &#x3D; p;
            p &#x3D; temp;
        &#125;
        return q;
    &#125;
&#125;;</code></pre>

<blockquote>
<p>执行用时 :12 ms, 在所有 C++ 提交中击败了31.29% 的用户<br>内存消耗 :8.4 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="26-200-岛屿数量-难度：中等"><a href="#26-200-岛屿数量-难度：中等" class="headerlink" title="26 200. 岛屿数量  难度：中等"></a>26 <a href="https://leetcode-cn.com/problems/number-of-islands/">200. 岛屿数量  </a>难度：中等</h3><p>我的代码很不优雅，dfs不够熟练。而且我的函数参数太多了注意一下网友的简洁解法。</p>
<p>有一个小坑：输入是空[]时要单独处理。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    
    void search(int i, int j, vector&lt;vector&lt;char&gt;&gt;&amp; grid)&#123;
        grid[i][j]&#x3D;&#39;0&#39;;
        if(i&lt;(grid.size()-1) &amp;&amp; grid[i+1][j]&#x3D;&#x3D;&#39;1&#39;) search(i+1,j,grid);
        if(j&lt;(grid[0].size()-1) &amp;&amp; grid[i][j+1]&#x3D;&#x3D;&#39;1&#39;) search(i,j+1,grid);
        if(i&gt;0 &amp;&amp; grid[i-1][j]&#x3D;&#x3D;&#39;1&#39;) search(i-1,j,grid);
        if(j&gt;0 &amp;&amp; grid[i][j-1]&#x3D;&#x3D;&#39;1&#39;) search(i,j-1,grid);
    &#125;
    int numIslands(vector&lt;vector&lt;char&gt;&gt;&amp; grid) &#123;
        if(grid.empty()) return 0;
        
        int num &#x3D; 0;
        for(int i&#x3D;0;i&lt;grid.size();i++)&#123;
            for(int j&#x3D;0;j&lt;grid[0].size();j++)&#123;
                    if(grid[i][j] &#x3D;&#x3D; &#39;1&#39;)&#123;
                        num++;
                        search(i,j,grid);
                    &#125;    &#x2F;* 是陆地 *&#x2F; 
                
            &#125;
        &#125;
        return num;
    &#125;
&#125;;</code></pre>

<blockquote>
<p>执行用时 :12 ms, 在所有 C++ 提交中击败了86.76% 的用户<br>内存消耗 :8.3 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="27-7-整数反转-难度：简单"><a href="#27-7-整数反转-难度：简单" class="headerlink" title="27 7. 整数反转  难度：简单"></a>27 <a href="https://leetcode-cn.com/problems/reverse-integer/">7. 整数反转  </a>难度：简单</h3><p>这是道简单题，记录一下INT_MAX和INT_MIN的用法。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int reverse(int x) &#123;
        int y&#x3D;0;
        while(x)&#123;
            if(y&gt;INT_MAX&#x2F;10 || y&lt;INT_MIN&#x2F;10)&#123;
                return 0;
            &#125;
            int t &#x3D; x%10;
            y &#x3D; y*10 + t;
            x &#x2F;&#x3D; 10;
        &#125;
        return y;

    &#125;
&#125;;</code></pre>

<blockquote>
<p>执行用时 :4 ms, 在所有 C++ 提交中击败了67.75% 的用户<br>内存消耗 :6.1 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="28-13-罗马数字转整数-难度：简单"><a href="#28-13-罗马数字转整数-难度：简单" class="headerlink" title="28 13. 罗马数字转整数  难度：简单"></a>28 <a href="https://leetcode-cn.com/problems/reverse-integer/">13. 罗马数字转整数  </a>难度：简单</h3><p>也是简单题，复习map的用法。map赋值不用函数。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int romanToInt(string s) &#123;
        unordered_map&lt;char, int&gt; mp;
        mp[&#39;I&#39;] &#x3D; 1;
        mp[&#39;V&#39;] &#x3D; 5;
        mp[&#39;X&#39;] &#x3D; 10;
        mp[&#39;L&#39;] &#x3D; 50;
        mp[&#39;C&#39;] &#x3D; 100;
        mp[&#39;D&#39;] &#x3D; 500;
        mp[&#39;M&#39;] &#x3D; 1000;

        int sum &#x3D; 0;
        for(int i&#x3D;0;i&lt;s.size();i++)&#123;
            if(mp[s[i]] &lt; mp[s[i+1]])&#123;
                sum -&#x3D; mp[s[i]];
            &#125;
            else &#123;
                sum +&#x3D; mp[s[i]];
            &#125;
        &#125;
        return sum;
    &#125;
&#125;;</code></pre>

<blockquote>
<p>执行用时 :28 ms, 在所有 C++ 提交中击败了42.34% 的用户<br>内存消耗 :8.9 MB, 在所有 C++ 提交中击败了55.71%的用户</p>
</blockquote>
<hr>
<h3 id="29-13-罗马数字转整数-难度：简单"><a href="#29-13-罗马数字转整数-难度：简单" class="headerlink" title="29 13. 罗马数字转整数  难度：简单"></a>29 <a href="https://leetcode-cn.com/problems/reverse-integer/">13. 罗马数字转整数  </a>难度：简单</h3><p>跟本篇中的 <a href="#23-445-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0-ii-httpsleetcode-cncomproblemsadd-two-numbers-ii%E9%9A%BE%E5%BA%A6%E4%B8%AD%E7%AD%89">第23</a> 是同一题，这次没用栈，用的直接操作链表，头插法。两种对比着看。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">&#x2F;**
 * Definition for singly-linked list.
 * struct ListNode &#123;
 *     int val;
 *     ListNode *next;
 *     ListNode(int x) : val(x), next(NULL) &#123;&#125;
 * &#125;;
 *&#x2F;
class Solution &#123;
public:
    ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) &#123;
        ListNode* head &#x3D; new ListNode(0);
        ListNode* r &#x3D; head;
        int index &#x3D; 0;
        while(l1!&#x3D;NULL || l2!&#x3D;NULL || index!&#x3D;0)&#123;
            int sum &#x3D; 0;
            if(l1)&#123;
                sum+&#x3D;l1-&gt;val;
            &#125;
            if(l2)&#123;
                sum+&#x3D;l2-&gt;val;
            &#125;
            sum+&#x3D;index;
            ListNode* temp &#x3D; new ListNode(sum%10);
            index &#x3D; sum&#x2F;10;
            head-&gt;next &#x3D; temp;
            head &#x3D; temp;
            if(l1)&#123;
                l1 &#x3D; l1-&gt;next;
            &#125;
            if(l2)&#123;
                l2 &#x3D; l2-&gt;next;
            &#125;  
        &#125;
        return r-&gt;next;
    &#125;
&#125;;</code></pre>

<blockquote>
<p>执行用时 :32 ms, 在所有 C++ 提交中击败了50.51% 的用户<br>内存消耗 :9.3 MB, 在所有 C++ 提交中击败了100.00%的用户</p>
</blockquote>
<hr>
<h3 id="30-3-无重复字符的最长子串-难度：中等"><a href="#30-3-无重复字符的最长子串-难度：中等" class="headerlink" title="30 3. 无重复字符的最长子串  难度：中等"></a>30 <a href="https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/">3. 无重复字符的最长子串  </a>难度：中等</h3><p>滑动窗口经典题，本想用map记录位置，后面还是改成用set只记录值，然后设一个left指针记录起始位置。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">class Solution &#123;
public:
    int lengthOfLongestSubstring(string s) &#123;
        unordered_set&lt;char&gt; st;
        int maxlen &#x3D; 0;
        int left &#x3D; 0, i;
        for(i&#x3D;0;i&lt;s.size();i++)&#123;
            if(st.find(s[i]) &#x3D;&#x3D; st.end())&#123;
                st.insert(s[i]);
            &#125;
            else&#123;
                maxlen &#x3D; max(maxlen, i - left);
                while(s[left] !&#x3D; s[i])&#123;
                    st.erase(s[left]);
                    left++;
                &#125;
                left++;
            &#125;
        &#125;
        maxlen &#x3D; max(maxlen, i-left);
        return maxlen;
    &#125;
&#125;;</code></pre>

<blockquote>
<p>执行用时 :40 ms, 在所有 C++ 提交中击败了36.23% 的用户<br>内存消耗 :9.2 MB, 在所有 C++ 提交中击败了78.89%的用户</p>
</blockquote>
<hr>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>博客里的我，堂堂复活</title>
    <url>/2022/01/26/misc-1/</url>
    <content><![CDATA[<h2 id="0-一些废话"><a href="#0-一些废话" class="headerlink" title="0 一些废话"></a>0 一些废话</h2><p>由于旧的笔记本电脑实在太拉了（2016年4000块），2021年的双11我买了个新的笔记本电脑（Lenovo Legion R7000P2021H，3060），玩原神和用UE4都很流畅。</p>
<p>此外，为了保持好心情，首先我没在新电脑上装微信，然后没存任何关于学校科研的资料（只有为了投影存了半天的PPT）、或者配一个conda环境。因此，新电脑既流畅、又没有晦气的东西，简直是无敌了。</p>
<p>再加上2021年12月我在实验室有了一个说得过去的位置，就把台式机搬过去了，新笔记本成为我个人娱乐、学习的主要工具。</p>
<p>说到“个人”，那当然是个人博客（？。虽然很久没写博文了，但心里总想着有博客这件事情。之前，我的博客环境搭在老笔记本上，在台式机上写博文再拷到笔记本上上传，很别扭，去维护的动力又少了几分。放假前我把博客的工程拷了一份，前阵子终于找了个时间把它迁移到了新电脑上——过程比我想象的方便多了。</p>
<h2 id="1-迁移博客到新电脑"><a href="#1-迁移博客到新电脑" class="headerlink" title="1 迁移博客到新电脑"></a>1 迁移博客到新电脑</h2><ul>
<li><p>安装 NodeJS</p>
<ul>
<li>下载 Node.js LTS，<code>node --version</code> 查看是否成功</li>
<li>修改自带 npm 源 <code>npm config set registry https://registry.npm.taobao.org</code> </li>
<li>下载 yarn <code>npm install -g yarn</code></li>
</ul>
</li>
<li><p>安装Git</p>
<ul>
<li>我之前一直用 GitHub Desktop，所以事先就安好 Git 了，这其中需要进行一些设置<ul>
<li><code>git config --global user.name &quot;yourname&quot;</code> 、<code>git config --global user.email &quot;youremail&quot;</code> 、等等</li>
</ul>
</li>
<li>Git Bash 里生成这台电脑的 ssh<ul>
<li><code>ssh-keygen -t rsa -C &quot;youremail&quot;</code></li>
</ul>
</li>
<li>GitHub 网页中，添加这个 ssh 的 id_rsa.pub（公共密钥）<ul>
<li>图中是我两个笔记本的 ssh。老笔记本的 laptop 拼错了</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/1.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><p>安装 hexo</p>
<ul>
<li><code>npm install hexo-cli -g</code> 、<code>npm install hexo-deployer-git --save</code> 、等等</li>
<li>在 blog 文件夹中打开 Git Bash</li>
<li>hexo init 一下，生成了一个原始的博客项目，可以看看能不能用</li>
</ul>
</li>
<li><p>复制原工程的一部分到 blog 目录，替换原始项目</p>
<ul>
<li>scaffolds&#x2F;</li>
<li>source&#x2F;：写的博文啥的都在这里</li>
<li>themes&#x2F;：关于主题的一切</li>
<li>package.json：存储博客的包的依赖<ul>
<li>npm 安装包时的 <code>--save</code> 就是把这个包记录到 json 文件中</li>
</ul>
</li>
<li>_config.yml：博客的一些配置信息</li>
</ul>
</li>
<li><p>跑一下，看看问题</p>
<ul>
<li>由于我之前加装了 <a href="https://github.com/DIYgod/APlayer">APlayer</a>，以支持网易云音乐（<a href="https://acbgzm.github.io/about/">比如这里</a>），会报一个解析不出来相关语句的错</li>
<li><code>npm install hexo-tag-aplayer --save</code></li>
</ul>
</li>
</ul>
<p>综合下来，其实就报了这一个错。之前我还觉得可能要把 katex、流量监控啥的都再安一遍，但这些应该都已经集成在 yun 主题里了？总之过程很顺利，之后再换电脑按这个流程迁移博客即可。</p>
<h2 id="2-关于博客的思考"><a href="#2-关于博客的思考" class="headerlink" title="2 关于博客的思考"></a>2 关于博客的思考</h2><p>在整理老电脑文件的过程中，看到了很多值得怀念的东西。</p>
<h4 id="博客是好文明"><a href="#博客是好文明" class="headerlink" title="博客是好文明"></a>博客是好文明</h4><p>俗话说：好记性不如烂笔头。</p>
<p>之前我觉得博客无非就是做两件事情：</p>
<ul>
<li>以<strong>分享或整理</strong>为目的，写一些知识笔记或其他文章</li>
<li>有<strong>存储</strong>的意义，能留下自己制作的一些内容，或表达的一些观点</li>
</ul>
<p>博客本身是个比较可贵的、能保留点东西的地方。其他平台的环境不必细说。其中很让人不舒服的是，凡事没有法制标准。什么东西、什么时候消失都无法预料且让人不觉得奇怪。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/2.png" style="zoom: 67%;" / loading="lazy">

<p>我在B站也被下架了几个二创视频（是下架而不是未通过审核）。图中第一个是当时六小灵堂老师的一些诉求，导致下架了一批无关二创；第二个则是一直比较高压的题材（这算哪门子高压呢）。此外也有几个是懒得申诉直接删了的。在整理老电脑文件的时候找到了一部分，可能找个时间想想以怎样的形式补个档。</p>
<p>因此，建立一个博客，就算仅作为产出的内容留档的地方也是有必要的。博客目前放在 GitHub Pages 上，让人感到比较舒服。</p>
<p>再加上有些东西作为完全主观的思考，放在太公共的地方不合适，也会让别人觉得迷惑。如果去写私人日记就有点意义不明。也有人会在微博之类的地方去写日记，在公共平台上圈一个私有的地方，不想给别人看还要发出来。只能说不知道是什么贵物。</p>
<p>因此，这是我的个人博客，而非私人博客。如果有人碰巧看到我的某些文章，并对其中的一些内容感到有趣，那真是非常好的事情。也欢迎所有人随时跟我交流相关的话题。总之，我以“希望看到的人看得愉快”的心态来写博文。</p>
<h4 id="博客中的我"><a href="#博客中的我" class="headerlink" title="博客中的我"></a>博客中的我</h4><p>以上的两点，其实是博文写完后、作为作品发挥的作用。写博客的过程本身也是一件好事。</p>
<p>研一暑假过完，我就感到有点忙。毕业总得要点成果；实验室晦气的杂活越来越多、越来越没底线；最重要的是马上要正式开启打工生涯了，有很多东西要开始准备了。</p>
<p>简单说一下这段时间我做了什么：</p>
<ul>
<li>8月下 - 11月中：断断续续，从0开始把科研做了，包括一定程度上的找课题、读文献、创新和所有的代码。目前实验还不完善，论文还没写</li>
<li>10月上 - 12月中：参加了一个UE4客户端公开课，主要交了三次作业</li>
<li>12月下 - 1月下：实习的三轮技术面+笔试+hr面，虽然流程尚未结束、结果没有最终确定，但起码撑了很多轮（也会写一篇文章详谈）</li>
<li>这期间穿插着各种不给钱的晦气杂活。其实没推进毕业（科研）那条线的时候我都在做烂活。其中不乏当枪手这种，有违我高洁的品格（笑）的勾当</li>
</ul>
<p>我本身的工作强度其实不大，就是每天下午开始，到不固定的时间结束。但我一直是不太会并行的，就算是屁大一点事往往也会一直挂念着，并且在每件事的区间内（往往以周来算），很难再去开始做另一件事。这段时间，我无所事事的平静日子减少了，每天都感觉有一个“主线”需要一直考虑。在这种情况下，连看看新的动画片、玩玩新的游戏的劲头都没有了，更不用谈去年立下的目标“高强度的新内容获取”。在9-11月我还能打完《死亡搁浅》、《极限脱出999、善人死亡》，但12月我每天只剩下原神刷刷体力、看看lol直播这种很空虚的消遣了。听很多程序员说，随着进入工作，人总会慢慢变得无聊。这大概就是一个开始吧。</p>
<p><code>空虚[today] = (1 - 恢复率) * 空虚[today - 1] + today_new;</code>，这种疲惫和空虚也是递归的，跟我水杯里的水一样。12月下旬我同时面临两个ddl，一个是跟未来工作有关的UE4大作业，另一个是最让人不齿的烂活，那几天我不得不从早上就开始上班，以推进这个烂活。到了12月21日下午，突然就感觉自我的概念要消失了。</p>
<p>自我是什么呢？</p>
<ul>
<li><p>《炎拳》里的阿格尼是一直没有自我的。一开始，阿格尼认为自己是柴薪；后面村长告诉阿格尼，在别人认识自己的时候，自己才存在，因此阿格尼去满足别人的期待，成为领袖、成为神，作为别人眼中的炎拳；再然后，阿格尼为了露娜使用了炎拳，失去了之前自我的一切，转而为了露娜一个人而活；最终接受了那句最深刻的诅咒“活下去”（炎拳我也想找个机会多写写）</p>
</li>
<li><p>《黄金时代》里，王二逃到山里的时候也觉得自我的存在变得稀薄，村里人如果都当自己不存在，那自我是不是真的不存在了；但王二后面还是觉得自己自然是存在的，存在是个事实</p>
</li>
<li><p>（以上对于《炎拳》和《黄金时代》我的记忆和理解可能有偏差。此外我觉得《黄金时代》是真的无聊）</p>
</li>
<li><p>我想，他们都是主角，当然有自我。突然回想起关于《化物语》的一条评论，大意是“我不是战场原，也做不成垃圾君，我可能只是星空中的一颗星星吧，为他们祝福”。我同样不是主角，但应该也没有理由是星星吧，那我到底是什么呢？</p>
<ul>
<li>找到了原文，是<a href="https://music.163.com/mv/?id=5680044&userid=296981063">网易云 君の知らない物語 (TV size 89sec.2A)</a> 的评论：<em>星空下的告白，看了好几遍呢。我不是战场原，也做不了垃圾君，大概我只能成为星空里的一颗星星吧，默默为他们祝福，祝福</em></li>
</ul>
</li>
<li><p>自我的存在不需要被证明。只不过，花自己的精力、去做跟自己完全无关的烂活；或者一味向未来目标的努力，会慢慢让“自我”这个概念变得不清晰，产生“我几把谁呀”等疑惑。在这种时候，做出一些表达、产出，可能会让“自我”更有实感一些</p>
</li>
</ul>
<p>那阵子正好在整理老笔记本的文件，找到了几年前写的Web端全景漫游demo。因此我花了2h，在老电脑上将这demo调整了一下并加入到博客中（<a href="https://acbgzm.github.io/lab/">在这里，点击相应链接</a>）。由于我一直觉得，博客是自我表达、产出的一个途径，因此我确实为自己的存在画上了一笔。</p>
<p>我想，在漫长的打工生涯中（75能退休吗？），肯定经常遇到类似又忙又没价值的日子，让“自我”产生磨损。在这时，能让自己的存在产生实感的，应该就是做一些内容上的产出，作为“自我”的延申。剧本、视频、游戏要精雕细琢，音乐、绘画我一窍不通，但在博客写上几笔，永远是比较自由、廉价的方式。</p>
<p>当时我只想元旦放假爽玩，没想到，交完这两个ddl，又突然进入了面试流程、并一直挂念到今天这种展开。这就先不提了。但我相信一件事情：把这些事情记录下来，在很多年后，也能通过这些文字找回当时的自我。</p>
<h4 id="写给自己看"><a href="#写给自己看" class="headerlink" title="写给自己看"></a>写给自己看</h4><h5 id="Don’t-Be-So-Serious"><a href="#Don’t-Be-So-Serious" class="headerlink" title="Don’t Be So Serious"></a>Don’t Be So Serious</h5>
    <div id="aplayer-rnhxMoVp" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="472045266" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>


<p>俗话说：大的要来了。</p>
<p>不写的另一个原因是：总想写得好一点、全一点，来个大的。</p>
<ul>
<li>在学习的过程中去整理一个比较精美的笔记，这本身是很有意义的。之前的ML、DL笔记都写得很长也很全。</li>
<li>但到了其他的文章，就开始纠结了，总想尽可能写好点，不想让博文充斥太多的抱怨或者废话。简单来说就是不够随意。</li>
</ul>
<p>最开始的想法是：虽然没人看，但写给别人看。抱着给别人看的心态去写东西，就会多花点心思，以此来提高写出来的博文的<strong>质量</strong>。</p>
<p>去年尝试使用游戏引擎做游戏，也回想了之前做视频的一些感受。产生了一个想法：对于一个球员，应该区分其贡献、能力、历史地位；<strong>对于创作者，也应该把水平、创作力、流量分开。</strong> </p>
<ul>
<li><strong>水平</strong>：指创作者的硬实力，审美的能力、技术力等等<ul>
<li>表现为<strong>质量</strong></li>
</ul>
</li>
<li><strong>创作力</strong>（创作能力）：以现在的水平进行创作的能力<ul>
<li>首先要保证不是降低了水平来创作</li>
<li>在相同质量下，衡量为<strong>产量</strong>。当时我做视频，2017整年只做了10个左右，在技术稳定后可能一个月才做一个（其实很紧张，各种点子1周、音频2周、视频+P图+渲染压制修改等破事1周）</li>
<li>要说副业、兴趣什么的，其实都是借口。创作力是客观存在差距的，有的人就是点子多、做的快</li>
</ul>
</li>
<li><strong>流量</strong>：也就是播放量，这个是随缘的，往往自己最满意的作品流量反而不高。这点不提</li>
<li>核心性质：<u>创作力在发挥作用时，水平能最好地提升</u> <ul>
<li>一边学，一边要多用、多练，才能提高水平。对于学任何软件、语言、知识，其实都是这样的</li>
</ul>
</li>
</ul>
<p>要是想保持写博客的动力，其实就是质量和产量的trade-off。</p>
<p>我的初心是想提高自己的<strong>创作力</strong>，以努力成为内容创作者。就算是要提高水平，也应该在大量创作的过程中去提升。因此：</p>
<ul>
<li>对于知识博文，依然是<strong>增长质量</strong>，做得精美、做给别人看，因为是专业</li>
<li>对于其他文章，不应该着眼于质量上的拔高，而是要在现有水平上多写，<strong>增长创作力</strong>，因为是写着玩</li>
</ul>
<p>年纪不小了，眼高手低的毛病却越来越严重了。希望我不会趋近开口就是大道理、只能活在学术夸夸群里的形态。</p>
<h5 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h5><p>我在很早就放弃了靠做视频吃饭的想法了。但现在想想，对于非天才型选手，走上职业道路可以按这样的步骤做：</p>
<ul>
<li>每个人都有自己想要表达的东西</li>
<li>首先：在一定程度上放弃表达。追求<strong>流量</strong>、追赶热点（也是我最讨厌的），增长“本钱”（各种资源）<ul>
<li>不能完全偏离自己的表达目标，否则会吸引到完全不同的受众</li>
</ul>
</li>
<li>然后：追求<strong>创作力</strong>，在保证<strong>水平</strong>的前提下多更新，在此过程中提高<strong>水平</strong>，积累观众</li>
<li>最后：表达</li>
</ul>
<p>什么是天才呢？有的人就是学得快，水平高；有的人三天一更，何等的创作力；有的人的兴趣就是流量，天天跟着流量跑也不会觉得心累。</p>
<p>而对于兴趣为主的选手，会直接进行最后一步，自己做得高兴、表达自己就好了。</p>
<p>延申到未来打工生涯，其实也是一样的：</p>
<ul>
<li>找个大公司换米、学技术，积攒“本钱”</li>
<li>用“本钱”面对所有的可能性</li>
</ul>
]]></content>
      <categories>
        <category>misc</category>
      </categories>
      <tags>
        <tag>生活小记</tag>
        <tag>misc</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ STL快速上手</title>
    <url>/2020/04/15/stl-0/</url>
    <content><![CDATA[<p>最近在准备一个机试，就火速入门了C++ STL，来快速解决一些经典问题。在此做简单记录，目的是今后能用这篇文章快速捡起STL的最基本用法。笔记几乎全部来自《算法笔记》，题目来自codeup和pat。</p>
<span id="more"></span>
<hr>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p><a href="#1-vector">vector</a><br><a href="#2-set">set</a><br><a href="#3-string">string</a><br><a href="#4-map">map</a><br><a href="#5-queue">queue</a><br><a href="#6-priorityqueue">priority_queue</a><br><a href="#7-stack">stack</a><br><a href="#8-pair">pair</a><br><a href="#9-algorithm%E5%A4%B4%E6%96%87%E4%BB%B6%E4%B8%8B%E7%9A%84%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0">algorithm头文件下的函数</a></p>
<h2 id="1-vector"><a href="#1-vector" class="headerlink" title="1 vector"></a>1 vector</h2><p><strong>变长数组</strong></p>
<h4 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h4><pre class="language-C++" data-language="C++"><code class="language-C++">#include&lt;vector&gt;
using namespace std;
vector&lt;int&gt; name;
vector&lt;node&gt; name;	&#x2F;&#x2F; 节点数组
vector&lt;vector&lt;int&gt; &gt; name;	&#x2F;&#x2F;二维数组
vector&lt;int&gt; vi[100];	&#x2F;&#x2F;vector数组，长度100，每个vector变长</code></pre>

<h4 id="1-2-元素访问"><a href="#1-2-元素访问" class="headerlink" title="1.2 元素访问"></a>1.2 元素访问</h4><p>vector可以用下标和迭代器访问。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">vector&lt;int&gt; vname;
vname.push_back(1);
vname.push_back(1);
vname.push_back(1);
vname.push_back(1);

int x &#x3D; vname[0];	&#x2F;&#x2F; 下标访问

vector&lt;int&gt;::iterator it &#x3D; vname.begin();
int x &#x3D; *it;	&#x2F;&#x2F; 迭代器访问
int y &#x3D; *(it+3);</code></pre>

<p>在常用STL容器里，只有在vector和string中，允许使用 name.begin()+3 这种迭代器加整数的访问方式。</p>
<h4 id="1-3-常用函数"><a href="#1-3-常用函数" class="headerlink" title="1.3 常用函数"></a>1.3 常用函数</h4><ul>
<li>push_back(x)：容器尾插入元素</li>
<li>pop_back()：容器尾部移除元素</li>
<li>size()</li>
<li>clear()</li>
<li>insert(it, x)：在迭代器处插入元素</li>
<li>erase(it)：删除迭代器处元素</li>
<li>erase(first, last)：删除[first, last)元素</li>
</ul>
<h4 id="1-4-vector常见用途"><a href="#1-4-vector常见用途" class="headerlink" title="1.4 vector常见用途"></a>1.4 vector常见用途</h4><ul>
<li>代替数组</li>
<li>用邻接表存储图</li>
</ul>
<hr>
<h2 id="2-set"><a href="#2-set" class="headerlink" title="2 set"></a>2 set</h2><p><strong>自动排序、不含重复元素的容器</strong></p>
<h4 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h4><pre class="language-C++" data-language="C++"><code class="language-C++">#include&lt;set&gt;
using namespace std;
set&lt;int&gt; name;
set&lt;node&gt; name;
set&lt;int&gt; a[100];	&#x2F;&#x2F;100个set容器组成的数组</code></pre>

<h4 id="2-2-元素访问"><a href="#2-2-元素访问" class="headerlink" title="2.2 元素访问"></a>2.2 元素访问</h4><p>与vector不同，set只能用迭代器访问。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">set&lt;int&gt; name;
set.insert(3);

set&lt;int&gt;::iterator it &#x3D; name.begin();
int x &#x3D; *it;	&#x2F;&#x2F; 迭代器访问</code></pre>

<h4 id="2-3-常用函数"><a href="#2-3-常用函数" class="headerlink" title="2.3 常用函数"></a>2.3 常用函数</h4><ul>
<li>insert(x)：插入元素</li>
<li>find(value)：返回set中对应值为value的迭代器</li>
<li>size()</li>
<li>clear()</li>
<li>erase(it)：删除迭代器处元素</li>
<li>erase(value)：删除值为value的元素</li>
<li>erase(first, last)：删除[first, last)内所有元素</li>
</ul>
<h4 id="2-4-set常见用途"><a href="#2-4-set常见用途" class="headerlink" title="2.4 set常见用途"></a>2.4 set常见用途</h4><ul>
<li>自动去重并按升序排序</li>
</ul>
<hr>
<h2 id="3-string"><a href="#3-string" class="headerlink" title="3 string"></a>3 string</h2><p><strong>字符串</strong></p>
<h4 id="3-1-定义"><a href="#3-1-定义" class="headerlink" title="3.1 定义"></a>3.1 定义</h4><pre class="language-C++" data-language="C++"><code class="language-C++">#include&lt;set&gt;
using namespace std;
string str1;
string str2 &#x3D; &quot;abc&quot;;</code></pre>

<h4 id="3-2-元素访问"><a href="#3-2-元素访问" class="headerlink" title="3.2 元素访问"></a>3.2 元素访问</h4><p>string容器可以用下标或迭代器访问。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">string str &#x3D; &quot;abcd&quot;;
for(int i&#x3D;0;i&lt;str.length();i++)&#123;
	c &#x3D; str[i];	&#x2F;&#x2F; 下标访问
&#125;
for(string::iterator it &#x3D; str.begin();it!&#x3D;str.end();it++)&#123;
	c &#x3D; *it;	&#x2F;&#x2F;迭代器访问。一般用不到，但erase()和insert()要求以迭代器作为参数。
&#125;</code></pre>
<p>输出方式</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">&#x2F;&#x2F; 读入和输出整个字符串，只能用cin和cout
cin&gt;&gt;str;
cout&lt;&lt;str;
&#x2F;&#x2F; c_str()将字符串转换成字符数组，用于printf输出
printf(&quot;%s&quot;, str.c_str());</code></pre>
<h4 id="3-3-常用函数"><a href="#3-3-常用函数" class="headerlink" title="3.3 常用函数"></a>3.3 常用函数</h4><ul>
<li>加法运算符：+ 和 +&#x3D;</li>
<li>比较运算符：&#x3D;、!&#x3D;、&lt;、&gt;&#x3D;等。”aa”&lt;”aaa”，”b”&gt;”a”</li>
<li>size()或length()</li>
<li>clear()</li>
<li>insert(pos, string)：在pos位置插入string，pos是下标</li>
<li>insert(it, it2, it3)：把字符串[it2, it3)插入到it位置</li>
<li>erase(it)：删除迭代器处字符</li>
<li>erase(first, last)：删除[first, last)内所有字符</li>
<li>erase(pos, length)：删除pos开始length个字符</li>
<li>substr(pos, len)：返回从pos开始length长度的子串</li>
<li>find(str)：如果str是字符串的子串，返回其第一次出现的位置。如果不是子串，返回string::npos。</li>
<li>string::npos:作为find函数失配时的返回值</li>
<li>replace(pos, len, str2)：从pos位置开始len长度的子串替换为str2</li>
<li>replace(it1, it2, str2)：[it1, it2)子串替换为str2</li>
</ul>
<hr>
<h2 id="4-map"><a href="#4-map" class="headerlink" title="4 map"></a>4 map</h2><p><strong>映射</strong></p>
<h4 id="4-1-定义"><a href="#4-1-定义" class="headerlink" title="4.1 定义"></a>4.1 定义</h4><pre class="language-C++" data-language="C++"><code class="language-C++">#include&lt;map&gt;
using namespace std;
map&lt;string, int&gt; mp;	&#x2F;&#x2F; string到int的映射
map&lt;set&lt;int&gt;, string&gt; mp;	&#x2F;&#x2F; set到string的映射</code></pre>

<h4 id="4-2-元素访问"><a href="#4-2-元素访问" class="headerlink" title="4.2 元素访问"></a>4.2 元素访问</h4><p>用下标访问map，可以参考数组（int到int的映射）的访问的格式。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">map&lt;char, int&gt; mp;
mp[&#39;c&#39;] &#x3D; 20;
c &#x3D; mp[&#39;c&#39;];	&#x2F;&#x2F; 用下标访问</code></pre>

<p>用迭代器访问map，通过一个it访问键和值两个元素。it-&gt;first访问键，it-&gt;second访问值。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">map&lt;char, int&gt; mp;
mp[&#39;b&#39;] &#x3D; 120;
mp[&#39;a&#39;] &#x3D; 100;
mp[&#39;c&#39;] &#x3D; 140;
for(map&lt;char, int&gt;::iterator it &#x3D; mp.begin(); it!&#x3D; mp.end(); it++)&#123;
	printf(&quot;%c %d\n&quot;,it-&gt;first, it-&gt;second);
&#125;</code></pre>
<p>输出结果：</p>
<blockquote>
<p>a 100<br>b 120<br>c 140<br>map内部会以键值从小到大自动排序。map和set内部都是使用红黑树实现的。</p>
</blockquote>
<h4 id="4-3-常用函数"><a href="#4-3-常用函数" class="headerlink" title="4.3 常用函数"></a>4.3 常用函数</h4><ul>
<li>find(key)：返回键为key的迭代器 <code>mp.find(&#39;a&#39;)</code></li>
<li>查找：<code>if(mp.find(key)!=mp.end())</code></li>
<li>size()</li>
<li>clear()</li>
<li>erase(it)：删除迭代器处元素</li>
<li>erase(key)：删除值key为键的元素</li>
<li>erase(first, last)：删除[first, last)内所有元素</li>
</ul>
<h4 id="4-4-map常见用途"><a href="#4-4-map常见用途" class="headerlink" title="4.4 map常见用途"></a>4.4 map常见用途</h4><ul>
<li>需要建立映射的场合</li>
<li>判断大整数或其他类型数据是否存在，可以把map当成bool数组使用</li>
</ul>
<hr>
<h2 id="5-queue"><a href="#5-queue" class="headerlink" title="5 queue"></a>5 queue</h2><p><strong>队列</strong></p>
<h4 id="5-1-定义"><a href="#5-1-定义" class="headerlink" title="5.1 定义"></a>5.1 定义</h4><pre class="language-C++" data-language="C++"><code class="language-C++">#include&lt;queue&gt;
using namespace std;
queue&lt;int&gt; name;</code></pre>

<h4 id="5-2-元素访问"><a href="#5-2-元素访问" class="headerlink" title="5.2 元素访问"></a>5.2 元素访问</h4><p>只能通过front()来访问队首元素，或者back()访问队尾元素。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">queue&lt;int&gt; q;
q.push(1);
q.push(2);
q.push(3);
printf(&quot;%d %d&quot;, q.front(), q.back());</code></pre>
<p>输出结果</p>
<blockquote>
<p>1 3</p>
</blockquote>
<p>比较好记：谁先来排队，谁就站在队头。</p>
<h4 id="5-3-常用函数"><a href="#5-3-常用函数" class="headerlink" title="5.3 常用函数"></a>5.3 常用函数</h4><ul>
<li>push(x)：x入队</li>
<li>front(), back()：返回队首和队尾元素</li>
<li>pop()：队首元素出队</li>
<li>empty()：检测是否为空</li>
<li>size()</li>
</ul>
<h4 id="5-4-queue常见用途"><a href="#5-4-queue常见用途" class="headerlink" title="5.4 queue常见用途"></a>5.4 queue常见用途</h4><ul>
<li>广度优先搜索</li>
<li>注意：使用front()和pop()函数前，先用empty()判断非空。</li>
</ul>
<hr>
<h2 id="6-priority-queue"><a href="#6-priority-queue" class="headerlink" title="6 priority_queue"></a>6 priority_queue</h2><p><strong>内部按照优先级排序的队列</strong></p>
<h4 id="6-1-定义"><a href="#6-1-定义" class="headerlink" title="6.1 定义"></a>6.1 定义</h4><pre class="language-C++" data-language="C++"><code class="language-C++">#include&lt;queue&gt;
using namespace std;
priority_queue&lt;typename&gt; name;</code></pre>

<h4 id="6-2-元素访问"><a href="#6-2-元素访问" class="headerlink" title="6.2 元素访问"></a>6.2 元素访问</h4><p>优先队列没有front()函数和back()函数，只能通过top()函数访问队首元素（也可以称为堆顶元素），也就是优先级最高的元素。</p>
<p><code>printf(&quot;%d&quot;, name.top());</code></p>
<h4 id="6-3-常用函数"><a href="#6-3-常用函数" class="headerlink" title="6.3 常用函数"></a>6.3 常用函数</h4><ul>
<li><code>name.push(x);</code>将x入队，复杂度O(logN)</li>
<li><code>name.pop();</code>队首元素出队，复杂度O(logN)</li>
<li><code>typename x = name.top();</code>获得队首元素，复杂度O(1)</li>
<li><code>name.empty();</code></li>
<li><code>name.size()</code></li>
</ul>
<h4 id="6-4-元素优先级设置"><a href="#6-4-元素优先级设置" class="headerlink" title="6.4 元素优先级设置"></a>6.4 元素优先级设置</h4><h5 id="6-4-1-基本数据类型的优先级设置"><a href="#6-4-1-基本数据类型的优先级设置" class="headerlink" title="6.4.1 基本数据类型的优先级设置"></a>6.4.1 基本数据类型的优先级设置</h5><p>对于int型、double型、char型，优先队列的默认设置是数字越大优先级越高。</p>
<p>也可以在定义过程中设置优先队列的优先级:</p>
<p><code>priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt; &gt; pq;</code></p>
<p>其中vector&lt;int&gt;填写的是来承载底层数据结构heap的容器；第三个参数greater&lt;int&gt;则是对第一个参数的比较类，<strong>less&lt;int&gt;表示数字大的优先级大，greater&lt;int&gt;表示数字小的优先级大。</strong></p>
<h5 id="6-4-2-结构体的优先级设置"><a href="#6-4-2-结构体的优先级设置" class="headerlink" title="6.4.2 结构体的优先级设置"></a>6.4.2 结构体的优先级设置</h5><pre class="language-C++" data-language="C++"><code class="language-C++">struct Student&#123;
    string name;
    int score;
    friend bool operator &lt; (Student s1, Student s2)&#123;
        return s1.score &lt; s2.score;
    &#125;
&#125;;</code></pre>

<p>定义友元函数，重载“&lt;”运算符（<strong>重载大于号会编译错误，其他符号可以由小于号代替，即s1 &gt; s2等价于判断s2 &lt; s1，s1 &#x3D;&#x3D; s2等价于判断!(s1 &lt; s2)&amp;&amp;!(s2 &lt; s1)</strong>)。</p>
<p>函数内部“return s1.score &lt; s2.score”，重载后小于号还是小于号的作用。此时Student类型的优先队列，内部是分数高的学生优先级高。</p>
<p><code>priority_queue&lt;Student&gt; pq;</code></p>
<p>同理，如果想要低分优先级高，把return中的小于号改为大于号即可。</p>
<h5 id="6-4-3-与sort-的比较"><a href="#6-4-3-与sort-的比较" class="headerlink" title="6.4.3 与sort()的比较"></a>6.4.3 与sort()的比较</h5><p>优先队列的重载和排序函数类似，都是由两个变量作为参数，return了true或false作为大小关系。</p>
<p>效果上，大小规则是相反的。在sort()中，如果return s1.score&gt;s2.score，是按照分数由大到小排列；在优先队列中，则是把小的分数元素放到队首。原因在于：优先队列默认规则是大的数优先级高，重载小于号为相反（s1.score&gt;s2.score），只是把默认的规则反向了一下。</p>
<p>有没有办法跟sort()中的cmp函数那样写在结构体外面呢?</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">struct Student&#123;
    string name;
    int score;
&#125;;
struct cmp&#123;
    bool operator () (Student s1, Student s2)&#123;
        return s1.score &gt; s2.score;
    &#125;
&#125;;</code></pre>

<p>在这种情况下，使用第二种定义方法定义优先队列：</p>
<p><code>priority_queue&lt;Student, vector&lt;Student&gt;, cmp&gt; pq;</code></p>
<p>即便是基本数据类型或者其他STL容器（如set），也可以通过以上方法定义优先级。</p>
<h5 id="6-4-4-引用"><a href="#6-4-4-引用" class="headerlink" title="6.4.4 引用"></a>6.4.4 引用</h5><p>最后，如果结构体内的数据较为庞大（例如出现了数组、字符串），可以使用引用来提高效率。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">struct Student&#123;
    string name;
    int score;
    friend bool operator &lt; (const Student &amp;s1, const Student &amp;s2)&#123;
        return s1.score &gt; s2.score;
    &#125;
&#125;;
priority_queue&lt;Student&gt; pq;</code></pre>

<p>或</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">struct Student&#123;
    string name;
    int score;
&#125;;
struct cmp&#123;
    bool operator () (const Student &amp;s1, const Student &amp;s2)&#123;
        return s1.score &gt; s2.score;
    &#125;
&#125;;
priority_queue&lt;Student, vector&lt;Student&gt;, cmp&gt; pq;</code></pre>

<h4 id="6-4-5-priority-queue常见用途"><a href="#6-4-5-priority-queue常见用途" class="headerlink" title="6.4.5 priority_queue常见用途"></a>6.4.5 priority_queue常见用途</h4><ul>
<li>解决一些贪心问题</li>
<li>对Dijkstra算法进行优化（优先队列本质是堆）。</li>
<li>注意：使用top()前必须用empty()判断队列非空。</li>
</ul>
<hr>
<h2 id="7-stack"><a href="#7-stack" class="headerlink" title="7 stack"></a>7 stack</h2><p><strong>后进先出的STL容器</strong></p>
<h4 id="7-1-定义"><a href="#7-1-定义" class="headerlink" title="7.1 定义"></a>7.1 定义</h4><pre class="language-C++" data-language="C++"><code class="language-C++">#include&lt;stack&gt;
using namespace std;
stack&lt;int&gt; name;</code></pre>

<h4 id="7-2-元素访问"><a href="#7-2-元素访问" class="headerlink" title="7.2 元素访问"></a>7.2 元素访问</h4><p>栈只能通过top()函数访问栈顶元素。</p>
<p><code>printf(&quot;%d&quot;, name.top());</code></p>
<h4 id="7-3-常用函数"><a href="#7-3-常用函数" class="headerlink" title="7.3 常用函数"></a>7.3 常用函数</h4><ul>
<li><code>name.push(x);</code>将x入栈，复杂度O(1)</li>
<li><code>name.pop();</code>栈顶元素出栈，复杂度O(1)</li>
<li><code>typename x = name.top();</code>获得队首元素，复杂度O(1)</li>
<li><code>name.empty();</code></li>
<li><code>name.size()</code></li>
</ul>
<h4 id="7-4-stack常见用途"><a href="#7-4-stack常见用途" class="headerlink" title="7.4 stack常见用途"></a>7.4 stack常见用途</h4><ul>
<li>解决递归问题</li>
</ul>
<h4 id="7-5-练习-括号匹配"><a href="#7-5-练习-括号匹配" class="headerlink" title="7.5 练习-括号匹配"></a>7.5 练习-<a href="http://codeup.cn/problem.php?cid=100000602&pid=1">括号匹配</a></h4><pre class="language-C++" data-language="C++"><code class="language-C++">#include&lt;cstdio&gt;
#include&lt;iostream&gt;
#include&lt;stack&gt;
#include&lt;string&gt;
using namespace std;

int main()&#123;
	int n;
	scanf(&quot;%d&quot;,&amp;n);
	getchar();
	string s;
	stack&lt;char&gt; st;
	while(n--)&#123;
		getline(cin,s);
		int len&#x3D;s.size();
		bool flag&#x3D;true;
		for(int i&#x3D;0;i&lt;len;i++)&#123;
			if(s[i]&#x3D;&#x3D;&#39;(&#39; || s[i]&#x3D;&#x3D;&#39;[&#39; || s[i]&#x3D;&#x3D;&#39;&#123;&#39;)&#123;
				st.push(s[i]);
			&#125;
			else if(s[i]&#x3D;&#x3D;&#39;)&#39; || s[i]&#x3D;&#x3D;&#39;]&#39; || s[i]&#x3D;&#x3D;&#39;&#125;&#39;)&#123;
				if(st.empty()&#x3D;&#x3D;true)&#123;
					flag&#x3D;false;	&#x2F;&#x2F;单独的后括号 
					break;
				&#125;
				char c &#x3D; st.top();	&#x2F;&#x2F;pop()是void类型，需要用top()返回栈顶元素 
				st.pop();
				if(c&#x3D;&#x3D;&#39;[&#39;&amp;&amp;s[i]&#x3D;&#x3D;&#39;]&#39; || c&#x3D;&#x3D;&#39;(&#39;&amp;&amp;s[i]&#x3D;&#x3D;&#39;)&#39; || c&#x3D;&#x3D;&#39;&#123;&#39;&amp;&amp;s[i]&#x3D;&#x3D;&#39;&#125;&#39;)&#123;
					continue;
				&#125;
				else&#123;
					flag&#x3D;false;	&#x2F;&#x2F;不匹配 
					break;
				&#125;
			&#125;
		&#125;
		if(flag&#x3D;&#x3D;true&amp;&amp;st.empty()&#x3D;&#x3D;true)&#123;
			printf(&quot;yes\n&quot;);
		&#125;
		else&#123;
			printf(&quot;no\n&quot;);
		&#125;
		while(st.empty()!&#x3D;true)&#123;
			st.pop();
		&#125;
		
	&#125;
	return 0;
&#125;
&#x2F;*
有一种更好的方法：
使用map创建char到int的对应关系，
通过两个括号对应的int相加是否等于某一定值，
判断两个括号是否匹配。
如：
map&lt;char&gt; list;
list中依次存储：&#123; 1, ( 2, [ 3, ] 4, ) 5, &#125; 6
判断相加是否为7
*&#x2F;</code></pre>

<hr>
<h2 id="8-pair"><a href="#8-pair" class="headerlink" title="8 pair"></a>8 pair</h2><p><strong>可以看作一个内部有两个元素的结构体</strong></p>
<h4 id="8-1-定义"><a href="#8-1-定义" class="headerlink" title="8.1 定义"></a>8.1 定义</h4><pre class="language-C++" data-language="C++"><code class="language-C++">#include&lt;utility&gt;
using namespace std;
pair&lt;typename1, typename2&gt; name;</code></pre>
<p>由于map内部实现涉及pair，因此添加map头文件后会自动添加utility头文件。</p>
<p>在定义时初始化，如：<br><code>pair&lt;string, int&gt; p(&quot;str&quot;, 100);</code></p>
<p>在代码中临时构建一个pair，有以下两种方法：</p>
<ul>
<li><code>p = pair&lt;string, int&gt;(&quot;str&quot;, 100)</code></li>
<li><code>p = make_pair(&quot;str&quot;, 100)</code></li>
</ul>
<h4 id="8-2-元素访问"><a href="#8-2-元素访问" class="headerlink" title="8.2 元素访问"></a>8.2 元素访问</h4><p>pair中只有两个元素，分别是first和second。按正常结构体的方式访问即可。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">pair&lt;string, int&gt; p;
p.first &#x3D; &quot;this is a pair&quot;;
p.second &#x3D;  10;

p &#x3D; make_pair(&quot;this is a pair&quot;, 10);

p &#x3D; pair&lt;string, int&gt;(&quot;this is a pair&quot;, 10);

cout&lt;&lt;p.first&lt;&lt;&#39; &#39;&lt;&lt;p.second&lt;&lt;endl;</code></pre>

<h4 id="8-3-常用函数"><a href="#8-3-常用函数" class="headerlink" title="8.3 常用函数"></a>8.3 常用函数</h4><p>  比较操作数。先比较first大小，如果相同再比较second大小。<br>  <pre class="language-C++" data-language="C++"><code class="language-C++">p1 &#x3D; pair&lt;int, int&gt;(1,3);
p2 &#x3D; pair&lt;int, int&gt;(1,2);
if(p1 &gt;&#x3D; p2)&#123;&#125;
else if(p1 &lt; p2)&#123;&#125;</code></pre></p>
<h4 id="8-4-pair常见用途"><a href="#8-4-pair常见用途" class="headerlink" title="8.4 pair常见用途"></a>8.4 pair常见用途</h4><ul>
<li>用来代替二元结构体。不用自己写构造函数了。</li>
<li>作为<strong>map的键值对</strong>进行插入、迭代等操作。如：<pre class="language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;
#include&lt;string&gt;
#include&lt;map&gt;
using namespace std;
int main()&#123;
	map&lt;string, int&gt; mp;
	mp.insert(make_pair(&quot;this is a pair&quot;, 100));
	mp.insert(pair&lt;string, int&gt;(&quot;another pair&quot;, 200));
	for(map&lt;string, int&gt;::iterator it &#x3D; mp.begin(); it !&#x3D; mp.end(); it++)&#123;
		cout &lt;&lt; it-&gt;first &lt;&lt; &#39; &#39; &lt;&lt; it-&gt;second &lt;&lt; endl;
	&#125;
&#125;</code></pre></li>
<li>注意，用迭代器操作是it-&gt;first，用实体操作是p.first。可以把迭代器想成指针。</li>
</ul>
<hr>
<h2 id="9-algorithm头文件下的常用函数"><a href="#9-algorithm头文件下的常用函数" class="headerlink" title="9 algorithm头文件下的常用函数"></a>9 algorithm头文件下的常用函数</h2><p><strong>需要添加using namespace std;</strong></p>
<h4 id="9-1-max-、min-、abs"><a href="#9-1-max-、min-、abs" class="headerlink" title="9.1 max()、min()、abs()"></a>9.1 max()、min()、abs()</h4><p>max(x, y)和min(x, y)</p>
<p>x和y可以是浮点数，但类型必须相同。</p>
<p>abs(x)，x为整形。浮点数的绝对值使用cmath头文件下的fabs()函数。</p>
<h4 id="9-2-swap"><a href="#9-2-swap" class="headerlink" title="9.2 swap()"></a>9.2 swap()</h4><p>swap(x, y)，交换x和y的值。</p>
<h4 id="9-3-reverse"><a href="#9-3-reverse" class="headerlink" title="9.3 reverse()"></a>9.3 reverse()</h4><p>reverse(it1, it2)可以将<strong>数组指针在[it1, it2)之间的元素</strong>或<strong>容器的迭代器在[it1, it2)范围内的元素</strong>进行反转。</p>
<p>例一：</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">int a[10] &#x3D; &#123; 1, 2, 3, 4, 5, 6 &#125;;
reverse(a, a+4);
for(int i&#x3D;0;i&lt;6;i++) printf(&quot;%d &quot;,a[i]);</code></pre>
<p>输出结果：</p>
<blockquote>
<p>4 3 2 1 5 6</p>
</blockquote>
<p>例二：</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">string str &#x3D; &quot;abcdefghi&quot;;
reverse(str.begin()+2, str.begin()+6);
printf(&quot;%s&quot;,str.c_str());</code></pre>
<p>输出结果：</p>
<blockquote>
<p>abfedcghi</p>
</blockquote>
<h4 id="9-4-next-permutation"><a href="#9-4-next-permutation" class="headerlink" title="9.4 next_permutation()"></a>9.4 next_permutation()</h4><p>next_permutation()给出一个序列在全排列中的下一个序列。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">int a[3] &#x3D; &#123;1, 2, 3&#125;;
do&#123;
	printf(&quot;%d %d %d\n&quot;, a[0], a[1], a[2]);
&#125;while(next_permutation(a, a+3));</code></pre>
<p>输出结果：</p>
<blockquote>
<p>1 2 3<br>1 3 2<br>2 1 3<br>2 3 1<br>3 1 2<br>3 2 1</p>
</blockquote>
<h4 id="9-5-fill"><a href="#9-5-fill" class="headerlink" title="9.5 fill()"></a>9.5 fill()</h4><p>fill()可以把数组或容器中的某一段区间赋某个相同的值。和memset不同，这里的赋值可以是数组类型对应范围中的任意值。</p>
<p>参数写法依然是左闭右开。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">int a[10] &#x3D; &#123; 1, 2, 3, 4, 5, 6 &#125;;
fill(a, a+4, 100);
for(int i&#x3D;0;i&lt;6;i++) printf(&quot;%d &quot;,a[i]);</code></pre>
<p>输出结果：</p>
<blockquote>
<p>100 100 100 100 5 6</p>
</blockquote>
<h4 id="9-6-sort"><a href="#9-6-sort" class="headerlink" title="9.6 sort()"></a>9.6 sort()</h4><p>sort(首元素地址, 尾元素的下一个地址, 比较函数(缺省则默认递增序))</p>
<p>sort()用的比较多，直接贴一下结构体排序的代码吧。</p>
<h3 id="PAT-A1025"><a href="#PAT-A1025" class="headerlink" title="PAT_A1025  "></a><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805474338127872">PAT_A1025  </a></h3><pre class="language-C++" data-language="C++"><code class="language-C++">#include&lt;cstdio&gt;
#include&lt;cstring&gt;
#include&lt;algorithm&gt;
using namespace std;
struct grade&#123;
	char num[20];
	int score;
	int final_rank;
	int location_number;
	int local_rank;
	grade()&#123;
	&#125;
	grade(char _num[],int _score)&#123;
		strcpy(num,_num);
		score&#x3D;_score;
	&#125;
&#125;;
bool cmp(grade g1,grade g2)&#123;
	if(g1.score!&#x3D;g2.score) return g1.score&gt;g2.score;
	else return strcmp(g1.num,g2.num)&lt;0;
&#125;
int main()&#123;
	int n,m;
	int i,j;
	grade stu[30010];
	char num[20];
	int score;
	int count&#x3D;0;
	scanf(&quot;%d&quot;,&amp;n);
	for(i&#x3D;1;i&lt;&#x3D;n;i++)&#123;
		scanf(&quot;%d&quot;,&amp;m);
		int temp&#x3D;count;
		for(j&#x3D;0;j&lt;m;j++)&#123;
			scanf(&quot;%s%d&quot;,num,&amp;score);
			stu[count]&#x3D;grade(num,score);
			stu[count].location_number&#x3D;i;
			count++;	
		&#125;
		sort(stu+temp,stu+count,cmp);
		stu[0+temp].local_rank&#x3D;1;
		for(j&#x3D;1;j&lt;m;j++)&#123;
			if(stu[j+temp].score&lt;stu[j+temp-1].score)&#123;
				stu[j+temp].local_rank&#x3D;j+1;
			&#125;
			else&#123;
				stu[j+temp].local_rank&#x3D;stu[j+temp-1].local_rank;
			&#125;
		&#125;
	&#125;
	
	sort(stu,stu+count,cmp);
	stu[0].final_rank&#x3D;1;
	for(j&#x3D;1;j&lt;count;j++)&#123;
		if(stu[j].score&lt;stu[j-1].score)&#123;
			stu[j].final_rank&#x3D;j+1;
		&#125;
		else&#123;
			stu[j].final_rank&#x3D;stu[j-1].final_rank;
		&#125;
	&#125;
	
	printf(&quot;%d\n&quot;,count);
	for(i&#x3D;0;i&lt;count;i++)
		printf(&quot;%s %d %d %d\n&quot;,stu[i].num,stu[i].final_rank,stu[i].location_number,stu[i].local_rank);
	return 0;
&#125;</code></pre>

<h4 id="9-7-lower-bound-和upper-bound"><a href="#9-7-lower-bound-和upper-bound" class="headerlink" title="9.7 lower_bound()和upper_bound()"></a>9.7 lower_bound()和upper_bound()</h4><p>两个函数均用在<strong>有序</strong>数组或容器中。</p>
<p>lower_bound(first, last, val)用来寻找在数组或容器的[first, last)范围内第一个值<strong>大于等于</strong>val的元素的位置，返回指针或迭代器。</p>
<p>upper_bound(first, last, val)找的是第一个值<strong>大于</strong>val的元素的位置。</p>
<p>如果查不到则返回可以插入该元素的位置。</p>
<pre class="language-C++" data-language="C++"><code class="language-C++">int a[10] &#x3D; &#123;1, 2, 3, 3, 3, 5, 5, 5, 5, 5&#125;;
&#x2F;&#x2F; -1
int* lowerPos &#x3D; lower_bound(a, a+10, -1);
int* upperPos &#x3D; upper_bound(a, a+10, -1);
printf(&quot;%d %d\n&quot;, lowerPos - a, upperPos - a);
&#x2F;&#x2F; 1
lowerPos &#x3D; lower_bound(a, a+10, 1);
upperPos &#x3D; upper_bound(a, a+10, 1);
printf(&quot;%d %d\n&quot;, lowerPos - a, upperPos - a);
&#x2F;&#x2F; 3
lowerPos &#x3D; lower_bound(a, a+10, 3);
upperPos &#x3D; upper_bound(a, a+10, 3);
printf(&quot;%d %d\n&quot;, lowerPos - a, upperPos - a);
&#x2F;&#x2F; 4
lowerPos &#x3D; lower_bound(a, a+10, 4);
upperPos &#x3D; upper_bound(a, a+10, 4);
printf(&quot;%d %d\n&quot;, lowerPos - a, upperPos - a);
&#x2F;&#x2F; 6
lowerPos &#x3D; lower_bound(a, a+10, 6);
upperPos &#x3D; upper_bound(a, a+10, 6);
printf(&quot;%d %d\n&quot;, lowerPos - a, upperPos - a);</code></pre>
<p>输出结果：</p>
<blockquote>
<p>0 0<br>0 1<br>2 5<br>5 5<br>10 10</p>
</blockquote>
<hr>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>什么都没做的2020</title>
    <url>/2021/02/14/summary-2020/</url>
    <content><![CDATA[<h3 id="part-1-陷入漫长假期的我与2020"><a href="#part-1-陷入漫长假期的我与2020" class="headerlink" title="part 1 陷入漫长假期的我与2020"></a>part 1 陷入漫长假期的我与2020</h3><p>这篇文章在刚回家，也就是1月17日左右就建档了。然而在2月14日才正式写完，原因除了在家无所事事 <del>沉迷游戏</del> ，更重要的是……</p>
<p><strong>仔细回想整个2020，我不是什么事情都没做吗（震声）</strong></p>
<ul>
<li>专业上，学了一些东西，但都是为了应付各种考试，几乎没有凭兴趣而坚持学习了什么。实在感到2020年自己在专业上没什么长进。</li>
<li>试着坚持做一些事情，如阅读，跑步，画画，弹琴。最后都无疾而终了。</li>
<li>依然没什么产出。19年还有备考这个借口，但20年好像习惯了这种懒散的生活。</li>
</ul>
<p>有时候真的感到焦虑：我是不是失去了认真做一件事、坚持下去的能力了？T T</p>
<hr>
<p>所以作为2020年的总结，就仅仅简单列个流水账，记录一下经历的事情吧。很惭愧，其他真的没做什么了。</p>
<ul>
<li>1月：考完试后放松了一阵子。其实听到了关于传染病的一些小道消息，元旦没有出去玩。并且出于类似考虑，7号就早早离开武汉了。回想起来，这点还是做的不错的，之后遇到类似的情况也要警觉一些，宁可信其有。</li>
<li>后面疫情刚爆发，记得是1月22号，我正好发烧了。去发热门诊，被关到隔离房间里检查了5个小时左右。见识了疫情初期医院的混乱：一方面短时间内没有制定出很好的管理方法，我被踢了几次皮球；另一方面疫情初期，大家对病毒可以说一无所知，当时连核酸检测是什么都不知道，医院连心电图都给我用了。现在回想起来也是比较有趣的经历。</li>
<li>2月：奔波过年，疫情彻底爆发。</li>
<li>3月-4月：全程在家。应该是在玩游戏+做毕设+准备复试吧。</li>
<li>5月：复试+毕设答辩。24号之后就突然轻松下来了。老家的爷爷去世。</li>
<li>6月：趁618装机。玩。联系到了老师，偶尔看看论文。返校收拾东西，跟武汉说再见。</li>
<li>7月-8月：玩+有一搭没一搭地学。跟家人爬了下山。</li>
<li>9月：入学，进入新环境。</li>
<li>10月-12月：在学校，主要在各种课程之间奔走，被分割的时间没有好好利用起来，基本上是在混日子。</li>
</ul>
<p>希望2021能按做的事情来介绍，而不是按时间写流水账了~~</p>
<hr>
<p>另外，我想记录一下今年的人际关系。众所周知，我是线下唯唯诺诺，线上也搞自闭的超级别扭人。跟人很难保持联系。</p>
<p>今年我真的努力做到（起码在我的视角里）真诚、大方地待人了，我感到自己进步了一些。</p>
<p>在新环境中，遇到了非常好的人，他们会把我拉到交际圈，让我有机会建立跟别人的联系。也有老同学联系我，找我聊天。真心地感谢。🙇‍</p>
<p>但我还是有点畏惧热热闹闹的场合。今后要努力克服。</p>
<h3 id="part-2-做一些跟内容创作有关的事情"><a href="#part-2-做一些跟内容创作有关的事情" class="headerlink" title="part 2 做一些跟内容创作有关的事情"></a>part 2 做一些跟内容创作有关的事情</h3><p>2020年，世界不太平，我也进入了人生的新阶段。在家呆了九个月的我，有充足的时间思考：我到底要做什么？</p>
<p>就像前面提到的，2020年我在知识上并没有做成什么。唯一比较满意的，就是我这一年欣赏了不少好作品，给生活增添了不少乐趣。</p>
<p>一方面可能是我的兴趣从二创，或者vtuber等直播，重新回到作品本身上；另一方面就是我意识到，玩结束不了的网游、看结束不了的直播容易感到更加空虚。</p>
<p>在此列出2020年我非常喜欢的作品。完整的目录我在bangumi上记录了，以下只列比较喜欢的。</p>
<ul>
<li>漫画<ul>
<li>三日间的幸福（年度最佳）</li>
<li>总之就是非常可爱</li>
</ul>
</li>
<li>轻小说（没看新的，只看了之前追的）<ul>
<li>四叠半神话大系（年度最佳。不是轻小说，但姑且放在这里）</li>
<li>弱角友崎</li>
<li>实力教室（11卷以后相当有趣啊）</li>
</ul>
</li>
<li>动画<ul>
<li>攻壳机动队（补的。但是是年度最佳）</li>
<li>辉夜大小姐第二季</li>
<li>魔女之旅</li>
</ul>
</li>
<li>游戏（大部分都挺喜欢的。不喜欢就直接不买了。都是几年前的，游戏我还没到追着玩的高度）。只列一下剧情为主的，糖豆人啥的就不列了。<ul>
<li>GTA5（没错，就是我2020年才买GTA5）（年度最佳）</li>
<li>AI: The Somnium Files</li>
<li>弹丸论破V3</li>
<li>还玩了些著名的独立游戏，真心都很不错：<ul>
<li>What Remains of Edith Finch</li>
<li>Finding Paradise（年度最佳）</li>
<li>Gris</li>
<li>Otaku’s Adventure&#x2F;宅男的人生冒险</li>
<li>Unheard&#x2F;疑案追声</li>
</ul>
</li>
<li>手游：<ul>
<li>明日方舟，有半年没玩吧，后面又捡回来了</li>
<li>公主连结re:dive，4月开服就玩了，公会战大家一起讨论很有意思（虽然冲600至今没成功）</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>一整年确实从虚拟世界中获得了不少快乐。</p>
<p>现在想想，大学最高兴的时候，就是大二那阵子投稿视频的时候。在课程之余p图、混音、剪辑，过着既充实又新鲜的生活，并且有幸获得了几十万的播放量。现在我早已对二创失去了兴趣，但偶尔还会打开视频，看看弹幕，感到无与伦比的成就感。</p>
<p>我一方面觉得创作内容很有追逐梦想的感觉，另一方面觉得能用技术帮助到内容的创作也是很有价值的事情。</p>
<p>虽然做不成天才，创作传世之作，或者带来技术突破；但如果能制作出让别人感到快乐、或者用来表达自己的东西，或者用技术帮助到优秀作品的创作，我认为都是非常好的事情。</p>
<p>于是我一下子就决定了：我要做跟内容创作有关的事情。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/summary-2020/SHIROBAKO.jpg" width="100%" height="100%" loading="lazy">



<h3 id="part-3-未来战士？"><a href="#part-3-未来战士？" class="headerlink" title="part 3 未来战士？"></a>part 3 未来战士？</h3><p>最后，又到了我最喜欢的立flag环节~</p>
<p>在2021，希望能做这些事情，按“我想做到！”的程度排名：</p>
<ul>
<li>捡回创作技能。2021要投稿视频，或者画画、写脚本。</li>
<li>学业精进。系统地学习一个方向，并总结成文章。</li>
<li>坚持做某一件事情。</li>
<li>保持高强度的新内容欣赏，写感想锻炼文笔。<ul>
<li>打完5+个游戏</li>
<li>看10+部动画</li>
<li>随缘看书，就不定计划了</li>
</ul>
</li>
<li>人际交际上进步一点。</li>
</ul>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活小记</tag>
        <tag>年终总结</tag>
      </tags>
  </entry>
  <entry>
    <title>忙与闲 - 2021年终总结</title>
    <url>/2022/02/24/summary-2021/</url>
    <content><![CDATA[<p>越写越懒，明天假期就结束了，今天就把年终总结发出来吧。有些地方还没写完，也有很多地方需要修改措辞，后续会慢慢补完。</p>
<p>2022&#x2F;6&#x2F;18更新：完结！把《死亡搁浅》的部分补完了。22年我打了同是19年的《只狼》，反而对《死亡搁浅》有了更多一些思考。漫画篇的话就先算了，我今年又看了《再见绘梨》，就等我开一篇新的来一起写吧。</p>
<h3 id="part-1-忙与闲"><a href="#part-1-忙与闲" class="headerlink" title="part 1 忙与闲"></a>part 1 忙与闲</h3><h4 id="又忙又闲的一年"><a href="#又忙又闲的一年" class="headerlink" title="又忙又闲的一年"></a>又忙又闲的一年</h4><p>自从寒假赋闲在家，我已经写了两篇生活小记了，基本上完整回顾了去年9月到1月的经历。事实也是如此，在过去的一年里，我的生活以9月为界，划分成闲与忙两部分。</p>
<p>如果完整地回顾去年一整年，不同时间段的主线应该是这样的：</p>
<ul>
<li>3~6月上：上课、考试<ul>
<li>可以说这辈子的课已经全上完了，可喜可贺</li>
<li>科研并没有上心。由于之前的方向太复杂，5月我注意到风格迁移问题，然后逐渐把方向转到了情感识别、图像分类上，以保持一个合理规模</li>
</ul>
</li>
<li>6月下~8月：玩乐为主<ul>
<li>7月跟着教程做了个小游戏</li>
<li>8月突发奇想地在 Leetcode 刷了些题</li>
</ul>
</li>
<li>9~12月：各种事情纷至沓来<ul>
<li>主动找了很多事情来做。主要在UE4公开课作业和科研两件事上奔走。尝试了长时间的并行，但实际上还是以周为单位串行了</li>
<li>同时也被安排了各种活。唯一好在都是杂活，没有横向项目</li>
<li><a href="https://acbgzm.github.io/2022/01/26/misc-1/">这篇文章</a>记录了一部分</li>
</ul>
</li>
<li>1月：求职，在<a href="https://acbgzm.github.io/2022/02/05/jobhunting-1/">这篇文章</a>中详谈了</li>
</ul>
<p>可以说去年我分别过了很闲、很忙的日子</p>
<ul>
<li>9月前（尤其是上课的日子）是绝对的闲。上课肯定是不会听的，反而把一整天的时间拆散了。再加上我的串行属性，零散的时间碌碌无为。这段时间过得很空虚<ul>
<li>回忆起刚上大学的时候，在一学期的高数课上看完了《龙珠》漫画全集，算是一段美好的回忆了。因此也在人生中的最后一个学期，用几节课看完了《炎拳》，善始善终</li>
</ul>
</li>
<li>9月开始就比较忙了。我目前的忙还不是连时间都没有的忙，相反感觉时间很多。但是各种任务让我心累，导致在忙的日子里的空闲时间，也没有劲头去进行一些更好一点的娱乐了。总之，也感到很空虚</li>
</ul>
<p>同时，感觉到有两件事情我没有做好</p>
<ul>
<li>首先，所谓“不懂得休息的人也不懂得工作”。在绝大多数时间里，我感到做事情的效率低，玩也玩不爽。并且失眠得厉害。</li>
<li>然后，坚持真是太难了！以刷题这件需要长期坚持的事为例，在闲的日子里，我必然是懒得打开网站的；在忙的日子里，又没精力再多做一件事情，总之是荒废了。<ul>
<li>可以自豪地说：目前想象不到有什么事情是我能连续做30天的</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/8.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="又忙又闲的生活"><a href="#又忙又闲的生活" class="headerlink" title="又忙又闲的生活"></a>又忙又闲的生活</h4><p>除了日子被整体划分成了忙与闲，每日的生活也是分忙、闲两个状态的。今后会越来越走上按部就班的日常，或许不会再出现大块的特别忙、特别闲的日子了，而是每天都会在两者之间找一个平衡点。</p>
<p>回想起去年写年终总结的时候，我还既不知道该拿什么毕业、又没有决定以后要找什么工作。不得不承认，去年（尤其是下半年）我在“忙”这方面取得了不错的成果。</p>
<p>同样在“忙”这方面，今年可以一眼望到头了：</p>
<ul>
<li>3~5月：回学校，补实验、写论文</li>
<li>6~？月：实习</li>
<li>8~9月：秋招</li>
<li>9~寒假：毕业相关事宜</li>
<li>在其中，穿插一个庞大的游戏开发知识补完计划；同时也会被安排各种不可预料的杂事</li>
<li>显然，每一件事都需要大量的精力才能做好。依然希望今年也能取得一个不错的结果。</li>
</ul>
<p>当然了，我并不想RP成圣人对自己进行什么自我剖析。我不对“自我提升”这件事有什么特别的执念，只希望自己能从各种意义上更舒服一点——尤其是在“闲”的方面。</p>
<p>生活的乐趣就是“闲”的部分罢了。在过去的一年里，我发现我对某段时光感到高兴，并不是因为卷赢了谁、或者做成了什么事情，而是感到在这一段时间内过的很舒服（在不太忙的同时玩得爽）——就像看《龙珠》是一段美好回忆一样。因此，去年立的flag“按照做成的事情来回顾”，到了今年我反而有些抵触了。作为一整年的总结，还是当作完全的开心场所，只去记录我的一些迷思吧。</p>
<p>这些迷思不一定“正确”，记录的各种游戏、动画片、漫画观后感，在今后看会“幼稚”，写出来的东西或许“矫情”。然而，在有了被各种任务搞得心累的体验之后，我坚定地认为：有精力研究一些奇怪的事情、有劲头记录自己的各种想法、有心情体验不同的游戏、能从无聊的东西中找乐子，都是幸福的事情。</p>
<p>这里理应搬出一句流传到俗的的话：“那一天我二十一岁，在我一生的黄金时代，我有好多奢望。我想爱，想吃，还想在一瞬间变成天上半明半暗的云。后来我才知道，生活就是个缓慢受锤的过程，人一天天老下去，奢望也一天天消失，最后变得像挨了锤的牛一样。可是我过二十一岁生日时没有预见到这一点。我觉得自己会永远生猛下去，什么也锤不了我。”</p>
<p>“生活是受锤的过程”，这段话出自《黄金时代》，在开篇没几页就出现了，是中年王二回忆刚来乡下的自己。也有说法说，意识到这一点的时候人就已经老了。</p>
<p>我并不对变老有什么恐惧，也不怕未来的不确定性。但我希望自己的“纯度”再高一点，不是变成纯真丁一郎，而是每天都能做事更专注、玩得更开心，希望一些美好的品格能保留下去。</p>
<p>就像下图，在脉脉里看到的帖子。很遗憾，在过去的一年里，我有这种趋势。希望以后天天玩得开心，无趣的那天晚点到来。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/9.jpg" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h3 id="part-2-内容创作是指游戏行业吗？"><a href="#part-2-内容创作是指游戏行业吗？" class="headerlink" title="part 2 内容创作是指游戏行业吗？"></a>part 2 内容创作是指游戏行业吗？</h3><p>去年我写道：我想做跟内容创作有关的事。不管是创作作品表达自我，还是用技术帮助到优秀内容的创作，都是好事。</p>
<p>要说过去的一年，在我身上发生了什么大事，那就是我决定以后去找游戏开发工作了，并向这个目标做了一些努力。做出这个决定并没有经过思想斗争，也没有一个确定的时间点，所以当我回头思考“为什么”时，也写不出什么理由。</p>
<p>在世俗的意见里，做人生的决定讲究尽力做到贪心。也就是根据自己的条件，寻找局部最优解。那么这大概不是一个好决定，尤其针对行业来说：</p>
<ul>
<li><p>游戏是面向用户的，讲究快速迭代，技术栈跟其他行业大不同。因此注定比软件累、比互联网岗位少，难润、难转行、难回小公司小城市</p>
<ul>
<li>此外，也是一句听了好几年的话了：一切功能最终都会发展到Web。从总的趋势来说确实如此</li>
</ul>
</li>
<li><p>进入游戏行业更有可能是做客户端。技术比起前端、后端来说可能更加不核心，变化更多</p>
</li>
<li><p>跟着项目走、跟着政策走</p>
<ul>
<li>依然，没有一个法制的标准，而是大人物的一句话</li>
<li>上次发版号是2021年的7月，望周知。小公司不知死活，大公司必须出海，小城市退路越来越少</li>
<li>时间往后一点，未成年人基本上告别游戏了</li>
<li>再往后一些，知名技术大牛跳楼了</li>
<li>到我写这篇文章的时候，莉莉丝这种大公司也砍了项目，也有各种不知真假的传言</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/10.jpg" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>需要指出，任何事物都不是简单一两句话能定性的。此外，一个宏大的概念再烂，也否定不了其中一些人所做的努力。我没有实际的工作经验，但仅在网络上，也能感受到一些游戏人卓越的黄金精神。有很多人是热情的理想主义者，乐于分享、充满活力，怀抱游戏梦，心里有广阔的世界。</p>
<p>但我不能把自己想象成工作随便找的优秀人才，因此只能从最普通的角度来考虑行业的诸多弊端。然后就会发现，游戏行业是不归路，属于经典的、喜闻乐见的从头烂到尾。对平凡人来说，进入游戏行业并不是现阶段的最优解。</p>
<p>但话说回来，用贪心做人生的选择有必要吗？去揣测大人物的生意、追求一个虚无缥缈的最优解，让我感到不舒服。作为一介gn，还是来选一个自己喜欢的吧。</p>
<p>没错。喜欢就好。普通最好了。我是什么样子呢？对自己没什么要求，也没有一腔热情。我既不卷、也不上进、更不优秀，不以“忙”为乐。对现实和未来不关心，也不觉得自己有使命。因此行业发展、自己工作的意义，去考虑只是徒增烦恼。</p>
<p>“游戏行业是内容创作吗？”这个问题的答案也没那么重要了。游戏本身是好的，我学到的的创作技能也是真的。总结起来就一点：行业是行业，人是人，我是我。</p>
<p>只要谈到“忙”方面的话题，就不禁觉得有点低落，因此直接结束吧。接下来就进入音乐餐厅环节，讲讲我去年在“闲”的种种体验。</p>
<h3 id="part-3-玩乐记录"><a href="#part-3-玩乐记录" class="headerlink" title="part 3 玩乐记录"></a>part 3 玩乐记录</h3><p>去年的另一个flag，“高强度的新内容欣赏”，我自认为做得还不错。首先进行一个总览：</p>
<ul>
<li><p>体验了不少游戏，而且类型上不拘一格，更重要的是很开心</p>
<ul>
<li>2021年出现了很多精品国产独立游戏，并且也没再执着于武侠等主题了，让人高兴。然而比较可惜的是，我一个都没玩。像《鬼谷八荒》、《戴森球计划》、《风来之国》、《小白兔电商》，我都有点兴趣，在这里记录一下，或许之后体验一下吧</li>
</ul>
</li>
<li><p>而动画就看得比较少了，其原因竟然是我没耐心连续看24分钟的视频，总觉得节奏慢，归根结底是心静不下来。有时想倍速看，但又觉得不如不看，因此就不看了。想想也挺可悲的，其实这两年的佳作有不少，今年我必多看几部</p>
</li>
<li><p>漫画我一直是随缘看一些没什么营养的作品，并不值得记录。但去年年初《电锯人》很火，我就跟风去看了，并且也把藤本树的《炎拳》、《look back》看了一下，下文会总结一下</p>
</li>
<li><p>对于小说，就更静不下心来看了。轻小说跟漫画一样比较随缘，纸质书去年除了比较无聊的《黄金时代》，还看了森见登美彦的《夜行》，光怪陆离，有趣很多。但也就不细说了</p>
</li>
</ul>
<h4 id="PC游戏篇"><a href="#PC游戏篇" class="headerlink" title="PC游戏篇"></a>PC游戏篇</h4><p>对于PC游戏，我不硬核。我不在意类型，也不非常在意作品的细节（只要没有硬伤就好），而更注重总体的体验，也喜欢作品传达的人文精神。实际上，当我回顾一部作品时，更多也是在回忆当时我的种种感受。比如我不会在意逆转裁判的推理谜题水平，也不在意P4G这十分过时的RPG对战玩法。得益于此，我能最大程度上避免剧透。</p>
<p>去年并没有写寒假玩的游戏，这里会把去年和今年两个寒假的游玩体验都写下来。</p>
<h5 id="逆转裁判123"><a href="#逆转裁判123" class="headerlink" title="逆转裁判123"></a>逆转裁判123</h5><p>从2020年、我还在家里愁复试的时候，到2021年2月，《逆转裁判》三部曲我断断续续玩了一整年。时间跨度太久，其中的剧情很多已经忘了，但一直记得全部打通时的畅快感。</p>

    <div id="aplayer-VIZpDZZi" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="4919552" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>在玩之前，我就对整个系列（尤其是3）的高评价有所耳闻。整个玩完了，我或许也明白了它为什么会被这么多人喜欢。《冰菓》里说：名作从诞生的时候就是名作，这句话不假。</p>
<p>好评如潮的文字游戏必然有超神的剧本。三部曲都相当独立，主要围绕成步堂的周边人，依次介绍大家的过去。在此之上，又存在一条浓墨重彩的主线，把整个故事的最高点留到了第三部的最后一章。在这一章里，所有人都紧密联系了起来，所有的伏笔被收回，真相大白，构成最终的华丽的逆转。</p>
<p>法律无情人有情。随着故事越来越往后，成步堂的身边也越来越热闹。大家或许因为机缘巧合聚在一起，但真正把人们联系起来的，正是每个人内心深处对于真相和正义的追求。三部玩下来，除了感叹时光流逝和大家的成长，也多了一份对法律的敬畏，再结合歌德的故事，有一种悲壮的感觉。</p>
<p>《逆转裁判》的人物塑造相当出彩。就算记不清剧情细节，我也记得御剑的改变、绫里家族的斗争、宝月姐妹的挣扎，记得悲情英雄、倒霉刑警。（还记得十分令人在意的希华小姐。）这种塑造并不依赖于大量的日常故事铺垫，而是随着整个游戏流畅的剧情安排，生动地进行展现。当然了，以今天的视角看，不免有样板戏、脸谱化的问题（比如去绑定咖啡、鞭子等物品），但依然瑕不掩瑜。</p>
<p>究其原因，除了前面提到的、人物身上体现出来的人文精神内核（构成人物的内在），还要归功于游戏又老又潮的画面和音乐（构成外在）。其中，“打击感”是经常被讨论的一点。作为一帧帧画出来的2D老游戏，《逆转裁判》的人物动作画得比较流畅，并且看上去有力又准确，再加上各种停顿的设计，确实有不错的打击感。</p>
<p>在整个游玩过程中，能感受到一些年代感，会看到一些经典桥段。值得一记的是成步堂的蓝刺头发型，不知从哪里看到一个说法，这好像也是以前人设上的讲究：跟毛利兰一样，都是为了观众能从剪影里直接认出人物。游戏里也会出现“手机是什么？”这类非常有年代感的对话，看到的时候会觉得有点神奇。</p>
<p>总体来说，虽然我的游玩过程不流畅，但《逆转裁判》三部曲可以构成一段不错的流畅体验。即使有过时的地方，也无法掩盖其拥有的在任何时代都熠熠生辉的人文内核。至于推理谜题的水平如何、剧情是否有“不合理”的地方，就见仁见智了。</p>
<p>如果跟前几年玩的《弹丸论破》三部曲做一个对比，同样是搜查&amp;审判的流程，我会更喜欢弹丸论破改进后的玩法、新潮的画面和表现手法（3D确实好），但毫无疑问，《逆转裁判》所传达的东西更能打动人，每个角色也更加有血有肉。</p>
<h5 id="星露谷物语"><a href="#星露谷物语" class="headerlink" title="星露谷物语"></a>星露谷物语</h5><p>玩《星露谷物语》是我纯度最高的一段游戏体验。在2021年的寒假，从过完年到开学，十几天玩了76.5h。有几天从一睁眼就开始玩，一直玩到睡觉。</p>
<p>《星露谷物语》整体上是很治愈的。在乡下的农场里，我开启了一段完全遵从内心的生活。每天种种地，下下矿，跟潘妮谈谈恋爱；有兴致了就跟鱼王战斗，去温泉泡澡，或者四处走走、单纯看看风景；每晚去酒吧见见朋友，每到节日全村欢聚一堂。</p>
<p>《星露谷物语》也很让人上瘾。作为经营游戏，游戏中的每项目标都是比较长期的，会形成这样的情况：今天完成了这一项目标，但另一个目标也马上要达成了，那就再玩几天吧。同样作为经营游戏，随着农场越来越漂亮，跟邻里乡亲的关系越来越好，我也更想一天天地在这里生活下去。</p>
<p>如果说说场外因素，这游戏还挺适合春节假期来玩的。春节刚好是我“小镇情怀”比较重的时候，也是在一年的结束和新年的开始、感受到时间流逝的时候。在这时体验一段逃离大城市的悠闲（？）生活，打点邻里关系、参加节日，度过一天又一天，可以说很应景了。</p>
<p>假期结束就没再玩了，记得我玩了不到两年，还没跟潘妮结婚，也没挖穿矿井。想的是开学了继续沉迷，后面去了学校果然也还有瘾，但懒得在台式机上加mod、移存档，就去玩别的游戏了。</p>
<p>回想一下，也许是个好的选择。看到一条评论：“一番奋斗之后你实现了大规模生产从而日进斗金，同时也失去了安逸的田园生活”。在合适的地方停下来，保持一段美好的回忆。在今后的某个春节假期，我或许又会开个新档，重新开启一段温馨自由的生活。</p>
<h5 id="泰坦陨落2、Apex-Legends"><a href="#泰坦陨落2、Apex-Legends" class="headerlink" title="泰坦陨落2、Apex Legends"></a>泰坦陨落2、Apex Legends</h5><p>去年的五一假期，我花了一下午打完了久负盛名的《泰坦陨落2》的单人战役。一句话评论：确实不错。</p>
<p>剧情可以说是标准的美国大片，也就是一人成军当英雄，还有经典的人机分别的桥段。由于《泰坦陨落2》本身质量实在够硬，叙事节奏好、氛围棒，整体的观感相当不错。</p>
<p>在种种优点里，最出色的是关卡设计。一方面，在短短几小时的流程中，不同的关卡有独特的玩法，可以说是相当奢侈了。比如让我印象深刻的切换时空的关卡，还有不同的侧重机甲对战、侧重FPS、侧重跑酷的关卡。另一方面，关卡玩法在丰富的同时也很合适、可以说完全是为故事服务的，不会让人觉得喧宾夺主。同样，得益于游戏流畅的上天入地的行动系统，有些关卡会形成正常体验和speedrun两条路线，让人感叹设计之精妙。在经历千辛万苦之后，看看别人一分钟速通的视频，也是一种乐趣。</p>
<p>在这之后，体验了几次Apex（相同的世界观、类似的操作手感，FPS大逃杀游戏），大约10h吧。年纪越来越大，竞争和变强的心思少了。回想前几年我还能跟同学兴致勃勃地征战PUBG，能摸索好几种开车打野战术，现在是没这个劲头了。</p>
<h5 id="Persona-4-Golden（年度最喜欢）"><a href="#Persona-4-Golden（年度最喜欢）" class="headerlink" title="Persona 4 Golden（年度最喜欢）"></a>Persona 4 Golden（年度最喜欢）</h5><p>去年上半年在学校的时候，我断断续续把《P4G》打完了。</p>
<p>如果《泰坦陨落2》是华丽的美国大片，那《P4G》就是经典的日式故事——除了当英雄，一定有一个“成长”的主题。《P4G》集齐了我最讨厌的日式RPG要素：打怪升级，回合式战斗，多周目。然而它还是我2021最喜欢的游戏。</p>
<p>但如果真正写点什么话，来总结、点评一下好在哪里，一下子又觉得百感交集，不知从何说起。《P4G》讲的故事不算特别：随着清新明快的OP，主角来到了小镇八十稻羽，度过不长不短的一年，认识一群朋友，寻找案件背后的真相。</p>

    <div id="aplayer-GRXARVlX" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="401798" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>这一年的生活也不算波澜壮阔：每天上学放学、跟朋友聊天、参加社团活动。但这一年太美好，让人觉得太遥远：每位主角坦诚地面对自己的缺点，真诚地对待朋友，跟彼此建立满级的羁绊。就算世界迷雾重重、真相藏在千丝万缕的线索中，但只要是现在的我们，就没有什么事情是做不到的。</p>
<p>这一年过得很快，道别的一天总会到来。他们或许不会感到很悲伤，因为所有人心里都知道：在最好的时光，我们结交了最真挚的朋友；今后不管大家相隔多远，心永远连接在一起。而我看着 <em>Never More</em> 的歌词，感到十分的伤感：就这样坐上电车的话，感觉就再也不能相见了。</p>

    <div id="aplayer-MetcQhgZ" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="402009" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>《P4G》的画面很有年代感，玩法很落后。但不管在什么时代，它的内核都是所有人共同的向往：勇敢地面对自己，真诚地对待他人，结交一辈子的朋友，找到 shadow world 中的一切真相——这就是完美的青春。我的青春是迷迷糊糊就度过了，但在《P4G》中，我好像也给自己补了个票。</p>
<p>通关后，我感到怅然若失，他们的生活还在继续，我却走到了岔路口。就像看《樱花庄》的最后一卷一样。</p>
<p>时间过了半年，冬天到了。下雪的时候，我耳机里放着 <em>SNOWFLAKES</em> ，感觉去到了很远很远的地方。</p>

    <div id="aplayer-YsqjbxGr" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="401827" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>嘿嘿🤭我的小理世🥵……🤤🤤嘿嘿🤤</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/13.png" style="zoom: 67%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/14.png" style="zoom: 67%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/15.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="Sayonara-Wild-Hearts"><a href="#Sayonara-Wild-Hearts" class="headerlink" title="Sayonara Wild Hearts"></a>Sayonara Wild Hearts</h5>
    <div id="aplayer-LtbMCIMb" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="1446180263" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>


    <div id="aplayer-lXeSIqma" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="1446180264" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>《再见狂野之心》（这个翻译并不准确，但是很帅）是我去年9月玩的独立游戏，风格很鲜明，音乐好听。在这里放上两首：游戏主菜单的歌曲 <em>Sayonara Wild Heart</em> ，以及我最喜欢的关卡 <em>Begin Again</em>。说是音游，也被定义为流行专辑电子游戏(POP ALBUM VIDEO GAME)，即高互动、沉浸式的流行乐专辑，是得过设计奖的。</p>
<p>独立游戏相比于3A，可以更着重于表达，也能在玩法上开拓创新。</p>
<p>在表达方面，《再见狂野之心》整个游戏并不是在讲剧情，而是以一种即内敛又张扬的表达手法（有点像《GRIS》），讲述一条并不清晰的主线，也就是主人公一次心态上的转变。能感到游戏在表达某种强烈的情绪，但也说不出来具体在讲什么，或许玩家内心是什么样子、就能产生什么理解。同样也可以通过不同的方式来解读，比如这一篇文章：<a href="https://www.bilibili.com/read/cv7390886/">《游戏和女性主义：再见，狂野之心》</a>，从游戏去性别特征的主角形象，联系到一个可能的设定来源，看上去很有道理。</p>
<p>而在玩法上，音游的部分难度适中，整个流程以体验为主，看看视觉效果、听听歌。对于我这种对音游不感冒的玩家，也能有不错的体验。至少比玩《几何冲刺》开心。</p>
<h5 id="死亡搁浅（年度最佳）"><a href="#死亡搁浅（年度最佳）" class="headerlink" title="死亡搁浅（年度最佳）"></a>死亡搁浅（年度最佳）</h5>
    <div id="aplayer-wMDQRnmc" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="472045266" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>从9月到11月中旬，我打完了《死亡搁浅》。最强烈的一个想法就是它其实有很强的表达欲（以下暂且称为“强表达性”），在很多地方也有非常偏向艺术性的处理，有点像独立游戏。</p>
<p>简单来说，《死亡搁浅》的表达是：从“绳”和“棍”两个意象出发，来传达“连接”的可贵之处。</p>
<p>游戏送快递的玩法让主角重走了一遍人类“连接”之路——这是空间意义上的。人可以靠双脚“连接”所有的空间，但人类本身从出生到死亡实际上“连接”了时间，对应跟麦叔的那些剧情。除了这两种，还有玩家和玩家之间的“连接”，这部分的玩法设计得应该说很独特也很出彩。当我帮助自己的时候其实也在无意中帮助了别人、当我给集体贡献一些材料的时候其实我也能走上这条公路。这个独特的玩法确实给处于孤独世界中的玩家带来一些温暖，当然也就表达了“连接”的好处。</p>
<p>此外，我认为有一点很难能可贵：《死亡搁浅》为了表达这些东西，实际上提供了很丰富的玩法，爬山、开车、大战BT、打枪，地图也是从森林到雪山到战场。然而， 由于游戏整体的“强表达性”，各种玩法和剧情其实融合得比较舒服，没有让人出戏或者喧宾夺主的感觉。剧情里山姆连接一个个城市、连接所有人、连接自己的过去、连接生和死；玩法上，虽然玩家之间永远见不到面，但在某种意义上也相连在一起。能体会到旅途上的孤独，面对BT的恐惧，但最深入人心的还是人与人之间羁绊的可贵。</p>
<p>另一方面，“强表达性”肯定会带来争议，没有一种想法会让所有人都支持，更不要提《死亡搁浅》传达的这么宏大的主题了。因此更多的3A游戏不会选择很明确地表达什么主题，而是只提供给玩家一段体验，剩下的你自己想去吧——这是非常正确的。动辄几百人的游戏团队，由每个人倾注心血做出来的作品，如果只用来表达几个人小团体的某些想法，是非常不负责任的。</p>
<p>但我肯定觉得《死亡搁浅》是一个好游戏，除了因为我本来就喜欢独立游戏和各种二次元魔怔作品，还有就是《死亡搁浅》敢于并且成功地、统一地表达了非常明确的主题，也引起了几乎每个玩家的思考和讨论（不管是支持还是反对的），从创作的角度我喜欢这种作品。</p>
<p>实际上，19年游戏发售后，20年新冠就开始了，也有更多人更加理解了小岛秀夫到底在传达什么东西。</p>

    <div id="aplayer-KlrBGqHG" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="472045267" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>《死亡搁浅》引人思考，因此我不由地就写了很多主题、表达方面的东西。实际上我同时觉得《死亡搁浅》的玩法、音乐、视觉效果，或者说总体的sense很不错。千辛万苦跨越雪山，看到湖边的心人实验室的时候；跟BT死斗后，下山时天气转晴、bgm《Bones》响起的时候，我作为玩家也百感交集。我觉得这种体验肯定能传达给每一个玩《死亡搁浅》的人，这也是所有人之间的“绳”。</p>
<p>听说小岛秀夫离职、开公司、组建团队加起来只用了3年就把《死亡搁浅》做完了，而且做的还是非常体现个人品味、表达非常统一的大型游戏，也让我第一次感受到了明星制作人的力量。他可能就是那个用“绳”把开发者、演员、玩家、媒体都连接起来的那个传奇人物吧。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/11.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="极限脱出999、善人死亡"><a href="#极限脱出999、善人死亡" class="headerlink" title="极限脱出999、善人死亡"></a>极限脱出999、善人死亡</h5><p>到了11月底，我就有点忙了，因此想玩一下操作轻松的、看剧情为主的游戏。前年我玩了《AI：梦境档案》，是第一次玩打越钢太郎的作品，因此就接着玩了更早一点的《极限脱出》系列。打越钢太郎的风格我也明白一点了：有一些奇怪的科学（神棍）设定，有时空方面的诡计设计，并且会针对诡计设计各种各样的叙事误导。还有就是续作会换掉主角（刚有消息的《梦境档案》续作，主角同样换人了）。</p>
<p>《999》不是很长，剧情让人满意，多结局的流程也很舒服。此外，《999》被称作为了NDS而诞生的游戏，然而我玩的是steam版，就没有感受到连硬件平台都作为诡计的一环的震撼了。</p>
<p>对于《善人死亡》，体量变大了很多。回想起玩《梦境档案》，我就是先走到了悬疑感比较重的支线，然后玩后面的线的时候看谁都像凶手，因此总体体验不错。《善人死亡》我刚走完一条线、发现流程图复杂得超出我的想象，就直接在网上查了一下推荐的游玩顺序。</p>
<p>不得不说，《善人死亡》很见功力。第一部已经是相当完整的故事了，第二部还能在整体的框架下做到把故事继续拔高。剧本更加复杂，各个支线更加有表现力、信息量也多，每个结局之间也有千丝万缕的联系，真结局让人相当震撼。</p>
<p>然而私心上我喜欢《999》。体量适中，2D的立绘更好看，各种谜题、诡计和最终的真结局也很不错了。《善人死亡》有点过于复杂，有时候我隔一天继续玩，就忘了上一次走到哪里、得到了什么结论。可能集中通关的体验更好一些。</p>
<p>最重要的是，《999》更多地在写主角之间的羁绊，人物塑造得不错。而第二部，随着故事越写越复杂，就把动机上升到拯救人类之类的宏大叙事，导致缺乏了一些真实感。因此到了第二部，我也更多地在关注第一部几位主角的故事——这个故事还挺悲伤的，实在让我有一种无力感。这也是遗憾的美学吧，然而我更喜欢大团圆结局。</p>
<p>至于第三部《零时困境》，我看到风评相对不好，就没再玩了。当然12月很忙也没心情玩了。至于以后，第三部，或者打越钢太郎的《ever17》、《remember11》等，虽然总听说多么神，但也没有特别想玩的感觉。就随缘吧，保持随时可以去轻松体验的心态也挺好。</p>
<h5 id="Road-96"><a href="#Road-96" class="headerlink" title="Road 96"></a>Road 96</h5>
    <div id="aplayer-xLiyWBNu" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="1852494200" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>今年春节，我玩了2021著名独立游戏《九十六号公路》。</p>
<p>从玩法上，宣传语号称“随机”、程序化生成，实际上也是通过连续的六段旅途，经历同样的追求自由之路、遇到同样的NPC。游戏主要是把几个NPC的故事拆开到几段旅途中，进行碎片化叙事。这本身是做得不错的：每个人都有不同的追求，以及不同的走上旅途的理由。随着一次次的旅途，娓娓道来每个人的故事。</p>
<p>作为独立游戏，《Road 96》确实很有表达欲。音乐挺搭，氛围也相当不错，然而：</p>
<ul>
<li>《Road 96》对政治太上心了。其实也是文化差异吧，游戏里是一个投票管用的国家，每个人的表达和诉求也很热烈勇敢，让我没有实感。</li>
<li>之前提到了，游戏是通过每个NPC不同的故事来讲几段旅途。然而，结局又是上升到了宏大叙事，个人又显得渺小了，就跟《极限脱出善人死亡》一样。我不喜欢这种感觉，也因此不喜欢政治。</li>
</ul>
<p>又或者这也是游戏想表达的一部分呢？如同 <em>The Road</em> 的歌词，不管踏上旅途的原因如何，不管在途中说什么、做什么、产生什么想法，不管旅途的终点在哪里，终究会发现——the road is my home。</p>
<h4 id="手游篇"><a href="#手游篇" class="headerlink" title="手游篇"></a>手游篇</h4><p>实际上我玩手游也主要是在PC的模拟器上。对于手游我反而比较硬核了，不仅追求一个不错的玩法，能让我每天都打开玩一下，而且肯定是各种系统都深度参与的前排活跃玩家，体力、强度之类必定算到最细。</p>
<h5 id="公主连结Re-dive"><a href="#公主连结Re-dive" class="headerlink" title="公主连结Re:dive"></a>公主连结Re:dive</h5><p>从2020年开服就玩了。去年的年终总结提到我们一直没进过前600。今年公会的大家都很努力，虽然不排刀，但组织得也不错。终于在3月的会战冲600成功。</p>
<p>从5月会战我就去比较后排的公会休闲了，到7月就慢慢退坑了。</p>
<p>游戏是好游戏。剧情轻松愉快，优衣咲恋老婆温柔，玩法多，运营良心，基本上没花钱。就是稍微前面一点的梯队，强度实在太高了。</p>
<p>尤其是会战期间要长时间待机，出刀的时候一点失误不能有。平时培养还要注意装备和等级，我是行会里唯一的9-5狼，直接卡了我三个月，找作业也要多费很多心思。并且：这是我们公会30个人的情况，比我们前面的还有600个公会。会战结束之后，各种模式无缝衔接，jjc随时击剑，越来越心累跟上班一样，慢慢就没热情了。挺佩服能一直作为核心玩家玩下去的人的。</p>
<p>在我退坑之后，公会开始排刀，目前应该稳定200了，心情复杂。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/12.jpg" style="zoom: 80%;" / loading="lazy"></li>
</ul>
<h5 id="原神"><a href="#原神" class="headerlink" title="原神"></a>原神</h5><p>《公主连结》功成身退后，我就入坑《原神》并且一直玩到今天了。在暑假也非常沉迷地玩了一段时间。</p>
<p>《原神》整体来说还是偏“体验”的，画面、音乐都没什么毛病可挑，大地图也很有趣，在很多细节的设计上都花了心思。剧情一般，但人物塑造得很不错，像我入坑后的神里绫华、宵宫、申鹤，都是非常有血有肉的角色。</p>
<p>然而作为手游，也有各种系统让玩家能每天刷刷刷、一直玩下去。这就会带来一些不是很美好的体验，包括钓鱼等既无聊又不得不完成的任务，以及一些为了重复而重复的活动。作为氪金手游，又带来了各种强度上的比拼，带来数值膨胀、怪物设计等一系列问题。这也是通病了，只不过《原神》从音乐到角色到剧情，整体感都做得不错，因此割裂感就凸显出来。实际上，相比于之前玩过的其他二次元手游，《原神》已经很振奋人心了。（可惜不便宜。）</p>
<p>下图是我在尘歌壶里拼了个圣诞树，试图欢度圣诞节。跟的<a href="https://www.bilibili.com/video/BV1Kb4y187Jk">这个教程</a>。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/16.jpg" style="zoom: 67%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/2022/17.jpg" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="世界弹射物语"><a href="#世界弹射物语" class="headerlink" title="世界弹射物语"></a>世界弹射物语</h5><p>在12月最忙的时候，为了在苦闷的生活中找点事做，我开始玩《世界弹射物语》。</p>
<p>事实上确实也还可以。像素风，玩法比较轻量级，剧情轻松为主带点浪漫色彩（是我喜欢的邂逅、告别的模式），而且基本上不用花钱。满足了我对手游的期待，轻松又有趣。</p>
<h4 id="动画篇"><a href="#动画篇" class="headerlink" title="动画篇"></a>动画篇</h4><h5 id="赛马娘"><a href="#赛马娘" class="headerlink" title="赛马娘"></a>赛马娘</h5>
    <div id="aplayer-dxpVTjIR" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="444548577" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>去年并没有看很多动画。只有：为了看新剧场版，重新看了一遍《EVA》的TV；补了一下《金色时光》；看了两季《赛马娘》。</p>
<p>作为超越《化物语》的新时代销量之王，作为公认的神作，《赛马娘2》的各种细节评价已经不需要我说了。</p>
<p>什么是动画？有的人大用特用MG动画，实现电影做不到的画面效果；有的人描述奇幻世界，讲述浪漫或宏大故事；有的人用黑深残打动人心。在各种情况下，《赛马娘》的画面不算出彩，剧情也是王道框架，故事发展被限制在现实事实中了，甚至连“胜负”这种关键性的悬念也不再存在。</p>
<p>《赛马娘》把现实世界的故事好好讲了一遍，仅此而已，又不止于此。</p>
<p>什么是优秀的改编、好的动画？动画世界可以是温柔的。动画可以让现实中魔幻的故事变得合理。动画世界一直不缺少奇迹和魔法，甚至多到廉价。但如果现实中的奇迹映射到动画中，就会更加闪耀。</p>
<p>而在动画之外，由于题材原因我之前并没看《赛马娘1》。但看完了《赛马娘》两季，我搜了几个现实赛马的视频来看，去查了动画中的马娘的背景故事，看了几篇讲赛马、赌马文化的文章。聊天时我会用星云天空和米浴的表情包。之后手游国服出来了我也会去玩。我想这就是好的动画吧。</p>
<h4 id="漫画篇-——下次见——"><a href="#漫画篇-——下次见——" class="headerlink" title="漫画篇 ——下次见——"></a>漫画篇 ——下次见——</h4>]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活小记</tag>
        <tag>年终总结</tag>
      </tags>
  </entry>
  <entry>
    <title>machine learning - andrew ng</title>
    <url>/2021/03/25/ml2014ang/</url>
    <content><![CDATA[<p>图片比较多，加载方法也比较笨，好在可以记忆浏览位置，如果有载图错误直接刷新即可。</p>
<p>代码笔记见 <a href="https://github.com/ACBGZM/ml-notes/tree/master/ng-ml2014/code">此处</a> 。</p>
<h2 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h2><h3 id="1-1-欢迎"><a href="#1-1-欢迎" class="headerlink" title="1-1 欢迎"></a>1-1 欢迎</h3><p>机器学习应用：</p>
<ul>
<li>Database mining<ul>
<li><strong>large datasets</strong> from growth of automation&#x2F;web.</li>
<li>e.g., Web click data, medical records, biology, engineering</li>
</ul>
</li>
<li>Applications cannot program by hand.<ul>
<li>e.g., Autonomous helicopter, handwriting recognition, NLP, CV</li>
</ul>
</li>
<li>Self-customizing programs<ul>
<li>e.g., Amazon product recommendations</li>
</ul>
</li>
<li>Understanding human learning (brain, real AI)</li>
</ul>
<h3 id="1-2-What-is-ML"><a href="#1-2-What-is-ML" class="headerlink" title="1-2 What is ML"></a>1-2 What is ML</h3><p>经验E、任务T、度量P：<strong>机器学习是 T, measured by P, improves with E.</strong></p>
<p>ML algorithms:</p>
<ul>
<li>Supervised learning（监督学习）</li>
<li>Unsupervised learning（无监督学习）</li>
<li>Others: Reinforcement learning（强化学习）, recommender systems（推荐系统）</li>
</ul>
<h3 id="1-3-监督学习"><a href="#1-3-监督学习" class="headerlink" title="1-3 监督学习"></a>1-3 监督学习</h3><p>“right answers” given：给一个数据集，含有一部分正确答案。使用监督学习算法得到对应关系。</p>
<p>监督学习类型：</p>
<ul>
<li>regression（回归问题）：输出具体值。如估计房价</li>
<li>classification（分类问题）：输出离散值。如判断肿瘤</li>
</ul>
<p>Q：实际问题，有多个attributes和多个features，甚至无穷个，如何处理？</p>
<p>A：以支持向量机算法为例，有数学方法来处理无穷多的features。</p>
<h3 id="1-4-无监督学习"><a href="#1-4-无监督学习" class="headerlink" title="1-4 无监督学习"></a>1-4 无监督学习</h3><p>给定数据集没有label，不知道有哪些类型。使用无监督学习算法寻找数据中的结构。</p>
<p>无监督学习类型：</p>
<ul>
<li><p>使用clustering（聚类算法）来自动分簇。</p>
<p>如：新闻分簇，DNA分组；组织计算机群、社交网络分析、市场分割、天文数据分析。</p>
</li>
<li><p>Cocktail party problem，两个麦克录下两个人同时说话，将他们分离。</p>
</li>
</ul>
<h2 id="第二章-单变量线性回归（Linear-Regression-with-One-Variable）"><a href="#第二章-单变量线性回归（Linear-Regression-with-One-Variable）" class="headerlink" title="第二章 单变量线性回归（Linear Regression with One Variable）"></a>第二章 单变量线性回归（Linear Regression with One Variable）</h2><h3 id="2-1-regression模型描述"><a href="#2-1-regression模型描述" class="headerlink" title="2-1 regression模型描述"></a>2-1 regression模型描述</h3><p>训练集</p>
<ul>
<li>m &#x3D;训练样本的个数</li>
<li>x &#x3D; input variable&#x2F;features</li>
<li>y &#x3D; output variable&#x2F;features</li>
<li>(x, y) &#x3D; 一个训练样本</li>
<li>(x<sup>(i)</sup>, y<sup>(i)</sup>) &#x3D; 第i个样本</li>
</ul>
<p>机器学习算法从训练集得出函数h（hypothesis），表示从x到y的对应关系。</p>
<p>在预测房价问题上，假定h是一元一次函数。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/1.jpg' / loading="lazy">




<h3 id="2-2-代价函数（cost-function）"><a href="#2-2-代价函数（cost-function）" class="headerlink" title="2-2 代价函数（cost function）"></a>2-2 代价函数（cost function）</h3><p>现在有h函数：$h_\theta(x) &#x3D; \theta_0 + \theta_1x$，还有一个训练集。</p>
<p>我们的目的是寻找h函数中的两个参数θ<sub>0</sub>、θ<sub>1</sub>，让h<sub>θ</sub>(x)拟合实际y.</p>
<p>定义代价函数，计算h和y的差，要让其取最小，表达式如下：</p>
<p>$$minimize_{\theta_0, \theta_1}  \frac{1}{2m}\sum_{i&#x3D;1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2$$</p>
<p>也就是使代价函数  $ J({\theta_0, \theta_1}) &#x3D; \frac{1}{2m}\sum_{i&#x3D;1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2 $  取到最小值。</p>
<h3 id="2-3-代价函数的直观理解I"><a href="#2-3-代价函数的直观理解I" class="headerlink" title="2-3 代价函数的直观理解I"></a>2-3 代价函数的直观理解I</h3><ul>
<li><p>Hypothesis:		$h_\theta(x) &#x3D; \theta_0 + \theta_1x$</p>
</li>
<li><p>Parameters:		$\theta_0, \theta_1$</p>
</li>
<li><p>Cost Funcion:	 $J({\theta_0, \theta_1}) &#x3D; \frac{1}{2m}\sum_{i&#x3D;1}^{m}({h_\theta(x^{(i)})-y^{(i)}})^2$</p>
</li>
<li><p>Goal:					$minimize_{\theta_0, \theta_1} J({\theta_0, \theta_1})$</p>
</li>
</ul>
<p>取不同的θ<sub>0</sub>θ<sub>1</sub>，得到不同的h(x)，对应不同的J(θ<sub>0</sub>, θ<sub>1</sub>).</p>
<p>下图是只有一个变量的简化情况，此时可以画出一元函数 J(θ<sub>1</sub>) 的曲线。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/2.jpg' / loading="lazy">

<p>选择θ，使J的值最小化。此时h函数最好地拟合了现实情况。</p>
<h3 id="2-4-代价函数的直观理解II"><a href="#2-4-代价函数的直观理解II" class="headerlink" title="2-4 代价函数的直观理解II"></a>2-4 代价函数的直观理解II</h3><p>有两个参数时，得到 J(θ<sub>0</sub>, θ<sub>1</sub>) 的三维图像。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/3.jpg'  width="50%" height="50%"/ loading="lazy">

<p>可以用等高线图在平面上展示。中心点处函数值最小。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/4.jpg'  width="50%" height="50%"/ loading="lazy">

<p>我们希望有一个算法，来<strong>自动找到使 J 最小的参数 θ</strong> .</p>
<h3 id="2-5-梯度下降（gradient-descent）"><a href="#2-5-梯度下降（gradient-descent）" class="headerlink" title="2-5 梯度下降（gradient descent）"></a>2-5 梯度下降（gradient descent）</h3><ol>
<li><p>开始时，给θ设置初始值；</p>
</li>
<li><p>改变θ，使 J 的值减少，直到取到了局部最小值；</p>
</li>
<li><p>重复上个过程。</p>
</li>
</ol>
<p>直观解释：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/5.jpg'  width="50%" height="50%"/ loading="lazy">

<p>​		不同的初始点可能走到不同的结束点。</p>
<p>数学解释：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/6.jpg'  width="70%" height="70%"/ loading="lazy">

<p>​		α 是学习率，表示梯度下降的步幅大小。偏导数表示梯度下降的方向。</p>
<p>​		需要注意的是，要让多个 θ 同时更新。先计算多个temp值，再一起赋新值。</p>
<h3 id="2-6-梯度下降的直观理解"><a href="#2-6-梯度下降的直观理解" class="headerlink" title="2-6  梯度下降的直观理解"></a>2-6  梯度下降的直观理解</h3><p>偏导数的直观解释：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/7.jpg'  width="50%" height="50%"/ loading="lazy">

<p>​		以单个变量的函数为例。偏导数的值保证了 $\theta$ 一定朝 $J(\theta)$ 下降的方向变化。</p>
<p>学习率 α 的直观解释：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/8.jpg'  width="50%" height="50%"/ loading="lazy">

<p>​		α 太小，步数太多；α 太大，会导致无法收敛甚至发散。</p>
<p>在到达optimum点后，偏导数值为0，梯度下降算法就什么也不做了。</p>
<p>当学习率 α 不变时，梯度下降进行的过程中，偏导数通常会变小，因此每一步下降幅度会减小。</p>
<p>因此在接近局部最小值时，步子会变小。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/9.jpg'  width="50%" height="50%"/ loading="lazy">



<h3 id="2-7-线性回归的梯度下降"><a href="#2-7-线性回归的梯度下降" class="headerlink" title="2-7 线性回归的梯度下降"></a>2-7 线性回归的梯度下降</h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/10.jpg'  width="70%" height="70%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/11.jpg'  width="70%" height="70%"/ loading="lazy">

<p>这个梯度下降算法称为 <strong>batch</strong> 梯度下降。</p>
<p>​		原因是：每一步梯度下降，都计算了<strong>整个训练集m个样本</strong>的插值平方总和。</p>
<p>​		也有方法不全览整个训练集，每次只关注小子集。这将在之后介绍（<a href="#17.2">17.2-17.4</a>）。</p>
<p>接下来的课：</p>
<ul>
<li>在线性代数上，存在一个解法，可以在不需要多步梯度下降的情况下，也能解出代价函数的最小值，这是另一种称为正规方程(<strong>normal equations</strong>)的方法。实际上在数据量较大的情况下，梯度下降法比正规方程要更适用一些。</li>
<li>梯度下降的通用算法</li>
</ul>
<h2 id="第三章-线性代数"><a href="#第三章-线性代数" class="headerlink" title="第三章 线性代数"></a>第三章 线性代数</h2><h3 id="3-1-矩阵和向量"><a href="#3-1-矩阵和向量" class="headerlink" title="3-1 矩阵和向量"></a>3-1 矩阵和向量</h3><p>向量是一个 n × 1 的矩阵。</p>
<p>默认的下标从1开始。</p>
<h3 id="3-2-加法和标量乘法"><a href="#3-2-加法和标量乘法" class="headerlink" title="3-2 加法和标量乘法"></a>3-2 加法和标量乘法</h3><h3 id="3-3-矩阵与向量相乘"><a href="#3-3-矩阵与向量相乘" class="headerlink" title="3-3 矩阵与向量相乘"></a>3-3 矩阵与向量相乘</h3><p>一元线性回归可以转换成矩阵和向量相乘。</p>
<p>下图是矩阵和向量的构造方法，以及代码。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/12.jpg'  width="70%" height="70%"/ loading="lazy">



<h3 id="3-4-矩阵乘法"><a href="#3-4-矩阵乘法" class="headerlink" title="3-4 矩阵乘法"></a>3-4 矩阵乘法</h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/13.jpg' / loading="lazy">



<h3 id="3-5-矩阵乘法的性质"><a href="#3-5-矩阵乘法的性质" class="headerlink" title="3-5 矩阵乘法的性质"></a>3-5 矩阵乘法的性质</h3><p>矩阵乘法是：</p>
<ul>
<li>不可交换的 A × B ≠ B × A </li>
<li>可结合的  (A × B)× C &#x3D; A ×(B× C）</li>
</ul>
<p>单位矩阵 I ：</p>
<ul>
<li>A · I &#x3D; I · A &#x3D; A</li>
</ul>
<h3 id="3-6-逆、转置"><a href="#3-6-逆、转置" class="headerlink" title="3-6 逆、转置"></a>3-6 逆、转置</h3><p>矩阵的逆</p>
<ul>
<li>AA<sup>-1</sup> &#x3D; A<sup>-1</sup>A &#x3D; I ，A是满秩方阵</li>
</ul>
<p>矩阵的转置</p>
<ul>
<li>A<sup>T</sup><sub>ij</sub> &#x3D; A<sub>ji</sub></li>
</ul>
<hr>
<h2 id="第四章-多变量线性回归（Linear-Regression-with-Multiple-Variables）"><a href="#第四章-多变量线性回归（Linear-Regression-with-Multiple-Variables）" class="headerlink" title="第四章 多变量线性回归（Linear Regression with Multiple Variables）"></a>第四章 多变量线性回归（Linear Regression with Multiple Variables）</h2><h3 id="4-1-多维特征"><a href="#4-1-多维特征" class="headerlink" title="4-1 多维特征"></a>4-1 多维特征</h3><p>从只有1个变量的情况，推广到有m组n维特征的情况。</p>
<p>$h_\theta(x) &#x3D; \theta_0 + \theta_1x_1 + \theta_2x_2 + … + \theta_nx_n$</p>
<p>设x<sub>0</sub> &#x3D; 1，则$h_\theta(x) &#x3D; \theta^Tx$（写成向量内积表达式）</p>
<h3 id="4-2-多变量梯度下降"><a href="#4-2-多变量梯度下降" class="headerlink" title="4.2 多变量梯度下降"></a>4.2 多变量梯度下降</h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/14.jpg'  width="70%" height="70%"/ loading="lazy">



<h3 id="4-3-梯度下降技巧1-特征缩放"><a href="#4-3-梯度下降技巧1-特征缩放" class="headerlink" title="4.3 梯度下降技巧1-特征缩放"></a><span id="4.3">4.3 梯度下降技巧1-特征缩放</span></h3><p>当多个特征取值范围相差很大，梯度下降收敛得很慢。</p>
<p>因此，进行<strong>Feature Scaling</strong>，将每个特征都控制在约 -1 ≤ x<sub>i</sub> ≤ 1 的范围内。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/15.jpg'  width="70%" height="70%"/ loading="lazy">

<p> 除了除以最大值，还有一个均值归一化的工作（<strong>mean normalization</strong>），让特征的均值接近0.</p>
<p>具体做法是用 （x<sub>i</sub> - μ<sub>i</sub>）代替 x<sub>i</sub>。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/16.jpg'  width="70%" height="70%"/ loading="lazy">



<h3 id="4-4-梯度下降技巧2-学习率"><a href="#4-4-梯度下降技巧2-学习率" class="headerlink" title="4.4 梯度下降技巧2-学习率"></a><span id="4.4">4.4 梯度下降技巧2-学习率</span></h3><p>绘制随迭代次数增加，代价函数值的变化图象，来确定梯度下降算法在正常运行。</p>
<p>也可以用设置阈值（检测平滑）的方式自动测试，但确定阈值是困难的。看图像在大多数时候更方便直观。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/17.jpg' / loading="lazy">

<p>当代价函数曲线是上升的或不收敛，通常的解决方法是使用更小的 α 值。</p>
<p>α 太小会让收敛变得很慢，多试几次，选一个合适的 α 值。</p>
<h3 id="4-5-选择合适的特征和多项式回归（polynomial-regression）"><a href="#4-5-选择合适的特征和多项式回归（polynomial-regression）" class="headerlink" title="4.5 选择合适的特征和多项式回归（polynomial regression）"></a><span id="4.5">4.5 选择合适的特征和多项式回归（polynomial regression）</span></h3><p>通过对函数图像的了解，和对数据的了解，<strong>选择合适的特征</strong>，来获得更好的模型。</p>
<p>如预测房屋价格，可以选择房屋的size，或者房屋的宽度等特征。</p>
<p>用多项式回归（<strong>polynomial regression</strong>）来理解选择特征：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/18.jpg' / loading="lazy">

<p>​		作简单的处理来拟合多项式模型：让x<sub>i</sub>为size的i次方，或对size开方。</p>
<p>​		在这种指数变换的情况下，做特征scaling是很有必要的。</p>
<p>也有些算法能够观察给出的数据，<strong>自动</strong>选择特征。</p>
<h3 id="4-6-正规方程"><a href="#4-6-正规方程" class="headerlink" title="4.6 正规方程"></a>4.6 正规方程</h3><p>对于某些<strong>线性回归问题</strong>，正规方程可以求解参数 θ 的最优值。</p>
<p><strong>正规方程：</strong></p>
<p>当 θ 是一个实数，让代价函数导数为0，可以解出 θ 的值。（函数求极小值，找导数为 0 的点）</p>
<p>推广到 θ 是向量，通过设置代价函数的偏导数为0，求解 θ 。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/19.jpg'  width="70%" height="70%"/ loading="lazy">

<p>具体做法：构造 m*(n+1) 矩阵 X 和 m 维向量 y ，用最小二乘法计算 θ 。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/20.jpg'  width="70%" height="70%"/ loading="lazy">



<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/21.jpg'  width="60%" height="60%"/ loading="lazy">

<p><strong>使用正规方程法，不需要做特征scaling。</strong></p>
<p>线性回归问题的求解参数 θ：</p>
<ul>
<li><p>梯度下降：在减小代价函数的过程中，<strong>迭代变换  θ</strong> 。</p>
<ul>
<li>需要选择 α 并进行需要多次迭代</li>
</ul>
</li>
<li><p>正规方程：<strong>解析求解，只需一步</strong>。</p>
<ul>
<li>n 很大时，(X<sup>T</sup>X)<sup>-1</sup> 很难算（可以把 n&#x3D;10000 当作进行选择的边缘）</li>
</ul>
</li>
</ul>
<h3 id="4-7-正规方程及不可逆性"><a href="#4-7-正规方程及不可逆性" class="headerlink" title="4.7 *正规方程及不可逆性"></a><span id="4.7">4.7 *正规方程及不可逆性</span></h3><p>使用正规方程求解参数 $\theta &#x3D; (X^TX)^{-1}X^Ty$ 时，如果：</p>
<p><strong>X<sup>T</sup>X不可逆</strong>时（其实发生得很少）</p>
<ul>
<li>咋办？	如在Octave里，有<strong>求伪逆的函数 pinv</strong>，矩阵不可逆时也可以正常求解。 </li>
<li>原因？    矩阵不满秩。<ul>
<li><strong>多个 x 线性相关</strong>。修改冗余的特征</li>
<li><strong>m≤n（特征多，数据少）</strong>。用正则化（<strong>regularization</strong>）解决</li>
</ul>
</li>
</ul>
<hr>
<h2 id="第五章-Octave-Tutorial"><a href="#第五章-Octave-Tutorial" class="headerlink" title="第五章 Octave Tutorial"></a>第五章 Octave Tutorial</h2><p>由于Octave不再具有先进性，我学习了Python3的numpy和pandas库，<a href="https://github.com/ACBGZM/ml-notes/tree/master/ng-ml2014/code/01-numpyandpandas">笔记见代码和注释</a>。</p>
<p><em><strong>（todo：此处总结成一篇文章后，链接到博客）</strong></em></p>
<h3 id="5-6-向量化"><a href="#5-6-向量化" class="headerlink" title="5.6 向量化"></a>5.6 向量化</h3><p>将一般的运算转化成<strong>使用线性代数库的矩阵、向量运算</strong>。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/22.jpg'  width="70%" height="70%"/ loading="lazy">

<p>在线性回归问题，同步更新θ的问题上，向量运算如下：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/23.jpg' / loading="lazy">

<ul>
<li><strong>θ 是向量，α 是数，δ 是向量</strong>。最终目的是<strong>更新 θ 向量</strong>。</li>
<li>对于 δ ，是由多个数组成的向量。<ul>
<li>δ<sub>k</sub> 由 h-y和x<sub>k</sub><sup>(i)</sup>相乘再相加而来，<strong>h-y是差值是一个数，x<sub>k</sub><sup>(i)</sup>是第i行第k个属性值也是一个数，δ<sub>k</sub> 就是一个数</strong>。</li>
<li><strong>δ 是由数组成的向量</strong>。</li>
<li>整体上，也可以看做图片上 $u &#x3D; 2v + 5w$ 的向量相加形式：先纵向形成向量，<strong>所有的h-y是一样的，也就是是一个倍数，x是第i行数据的一个向量。δ就像u一样，做类似的向量×系数再相加</strong>。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="第六章-逻辑回归（Logistic-Regression）"><a href="#第六章-逻辑回归（Logistic-Regression）" class="headerlink" title="第六章 逻辑回归（Logistic Regression）"></a>第六章 逻辑回归（Logistic Regression）</h2><p>当要预测的 y 是离散的，就是 Classification 问题。</p>
<p>Logistic Regression 算法就是一个广泛应用的 Classification 算法。</p>
<h3 id="6-1-分类问题（classification）"><a href="#6-1-分类问题（classification）" class="headerlink" title="6.1 分类问题（classification）"></a>6.1 分类问题（classification）</h3><p>Q：为什么线性回归不好用了？</p>
<p>A：线性回归解决这类问题的方法是拟合后设置阈值，再区分成离散值。</p>
<p>​	   如图，当在远处值，拉低了直线的斜率，就会让前面的肿瘤被误判成0.</p>
<p>（别忘了 x<sub>0</sub> 默认为1，以此让 θ<sub>0 </sub>作为偏移量，让直线离开原点）</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/24.jpg'  / loading="lazy">

<p>另外，与linear regression不同，logistic regression算法需要让 $h_\theta(x)$ 的值在 [0, 1]。</p>
<h3 id="6-2-假设表示（hypothesis-representation）"><a href="#6-2-假设表示（hypothesis-representation）" class="headerlink" title="6.2 假设表示（hypothesis representation）"></a>6.2 假设表示（hypothesis representation）</h3><p>当有一个分类问题，我们要用哪个方程，来表示我们的假设？</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/25.jpg'  / loading="lazy">

<p>Sigmoid&#x2F;Logistic 方程如图，我们通过拟合出 θ，让 x 和 h 反应真实情况。</p>
<p>从结果上，我们的结论是 $h_\theta(x)&#x3D;P(y&#x3D;1|x;\theta)$ ，即：x 这个样本，有 h 的概率，是 y&#x3D;1 代表的情况.</p>
<p>（” probability that y &#x3D; 1, given x, parameterized by θ “）</p>
<h3 id="6-3-判定边界（decision-boundary）"><a href="#6-3-判定边界（decision-boundary）" class="headerlink" title="6.3 判定边界（decision boundary）"></a>6.3 判定边界（decision boundary）</h3><p>这个概念让我们理解假设函数 h 是如何做出预测的。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/26.jpg'  width="60%" height="60%"/ loading="lazy">

<ul>
<li><p>当 $\theta^Tx ≥ 0$ 时，有  $h_\theta(x) &#x3D; g(\theta^Tx) ≥ 0.5$ ，取离散值 y &#x3D; 1；</p>
</li>
<li><p>当 $\theta^Tx ≤ 0$ 时，有  $h_\theta(x) &#x3D; g(\theta^Tx) ≤ 0.5$ ，取离散值 y &#x3D; 0。</p>
</li>
</ul>
<p>一个例子：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/27.jpg'  width="60%" height="60%"/ loading="lazy">

<p>以上图两个x的情况为例，$\theta$ 可以确定一条直线，把 y &#x3D; 1 和 y &#x3D; 0 的情况分隔开，这条直线就叫判定边界。</p>
<p>在 <a href="#4.5">4.5 节</a>中介绍了多项式回归，在特征 x 中，添加额外的高阶多项式项。在逻辑回归中也适用：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/28.jpg'  width="60%" height="60%"/ loading="lazy">

<p>区别是线性回归改的是 $h_\theta(x) &#x3D; \theta^Tx$，而逻辑回归问题改的是 $\theta^Tx$ ， $h_\theta(x) &#x3D; g(\theta^Tx)$。</p>
<p>需要注意的是：判定边界是由 θ 确定的，不是由训练集定义的。训练集的 x 所做的只是拟合出合适的 θ。</p>
<p>问题来了：如何根据数据，自动拟合出参数 θ ？（有些包的函数可以，如scipy）</p>
<h3 id="6-4-代价函数（cost-function）"><a href="#6-4-代价函数（cost-function）" class="headerlink" title="6.4 代价函数（cost function）"></a>6.4 代价函数（cost function）</h3><p>问题如下：如何选择 θ ？该定义怎样的代价函数来迭代 θ ？</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/29.jpg'  width="50%" height="50%"/ loading="lazy">

<p>在线性回归问题中，$h_\theta(x)&#x3D;\theta^Tx$ 是线性函数，定义 $Cost(h_\theta(x), y) &#x3D; \frac{1}{2}(h_\theta(x)-y)^2$ ，进而定义代价函数 $J(\theta) &#x3D; \frac{1}{m} \Sigma Cost$。$J(\theta)$是 convex 函数，梯度下降可以应用。</p>
<p>而在逻辑回归问题中， $h_\theta(x) &#x3D; \frac{1}{1+e^{-\theta^Tx}}$ <strong>不是线性</strong>的，如果还是像线性回归那样计算 $J(\theta)$ ，会发现 $J(\theta)$ 是一个 <strong>non-convex 函数，使梯度下降无法应用</strong>。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/30.jpg'  width="60%" height="60%"/ loading="lazy">

<p>需要另外找一个是凸函数的Cost函数：</p>
<div>
$$
Cost(h_\theta(x), y)=
\begin{cases} 
-log(h_\theta(x)),&\text{if }  \quad y=1 \\
-log(1-h_\theta(x)),&\text{if }  \quad y=0
\end{cases}
$$
</div>


<ul>
<li>y &#x3D; 1 时，预测出的 h 越偏离 1，Cost 越大；</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/31.jpg'  width="60%" height="60%"/ loading="lazy">

<ul>
<li>y &#x3D; 0 时，预测出的 h 越偏离 0，Cost 越大。</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/32.jpg'  width="60%" height="60%"/ loading="lazy">

<p>本节定义了单训练样本的代价函数，虽然没有进行详细的凸性分析，但代价函数此时是凸函数。接下来将扩展此结论，给出整个训练集的代价函数的定义。并给出更简单的写法。</p>
<h3 id="6-5-简化代价函数和应用梯度下降"><a href="#6-5-简化代价函数和应用梯度下降" class="headerlink" title="6.5 简化代价函数和应用梯度下降"></a>6.5 简化代价函数和应用梯度下降</h3><p><strong>简化代价函数：</strong></p>
<ul>
<li>逻辑回归的代价函数：</li>
</ul>
<p>$$J(\theta) &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m}Cost({h_\theta(x^{(i)}), y^{(i)}})$$</p>
<div>
$$\begin{equation}
Cost(h_\theta(x), y)=\left\{
\begin{array}{rcl}
-log(h_\theta(x)) & \text{if} \quad y=1\\
-log(1-h_\theta(x)) & \text{if} \quad y=0\\
\end{array} \right.
\end{equation}$$
</div>

<p>$$Note:y总是取0或1$$</p>
<ul>
<li>Cost函数简写为：</li>
</ul>
<p>$$Cost(h_\theta(x), y)&#x3D; -y \log h_\theta(x)-(1-y) \log(1-h_\theta(x))$$</p>
<ul>
<li>代价函数简写为：</li>
</ul>
<p>$$J(\theta) &#x3D; -\frac{1}{m}[\sum_{i&#x3D;1}^{m}y^{(i)} \log h_\theta(x^{(i)})+(1-y^{(i)}) \log(1-h_\theta(x^{(i)}))]$$</p>
<p>Q：为什么选择这样形式的代价函数？</p>
<p>A：虽然没有详细解释，但这个式子是通过极大似然法得来的，是统计学中，为不同的模型快速寻找参数的方法。并且这是一个convex函数。</p>
<p><strong>应用梯度下降：</strong></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/33.jpg' / loading="lazy">

<p>（todo：↑ 上式少写了一个 $\frac{1}{m}$？）</p>
<p>从形式上，逻辑回归和线性回归的梯度下降表达式一样。但两者的 $h_\theta(x)$ 定义不同，实际上是完全不同的东西。</p>
<p>在线性回归介绍的方法，像 <a href="#4.4">如何监控梯度下降正常运行</a>、<a href="#4.3">feature scaling</a> 在逻辑回归也适用。</p>
<h3 id="6-6-高级优化"><a href="#6-6-高级优化" class="headerlink" title="6.6 高级优化"></a>6.6 高级优化</h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/34.jpg' / loading="lazy">

<p>除了梯度下降，还有其他的高级算法能让 $J(\theta)$ 收敛。如共轭梯度、BFGS、L-BFGS。</p>
<p>只需知道用法即可，不一定要弄清所有的实现细节，也不要自己去实现这些算法。</p>
<pre class="language-matlab" data-language="matlab"><code class="language-matlab"><span class="token keyword">function</span> <span class="token punctuation">[</span>jVal<span class="token punctuation">,</span> gradient<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">costFunction</span><span class="token punctuation">(</span>theta<span class="token punctuation">)</span>
	jVal <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token operator">^</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token operator">^</span> <span class="token number">2</span><span class="token punctuation">;</span>
	gradient <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">gradient</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">gradient</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token function">theta</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

options <span class="token operator">=</span> <span class="token function">optimset</span><span class="token punctuation">(</span><span class="token string">'GradObj'</span><span class="token punctuation">,</span> <span class="token string">'on'</span><span class="token punctuation">,</span> <span class="token string">'MaxIter'</span><span class="token punctuation">,</span> <span class="token string">'100'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
initialTheta <span class="token operator">=</span> <span class="token function">zeros</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span>optTheta<span class="token punctuation">,</span> functionVal<span class="token punctuation">,</span> exitFlag<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">fminunc</span><span class="token punctuation">(</span><span class="token operator">@</span>costFunction<span class="token punctuation">,</span> initialTheta<span class="token punctuation">,</span> options<span class="token punctuation">)</span></code></pre>

<p>以上代码调用 fminunc 来进行函数的收敛和 optTheta 的迭代。</p>
<p>对于一般场景的逻辑回归，多个theta的处理方式如下：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/35.jpg'  width="60%" height="60%"/ loading="lazy">



<h3 id="6-7-多分类问题：一对多（multi-class-classification）"><a href="#6-7-多分类问题：一对多（multi-class-classification）" class="headerlink" title="6.7 多分类问题：一对多（multi-class classification）"></a><span id='6.7'>6.7 多分类问题：一对多（multi-class classification）</span></h3><p>训练集的数据有多个分类，如何拟合分类器？</p>
<p>one-vs-all 方法：把训练集的每个类别分别单独拟合一个分类器。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/36.jpg'  width="70%" height="70%"/ loading="lazy">

<p>训练一个逻辑回归分类器 $h_\theta^{(i)}(x)$ ，对于每个类别 $i$ ，分类器预测 $y &#x3D; i$ 的概率。</p>
<p>为了预测任意一个输入 $x$ 的分类，取 $\max i: h_\theta^{(i)}(x)$ ，即让分类器取最大概率值的 $i$ 类别。</p>
<hr>
<h2 id="第七章-正则化（Regularization）"><a href="#第七章-正则化（Regularization）" class="headerlink" title="第七章 正则化（Regularization）"></a>第七章 正则化（Regularization）</h2><h3 id="7-1-过拟合问题（overfitting）"><a href="#7-1-过拟合问题（overfitting）" class="headerlink" title="7.1 过拟合问题（overfitting）"></a><span id="7.1">7.1 过拟合问题（overfitting）</span></h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/37.jpg'  width="70%" height="70%"/ loading="lazy">

<ul>
<li><p><strong>过拟合（overfit）</strong>：过于贴近训练数据的特征了，在训练集上近乎完美的预测&#x2F;区分了所有的数据，但是<strong>缺乏泛化能力</strong>，在新的测试集上表现不佳。</p>
</li>
<li><p><strong>欠拟合（underfit）</strong>：测试样本的特性没有学习到，或模型过于简单无法拟合或区分样本。</p>
</li>
</ul>
<p>(<strong>识别过拟合、欠拟合的情况。</strong>)</p>
<p>当 <strong>features 的数量太多</strong>，甚至比 training data 的数量都多，就很有可能出现过拟合。比如像窗户的数量、门的数量等特征，看上去都与房屋的价格有关。</p>
<p><strong>解决过拟合问题的方法</strong>：</p>
<ul>
<li><strong>减少特征的数量</strong><ul>
<li>人工选择特征，进行取舍。舍弃特征的同时也会舍弃信息。</li>
<li>使用模型选择算法，自动选择保留的特征。（在之后介绍）</li>
</ul>
</li>
<li><strong>正则化（regularization）</strong><ul>
<li>保留所有特征，但减少参数 $\theta_j$ 的量级&#x2F;大小。</li>
<li>情况：有多个特征、并且每个特征都是或多或少有用的，我们不希望把他们舍弃掉。</li>
</ul>
</li>
</ul>
<h3 id="7-2-正则化的直观理解、代价函数（cost-function）"><a href="#7-2-正则化的直观理解、代价函数（cost-function）" class="headerlink" title="7.2 正则化的直观理解、代价函数（cost function）"></a><span id="7.2">7.2 正则化的直观理解、代价函数（cost function）</span></h3><h4 id="正则化的直观理解："><a href="#正则化的直观理解：" class="headerlink" title="正则化的直观理解："></a>正则化的直观理解：</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/38.jpg'  width="60%" height="60%"/ loading="lazy">

<p>通过减小参数 θ<sub>3</sub>、θ<sub>4</sub> 的值，给 x<sub>3</sub>、x<sub>4</sub> 两个特征加入惩罚，曲线就跟二次函数没什么区别了。</p>
<p>这就是正则化背后思想：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/39.jpg'  width="60%" height="60%"/ loading="lazy">

<p>不知道具体哪些特征 x 该舍弃掉，就<strong>减小所有参数 θ</strong> 。</p>
<h4 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h4><p>在 $J(\theta)$ 后面加上一项 $\lambda\sum^{n}_{i&#x3D;1}\theta_j^2$ ，在收敛 $J(\theta)$ 的同时让所有 $\theta$ 变小。</p>
<p>tips：</p>
<ul>
<li>通常不含常数项 $\theta_0$ ，但影响不大。</li>
<li>要<strong>选择合适的正则化参数 $\lambda$<strong>。$\lambda$ 的作用是在两个目标中平衡：表达式前面的项代表</strong>对数据更好的拟合</strong>，后面的项代表<strong>让参数尽量小来避免过拟合</strong>。<ul>
<li>$\lambda$ 越大，$\theta$ 的惩罚越大。如果 $\lambda$ 太大，$\theta$ 都接近0，就相当于把假设函数的项都忽略掉了，最后只剩一个常数项 $\theta _0$，造成欠拟合。</li>
</ul>
</li>
</ul>
<p>(问题：<strong>如何自动选择 $\lambda$</strong> ？)</p>
<h3 id="7-3-线性回归的正则化（regularized-linear-regression）"><a href="#7-3-线性回归的正则化（regularized-linear-regression）" class="headerlink" title="7.3 线性回归的正则化（regularized linear regression）"></a><span id="7.3">7.3 线性回归的正则化（regularized linear regression）</span></h3><p>拟合线性回归模型的两种算法：<strong>基于梯度下降、基于正规方程</strong>。</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>在 $J(\theta)$ 的最后加一项 $\frac{\lambda}{2m}\sum^n_{j&#x3D;1}\theta^2_j$ 。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/40.jpg'  width="60%" height="60%"/ loading="lazy">

<p>梯度下降，求偏导 $\frac{\lambda}{m}\theta_j$ ：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/41.jpg'  width="60%" height="60%"/ loading="lazy">

<p>直观的理解，正则化的梯度下降是让 $\theta_j$ <strong>每次额外乘以一个比1略小的数</strong> $(1-\alpha\frac{\lambda}{m})$ ，每次都把参数改小一点。</p>
<p>将 θ<sub>0</sub> 和其余 θ 区别开。（$(1-\alpha\frac{\lambda}{m})$ 跟1差不多大，所以区分开的影响不大。可以对比两个式子看正则化给梯度下降带来的变化。）</p>
<h4 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h4><p>回忆一下正规方程：构造 m*(n+1) 矩阵 X 和 m 维向量 y ，设定代价函数的导数为 0，用最小二乘法计算 θ。</p>
<p>正规方程正则化的方式是加一个 (n+1)*(n+1) 的方阵，这个方阵的构造如下图所示。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/42.jpg'  width="60%" height="60%"/ loading="lazy">

<p>*<strong>正则化中的 $X^TX$不可逆问题（见<a href="#4.7">4.7</a>）</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/43.jpg'  width="60%" height="60%"/ loading="lazy">

<p>在不进行正则化时，一些语言的库函数也可以<strong>计算不可逆的矩阵</strong>。</p>
<p>进行正则化后，如果 $\lambda&gt;0$ ，可证得 $(X^TX+\lambda R)$ <strong>一定是可逆的</strong>。</p>
<h3 id="7-4-逻辑回归的正则化（regularized-logistic-regression）"><a href="#7-4-逻辑回归的正则化（regularized-logistic-regression）" class="headerlink" title="7.4 逻辑回归的正则化（regularized logistic regression）"></a><span id="7.4">7.4 逻辑回归的正则化（regularized logistic regression）</span></h3><p>跟线性回归的梯度下降差不多。</p>
<p>在 $J(\theta)$ 的最后加一项 $\frac{\lambda}{2m}\sum^n_{j&#x3D;1}\theta^2_j$ 。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/44.jpg'  width="60%" height="60%"/ loading="lazy">

<p>梯度下降求偏导 $\frac{\lambda}{m}\theta_j$ ：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/45.jpg'  width="60%" height="60%"/ loading="lazy">

<p>实现方法：先定义代价函数：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/46.jpg'  width="60%" height="60%"/ loading="lazy">

<p>再把代价函数用到fminunc：<code>fminunc(@costFunction);</code> 。</p>
<hr>
<h2 id="第八章-神经网络：表述（Neural-Networks-Representation）"><a href="#第八章-神经网络：表述（Neural-Networks-Representation）" class="headerlink" title="第八章 神经网络：表述（Neural Networks: Representation）"></a>第八章 神经网络：表述（Neural Networks: Representation）</h2><h3 id="8-1-非线性假设"><a href="#8-1-非线性假设" class="headerlink" title="8.1 非线性假设"></a><span id="8.1">8.1 非线性假设</span></h3><p>Q：为什么要学习神经网络算法？（跟线性回归、逻辑回归相比有什么先进性）</p>
<p>A：如果<strong>n太大，feature太多</strong>，之前介绍的算法就不理想了。 对于许多实际的机器学习问题，n一般是很大的。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/47.jpg'  width="60%" height="60%"/ loading="lazy">

<p>如图，100个n会产生约5000个二次项，对应同样多的参数。如果包含所有的二次项，运算量会很大，并且很可能会出现过拟合的现象。如果再包含三次项，……。当<strong>n很大</strong>，通过增加feature来建立非线性分类器不是一个好办法。</p>
<p>而现实中的问题往往有很大的n。如50*50分辨率的灰度图像，要存储每个像素的值，n&#x3D;50*50&#x3D;2500；进行特征映射，n&#x3D;2500*2500&#x2F;2≈3,000,000。</p>
<h3 id="8-2-神经元和大脑"><a href="#8-2-神经元和大脑" class="headerlink" title="8.2 神经元和大脑"></a><span id="8.2">8.2 神经元和大脑</span></h3><p>可以用单个算法来模拟大脑的学习算法吗？</p>
<p>本节课举例论证了：人的一些感官是相通的——可以把任何sensor接入大脑，然后大脑的学习算法就能找出学习数据的方法，并处理这些数据。</p>
<p>现在的问题：如何找出大脑的学习算法？</p>
<h3 id="8-3-模型表示Ⅰ"><a href="#8-3-模型表示Ⅰ" class="headerlink" title="8.3 模型表示Ⅰ"></a><span id="8.3">8.3 模型表示Ⅰ</span></h3><p>当使用神经网络时，如何表示我们的假设或模型？大脑里的神经元是相互连接的，通过接收、处理、传递电信号的方式工作 。</p>
<p>通过以下方式表示单个的人工神经元：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/48.jpg'  width="80%" height="80%"/ loading="lazy">

<p>模型的表示：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/49.jpg'  width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>用 $a_i^{(j)}$ 表示第 $j$ 层的第 $i$ 个神经元。</p>
</li>
<li><p>用 $\Theta^{(j)}$ 表示从第 $j$ 层到 $j+1$  层的权重矩阵。</p>
</li>
<li><p>如果一个网络，在第 $j$ 层有 $s_j$ 个单元，在第 $j+1$ 层有 $s_{j+1}$ 个单元，那么 $\Theta^{(j)}$ 的维度是 $s_{j+1}×(s_j+1)$。是<strong>从后往前</strong>的形式。</p>
<ul>
<li>我的理解：矩阵的行是下一层的每个神经元的权重列表，列对应上一层的神经元。列要加一是<strong>加上上一层的偏置</strong>，也就是$x_0$。</li>
</ul>
</li>
</ul>
<h3 id="8-4-模型表示Ⅱ：向量化和前向传播"><a href="#8-4-模型表示Ⅱ：向量化和前向传播" class="headerlink" title="8.4 模型表示Ⅱ：向量化和前向传播"></a><span id="8.4">8.4 模型表示Ⅱ：向量化和前向传播</span></h3><p><em>本章直观地理解向量化的方法，并明白为什么这是学习复杂的非线性假设函数的好方法。</em></p>
<p>向前传播的向量化：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/50.jpg'  width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>$z^{(j+1)} &#x3D; \Theta^{(j)}a^{(j)}$，每层的 $z:(s_{j+1}×1)$ 是偏置矩阵 $\Theta:(s_{j+1}×(s_j+1))$ 和上层的输出 $a:((s_j+1)×1)$ 做矩阵运算得出的。</p>
</li>
<li><p>$a^{(j)} &#x3D; g(z^{(j)})$，每行的输出 $a$ 是对 $z$ 做 sigmoid 运算。</p>
</li>
<li><p>每一层都添加偏置  $a_0^{(j)}&#x3D;1$</p>
</li>
</ul>
<p>这种前向传播的方法也可以帮助我们了解神经网络的作用，和神经网络算法为什么能在学习非线性假设函数时有好的表现。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/51.jpg'  width="80%" height="80%"/ loading="lazy">

<p>神经网络在每一层都像是做一个逻辑回归，根据输入拟合一些权值；而且每层的输入都是前层根据自动拟合的权值，计算得到的输出，因此可以包含一些很复杂的特征。神经网络可以利用隐藏层计算更复杂的特征，并最终输出到输出层。</p>
<p><em>在接下来的两章，讨论具体的例子，描述如何利用神经网络来计算输入的非线性假设函数。</em></p>
<h3 id="8-5-举例直观理解Ⅰ"><a href="#8-5-举例直观理解Ⅰ" class="headerlink" title="8.5 举例直观理解Ⅰ"></a><span id="8.5">8.5 举例直观理解Ⅰ</span></h3><p>使用神经网络计算 AND 门：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/52.jpg'  width="80%" height="80%"/ loading="lazy">

<p>计算 OR 门：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/53.jpg'  width="80%" height="80%"/ loading="lazy">

<p>只要设置恰当的权值，神经网络就能起到相应的作用。</p>
<h3 id="8-6-举例直观理解Ⅱ"><a href="#8-6-举例直观理解Ⅱ" class="headerlink" title="8.6 举例直观理解Ⅱ"></a><span id="8.6">8.6 举例直观理解Ⅱ</span></h3><p>使用感知机来解决非线性问题：亦或。</p>
<p>使用有一层隐藏层的神经网络计算 NOR 函数：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/54.jpg'  width="80%" height="80%"/ loading="lazy">

<p>x<sub>1</sub> NOR x<sub>2</sub> &#x3D; (x<sub>1</sub> AND x<sub>2</sub>)    OR    ( (NOT x<sub>1</sub>) AND (NOT x<sub>2</sub>) )</p>
<p>通过识别手写数字的视频展示了，神经网络可以学习相当复杂的函数，并且有一定的抗干扰能力：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/55.jpg'  width="60%" height="60%"/ loading="lazy">



<h3 id="8-7-神经网络的多元分类问题（multi-class-clasification）"><a href="#8-7-神经网络的多元分类问题（multi-class-clasification）" class="headerlink" title="8.7 神经网络的多元分类问题（multi-class clasification）"></a><span id="8.7">8.7 神经网络的多元分类问题（multi-class clasification）</span></h3><p>上节的手写数字识别问题就是多分类问题，需要识别10种类型的数字。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/56.jpg'  width="80%" height="80%"/ loading="lazy">

<p>方法是设置多个输出层神经元，再把训练集 $(x^{(i)},y^{(i)})$ 、输出 $h_\Theta(x^{(i)})&#x3D;y^{(i)}$ 的 $y$ 都变成向量的形式。途中输出分成四类，则 $y$ 是四维向量。</p>
<p><em>本章讲述了如何构建模型来表示神经网络算法。在下一章，将学习如何构建训练集，如何让神经网络自动学习参数。</em></p>
<h2 id="第九章-神经网络的学习（Neural-Networks-Learning）"><a href="#第九章-神经网络的学习（Neural-Networks-Learning）" class="headerlink" title="第九章 神经网络的学习（Neural Networks: Learning）"></a>第九章 神经网络的学习（Neural Networks: Learning）</h2><p>讲一个学习算法，可以在给定数据集上，为神经网络拟合参数。</p>
<h3 id="9-1-代价函数（cost-function）"><a href="#9-1-代价函数（cost-function）" class="headerlink" title="9.1 代价函数（cost function）"></a><span id="9.1">9.1 代价函数（cost function）</span></h3><h4 id="神经网络的符号表示："><a href="#神经网络的符号表示：" class="headerlink" title="神经网络的符号表示："></a>神经网络的符号表示：</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/57.jpg'  width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>$L$：层数</p>
</li>
<li><p>$s_l$：第 $l$ 层的神经元数（不含偏置神经元）</p>
</li>
<li><p>$K$：输出神经元的个数。</p>
<ul>
<li>二元神经网络的 $s_l&#x3D;K&#x3D;1$，多元神经网络的 $s_l&#x3D;K$。</li>
<li>$h_\Theta(x) ∈ \R^K$</li>
</ul>
</li>
</ul>
<h4 id="神经网络的代价函数："><a href="#神经网络的代价函数：" class="headerlink" title="神经网络的代价函数："></a>神经网络的代价函数：</h4><p>逻辑回归的代价函数一般表达式如下：</p>
<p>$$J(\theta) &#x3D; -\frac{1}{m}\bigg[\sum_{i&#x3D;1}^{m}y^{(i)} \log h_\theta(x^{(i)})+(1-y^{(i)}) \log(1-h_\theta(x^{(i)}))\bigg]+\frac{\lambda}{2m}\sum_{j&#x3D;1}^n\theta^2_j$$<br>对于神经网络，有</p>
<p>$$h_\Theta(x)∈\R^K,(h_\Theta(x))_i&#x3D;i^{th} output$$</p>
<div>
$$
J(\Theta) = -\frac{1}{m}\bigg[\sum_{i=1}^{m}\sum_{k=1}^{K}y^{(i)}_k \log (h_\Theta(x^{(i)}))_k+(1-y^{(i)}_k) \log(1-h_\Theta(x^{(i)}))_k\bigg] \\
+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta^{(l)}_{ji})^2
$$
</div>

<p>与逻辑回归的代价函数不同点在于：</p>
<ul>
<li><p>把 K 个输出神经元的损失加起来，再求 m 个数据上的平均</p>
</li>
<li><p>正则化项，取所有边权重的平方和。i 从1开始，不含偏移神经元的权重。</p>
</li>
</ul>
<h3 id="9-2-反向传播算法（backpropagation-algorithm）"><a href="#9-2-反向传播算法（backpropagation-algorithm）" class="headerlink" title="9.2 反向传播算法（backpropagation algorithm）"></a><span id="9.2">9.2 反向传播算法（backpropagation algorithm）</span></h3><p><em>一个让上节的损失函数取到最小值的算法</em></p>
<p>上节定义了 $J(\Theta)$，如果想让 $J(\Theta)$ 取到最小值，就需要计算 $\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)$</p>
<p>首先来看<strong>前向传播的向量化</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/58.jpg'  width="80%" height="80%"/ loading="lazy">

<p>此处可以仔细理解一下<strong>前向传播的计算过程</strong>。</p>
<ul>
<li>以上图为例，如果只有一组数据集 $(x, y)$，</li>
<li>$a^{(1)}$的规格是 3×1，$\Theta^{(1)}$的规格是 5×3，</li>
<li>向量化后相乘，得到 $z^{(2)}$的规格是 5×1，求sigmoid后加一个偏移项则 $a^{(2)}$的规格是 6×1，</li>
<li>$\Theta^{(2)}$的规格是 5×6，相乘得到 $z^{(2)}$的规格是 5×1，</li>
<li>以此类推。使用前向传播，可以从输入，通过神经网络，得到输出。</li>
</ul>
<p>回到主题，为了计算导数项 $\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)$，需要使用<strong>反向传播算法</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/59.jpg'  width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>对于每一个节点，我们计算这样一项 $\delta^{(l)}_j$，<strong>代表了第 $l$ 层第 $j$ 个节点的“误差”</strong></p>
</li>
<li><p><strong>从输出层，反向计算每层的误差</strong>。</p>
</li>
<li><p>然后，根据 $\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta) &#x3D; a^{(l)}_j\delta^{(l+1)}_i$（此式省略了 $\lambda$ 等参数），可以<strong>求得想要的偏导数</strong>。</p>
</li>
</ul>
<h4 id="反向传播算法的整体过程"><a href="#反向传播算法的整体过程" class="headerlink" title="反向传播算法的整体过程"></a>反向传播算法的整体过程</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/60.jpg'  width="80%" height="80%"/ loading="lazy">


<ul>
<li><p>有训练集 ${( x^{(1)},y^{(1)} ),…,(x^{(m)},y^{(m)})   }$</p>
</li>
<li><p><strong>设置初始误差矩阵</strong> $\Delta^{(l)}_{ij}&#x3D;0$，保存整个神经网络的误差</p>
</li>
<li><p>For $i&#x3D;1$ to $m$ ：</p>
<ul>
<li><strong>设置输入层</strong> $a^{(1)}&#x3D;x^{(1)}$</li>
<li>进行<strong>前向传播</strong>，求<strong>每层输出</strong> $a^{(l)}$</li>
<li>使用 $y^{(i)}$ ，计算<strong>输出层的误差</strong>  $\delta^{(L)} &#x3D;a^{(L)}-y^{(i)}$</li>
<li>进行<strong>反向传播</strong>，计算<strong>每层误差</strong> $\delta^{(L-1)},\delta^{(L-2)},…,\delta^{(2)}$</li>
<li><strong>累加误差矩阵</strong> $\Delta^{(l)}_{ij}:&#x3D;\Delta^{(l)}_{ij}+ a^{(l)}_j\delta^{(l+1)}_i$</li>
</ul>
</li>
<li><p>计算 $D$</p>
<ul>
<li>$D^{(l)}_{ij}:&#x3D;\frac{1}{m}\Delta^{(l)}_{ij}+\lambda\Theta^{(l)}_{ij}$，if $j ≠0$</li>
<li>$D^{(l)}_{ij}:&#x3D;\frac{1}{m}\Delta^{(l)}_{ij}$               ，if $j &#x3D;0$</li>
</ul>
</li>
<li><p>$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta) &#x3D; D^{(l)}_{ij}$ ，求得偏导</p>
</li>
<li><p>使用偏导进行梯度下降，或其他高级优化算法</p>
</li>
</ul>
<p>补充：</p>
<ul>
<li><p>反向传播不用计算 $\delta^{(1)}$ ，因为不需要对输入层考虑误差项。</p>
</li>
<li><p>累加误差矩阵写成向量相乘形式： $\Delta^{(l)}_{ij}:&#x3D;\Delta^{(l)}_{ij}+ \delta^{(l+1)}_i(a^{(l)}_j)^T$</p>
</li>
</ul>
<h3 id="9-3-反向传播算法的直观理解"><a href="#9-3-反向传播算法的直观理解" class="headerlink" title="9.3 反向传播算法的直观理解"></a><span id="9.3">9.3 反向传播算法的直观理解</span></h3><p>前向传播的直观理解：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/61.jpg'  width="80%" height="80%"/ loading="lazy">

<p>反向传播的直观理解：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/62.jpg'  width="80%" height="80%"/ loading="lazy">

<p>可以把神经网络的代价函数中的主要部分，类比为线性回归的代价函数的方差计算。只需明确：本质上做的是“求与真实值y的偏离程度”这件事。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/63.jpg'  width="90%" height="90%"/ loading="lazy">

<p>而反向传播过程中，计算的 $\delta_j^{(l)}$ 就是 <strong>cost 关于 z 的偏导数</strong>。具体来说，cost 是一个关于真实值 y 和神经网络的输出值 h(x) 的函数。 $\delta_j^{(l)}$实际上是 cost 关于这些计算出的中间项的偏导数。$\delta_j^{(l)}$衡量的是，为了影响这些<strong>中间值 z <strong>（进而影响</strong>整个神经网络的输出 h</strong>），我们想要改变的神经网络的<strong>权重</strong>的程度。</p>
<h3 id="9-4-使用注意：展开参数"><a href="#9-4-使用注意：展开参数" class="headerlink" title="9.4 使用注意：展开参数"></a><span id="9.4">9.4 使用注意：展开参数</span></h3><p><em>怎样将参数从矩阵展开成向量，以满足高级最优化步骤中的使用需要</em></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/64.jpg'  width="70%" height="70%"/ loading="lazy">

<p>如上图，定义一个代价函数 costFunction，输入参数是 theta，函数返回代价值 jVal 以及导数值（梯度） gradient。</p>
<p>然后将这个函数传递给高级最优化算法 fminunc，这些库函数都假定 theta、initialTheta 是参数向量，同时假定代价函数的第二个返回值，也就是梯度值 gradient 也是一个向量。</p>
<p>这部分在我们使用逻辑回归的时候没有问题，但在神经网络中，参数矩阵 $\Theta$ 和梯度矩阵$D$ 都是矩阵而非向量，因此需要将这些矩阵展开成向量，从而作为参数输入到函数中。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/65.jpg'  width="70%" height="70%"/ loading="lazy">

<p>以上图 Octave 语言为例：</p>
<ul>
<li>首先有初始参数值矩阵 $\Theta$ ，<strong>将他们展开成向量 initialTheta</strong>，传递到高级优化函数 fminunc 中。</li>
<li>使用 fminunc 函数需要事先定义 costFunction 函数，它的参数是 thetaVec，所有的参数展开成一个向量的形式。因此需要做的第一件事就是用 reshape 功能<strong>将 thetaVec 转换成参数值矩阵</strong> $\Theta$，然后才能进行前向、反向传播，来计算导数 $D$ 和计算代价函数 $J(\Theta)$。最终，<strong>将 $D$ 展开成向量</strong>，得到需要返回的梯度向量 gradientVec。</li>
</ul>
<p>一般是用 reshape() 函数来进行矩阵、向量形式的转换。</p>
<ul>
<li>用矩阵存储的好处：更方便进行正向、反向传播。</li>
<li>用向量存储的好处：一些高级优化算法要求参数要展开成一个长向量的格式。</li>
</ul>
<h3 id="9-5-梯度检验（gradient-checking）"><a href="#9-5-梯度检验（gradient-checking）" class="headerlink" title="9.5 梯度检验（gradient checking）"></a><span id="9.5">9.5 梯度检验（gradient checking）</span></h3><p><em>前面几节讲了怎样在神经网络中进行前向、反向传播，来计算导数。但反向传播算法很难实现，并且在执行上可能出现一些bug，比如 $J(\Theta)$ 减小但最终得到的神经网络误差很大。</em></p>
<p><em>梯度检验思想能解决几乎所有这种问题，能完全保证前向、反向传播算法是百分百正确的。在使用反向传播的场合中，最好做一下梯度检验。</em></p>
<h4 id="第一步：估算导数"><a href="#第一步：估算导数" class="headerlink" title="第一步：估算导数"></a>第一步：估算导数</h4><p><strong>当 $\Theta$ 是实数：</strong></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/66.jpg'  width="70%" height="70%"/ loading="lazy">

<p>在数值上去拟合 $J(\Theta)$ 的导数：</p>
<div>
$$
\frac{d}{d\Theta}≈\frac{J(\Theta+\epsilon)-J(\Theta-\epsilon)}{2\epsilon}
$$
</div>

<p><strong>当 $\Theta$ 是向量：</strong>（比如是从矩阵展开而来的）</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/67.jpg'  width="70%" height="70%"/ loading="lazy">

<p>在Octave的实现：</p>
<pre class="language-octave" data-language="octave"><code class="language-octave">for i &#x3D; 1:n,
	thetaPlus &#x3D; theta;
	thetaPlus(i) &#x3D; thetaPlus(i) + EPSILON;
	thetaMinus &#x3D; theta;
	thetaMinus(i) &#x3D; thetaMinus(i) - EPSILON;
	gradApprox(i) &#x3D; (J(thetaPlus) - J(thetaMinus)) &#x2F; (2 * EPSILON);
end;</code></pre>

<p>用 <code>gradApprox</code> 来近似计算 $\frac{\partial}{\partial\Theta_i}J(\Theta)$</p>
<h4 id="第二步：对比估算值和反向传播的结果"><a href="#第二步：对比估算值和反向传播的结果" class="headerlink" title="第二步：对比估算值和反向传播的结果"></a>第二步：对比估算值和反向传播的结果</h4><p>检查 <code>gradApprox ≈ Dvec</code></p>
<p><code>DVec</code> 是反向传播得出的导数矩阵。检查估算的 <code>gradApprox</code> 是否在数值上跟反向传播算法计算出的导数接近。</p>
<h4 id="总体实现过程："><a href="#总体实现过程：" class="headerlink" title="总体实现过程："></a>总体实现过程：</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/68.jpg'  width="70%" height="70%"/ loading="lazy">

<p>在第三步中，如果估计值和反向传播的结果在数值上近似，那么第四步，就<strong>把梯度检验关掉，不要再计算估计值</strong> <code>gradApprox</code> 了，使用反向传播的结果来进行神经网络的学习。</p>
<p>原因是近似求导计算 <code>gradApprox</code> 是计算量非常大、耗时、不稳定的。而计算 <code>DVec</code> 的反向传播算法是高性能的求导方法（仅向量计算）。</p>
<h3 id="9-6-随机初始化（random-initialization）"><a href="#9-6-随机初始化（random-initialization）" class="headerlink" title="9.6 随机初始化（random initialization）"></a><span id="9.6">9.6 随机初始化（random initialization）</span></h3><p><em>如何初始化 $\Theta$ ？</em></p>
<p>在逻辑回归中，可以将初始参数都设置为 0 .但在训练网络时，将 0 作为初始值起不到任何效果：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/69.jpg'  width="70%" height="70%"/ loading="lazy">

<p>由于权重是相同的（都是0），在前向传播中，同层的节点将会得到相同的输出值；而在反向传播中，也会得到相同的导数值。如图，相同颜色的边会得到相同的权值、偏导。这被称为“对称权重问题（symmetric weights）”</p>
<p>为了解决这个问题，使用随机初始化的思想，进行 symmetry breaking。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/70.jpg'  width="70%" height="70%"/ loading="lazy">

<p>rand取 [0,1]，(2*rand - 1)*EPSILON 取 [-EPSILON, EPSILON].</p>
<p>综合前几节，训练神经网络的过程如下：</p>
<ul>
<li>随机初始化权重于 $[-\epsilon,\epsilon]$</li>
<li>进行反向传播</li>
<li>进行梯度检验</li>
<li>使用梯度下降或其他高级优化算法，最小化 $J(\Theta)$</li>
<li>得出 $\Theta$ 的最优值</li>
</ul>
<h3 id="9-7-组合起来"><a href="#9-7-组合起来" class="headerlink" title="9.7 组合起来"></a><span id="9.7">9.7 组合起来</span></h3><h3 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h3><ul>
<li><h4 id="确定一个网络结构"><a href="#确定一个网络结构" class="headerlink" title="确定一个网络结构"></a>确定一个网络结构</h4><ul>
<li><strong>输入层</strong>单元的个数：数据集中 $x^{(i)}$ 特征的维度</li>
<li><strong>输出层</strong>单元的个数：分类的个数<ul>
<li>对于分类问题，要记得<strong>把输出 y 写成向量的形式</strong>，其中有一项 1 表示哪一类，剩下都为0.</li>
</ul>
</li>
<li><strong>隐藏层</strong>如何设计：<ul>
<li>一个合理的默认选项是使用一个隐藏层。</li>
<li>使用多个隐藏层时，每层的隐藏单元数量相同。（通常越多越好，但要注意计算量问题）</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="需要实现的步骤"><a href="#需要实现的步骤" class="headerlink" title="需要实现的步骤"></a>需要实现的步骤</h4><ul>
<li>① 随机初始化权重</li>
<li>② 实现前向传播算法，计算每条数据 $x^{(i)}$ 经过神经网络得到的输出 $h_\Theta(x^{(i)})$</li>
<li>③ 实现损失函数 $J(\Theta)$</li>
<li>④ 实现反向传播算法，计算偏导 $\frac{\partial}{\partial\Theta^{(i)}_{jk}}J(\Theta)$<ul>
<li>使用一个循环 <code>for i = 1:m</code> 来进行前向传播和反向传播，对于数据集中的每条数据 $(x^{(i)},y^{(i)})$ 计算每层的 $a^{(l)}$ 和 $\delta^{(l)}$ ；然后在循环中更新 $\Delta^{(l)}$</li>
</ul>
</li>
<li>⑤ 实现梯度检验，对比反向传播的结果 $\frac{\partial}{\partial\Theta^{(l)}_{jk}}J(\Theta)$ 和数值上的估计值。然后禁用梯度检验的代码。</li>
<li>⑥ 使用梯度下降算法，跟反向传播结合，最小化 $J(\Theta)$，得到最终权重 $\Theta$</li>
</ul>
</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/71.jpg'  width="70%" height="70%"/ loading="lazy">



<ul>
<li><h4 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h4></li>
</ul>
<p>如下图，① 随机选取一个起始点，④ 反向传播算出梯度下降的方向，⑥ 梯度下降就是沿着这个方向一点点下降，知道到达局部最优点。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/72.jpg'  width="70%" height="70%"/ loading="lazy">



<h3 id="9-8-无人驾驶"><a href="#9-8-无人驾驶" class="headerlink" title="9.8 无人驾驶"></a><span id="9.8">9.8 无人驾驶</span></h3><p><em>一个有趣并具有历史意义的神经网络学习的例子：实现自动驾驶</em></p>
<p>左上方第一个白条显示人类驾驶员选择的方向，第二、三个白条显示两个网络的置信度、以及学习算法选择的行驶方向。</p>
<p>刚开始随机初始化后，输出整段模糊的灰色区域，学习后才集中到一块白亮的小区域，选择明确的驾驶方向。</p>
<p>左下方显示前方景象。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/73.jpg'  width="70%" height="70%"/ loading="lazy">

<p>这个任务基于三层的神经网络，使用反向传播算法。数据集是压缩后的前方图片和驾驶员选择的方向。</p>
<p>训练完成后，每秒钟生成12张数字化的图片，传送给神经网络进行计算，使用输出进行自动驾驶。</p>
<p>分为单行道网络、双车道网络，分别计算置信度（confidence），根据不同的路况，置信度高的网络就被选择来控制行驶方向。</p>
<hr>
<h2 id="第十章-应用机器学习的建议-Advice-for-Applying-Machine-Learning"><a href="#第十章-应用机器学习的建议-Advice-for-Applying-Machine-Learning" class="headerlink" title="第十章 应用机器学习的建议(Advice for Applying Machine Learning)"></a>第十章 应用机器学习的建议(Advice for Applying Machine Learning)</h2><p>给出一些建议，当在开发一个机器学习系统时，如何决定该选择哪条道路。</p>
<p>10.2、10.3讲怎样评估机器学习算法的性能，10.4、10.5讲机器学习诊断法。</p>
<h3 id="10-1-决定下一步做什么"><a href="#10-1-决定下一步做什么" class="headerlink" title="10.1 决定下一步做什么"></a><span id="10.1">10.1 决定下一步做什么</span></h3><p><strong>Debugging a learning algorothm：</strong></p>
<p>假设已经实现了正则化的线性回归，得到用来预测房价的模型：</p>
<div>
$$
J(\theta) = \frac{1}{2m}\bigg[\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2 + \lambda\sum^m_{j=1}\theta^2_j\bigg]
$$
</div>

<p>然而，当在训练集上测试得到的参数时，我们发现预测的结果有很大的误差。我们应该如何改进？</p>
<ul>
<li>收集更多的训练样本（在有些时候没有效果。在之后的章节会将如何避免把过多时间浪费在收集样本上）</li>
<li>尝试使用更少的特征</li>
<li>尝试获取更多特征，来收集更多的数据</li>
<li>尝试增加多项式特征（$x^2_1,x^2_2,x_1x_2,etc.$）</li>
<li>尝试增加&#x2F;减小 $\lambda$</li>
</ul>
<p>大多数人的方法是，随便从这些方法中选择一种，然后花很长时间检验这种方法是否有效。</p>
<p>有一种简单的方法，可以轻松地排除掉一些选项：机器学习诊断法（machine learning diagnostic）。我们可以使用诊断法，明确在机器学习算法中，什么有效什么无效，并且可以获得如何提升性能的指导。</p>
<h3 id="10-2-评估一个假设"><a href="#10-2-评估一个假设" class="headerlink" title="10.2 评估一个假设"></a><span id="10.2">10.2 评估一个假设</span></h3><p><em>如何评估学习算法得到的假设。基于这一节，在之后会讲如何防止过拟合和欠拟合的问题。</em></p>
<p>我们获得学习算法的方式是让代价函数取最小，但会产生<strong>过拟合假设</strong>的问题。当在特征少的情况下，我们可以通过函数图像发现过拟合现象，但<strong>当特征多时，函数难以可视化，我们需要另一种评价假设函数的方法</strong>。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/74.jpg'  width="70%" height="70%"/ loading="lazy">

<p>把数据集随机分为训练集和测试集。数量上 7:3 是比较常见的分法。</p>
<p>一种典型的训练、测试线性回归的方法：</p>
<ul>
<li>在 70% 占比的训练集上学习 $\theta$（最小化误差函数$J(\theta)$）</li>
<li>计算出测试误差：$J_{test}(\theta) &#x3D;\frac{1}{2m}\sum^{m_{test}}_{i&#x3D;1}(h_\theta(x^{(i)}_{test})-y^{(i)}_{test})^2$</li>
</ul>
<p>训练、测试逻辑回归：</p>
<ul>
<li>在训练集上学习 $\theta$</li>
<li>在测试集上计算测试误差，有两种定义方法：<ul>
<li>计算逻辑回顾误差：$J_{test}(\theta) &#x3D; -\frac{1}{m_{test}}\sum^{m_{test}}_{i&#x3D;1}y_{test}^{(i)}\log h_{\theta}(x^{(i)}_{test})+(1-y_{test}^{(i)})\log h_{\theta}(x^{(i)}_{test})$</li>
<li>错误分类误差（0&#x2F;1 misclassification erorr）：详见图。</li>
</ul>
</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/75.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="10-3-模型选择和交叉验证集"><a href="#10-3-模型选择和交叉验证集" class="headerlink" title="10.3 模型选择和交叉验证集"></a><span id="10.3">10.3 模型选择和交叉验证集</span></h3><p><em>如何评估学习算法：模型选择问题。</em></p>
<p><em>对于一个数据集，最合适的多项式次数如何确定；怎样选择合适的特征来构造学习算法；如何确定学习算法的正则化参数</em> $\lambda$</p>
<h4 id="模型选择Ⅰ"><a href="#模型选择Ⅰ" class="headerlink" title="模型选择Ⅰ"></a>模型选择Ⅰ</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/76.jpg'  width="80%" height="80%"/ loading="lazy">

<p>一种选择模型的方法：在多种特征的选择方法中，<strong>分别进行训练</strong>得出 $\Theta$，然后<strong>分别在测试集上进行测试</strong>，然后从这些模型中<strong>选出表现最好的一个</strong>（ $J_{test}(\Theta^{(i)})$ 最小）。</p>
<p>问题：判断这个模型的<strong>泛化能力</strong>如何？</p>
<p>我们可以观察这个假设模型对测试集的拟合情况，但问题是，这样做仍然不能公平地估计出这个假设的泛化能力。原因：参数 d 是多项式的次数（特征的维度），<strong>我们用测试集拟合了参数 d，选择了能够最好地拟合测试集的参数 d 的值</strong>。我们的参数向量 $\Theta$  在测试集上的性能很可能是对泛化误差过于乐观的估计。也就是说，<strong>测试集的误差 $J_{test}$ 让我们找出了最好的模型，但不能用同一个误差来衡量模型的泛化能力。</strong></p>
<h4 id="交叉验证集"><a href="#交叉验证集" class="headerlink" title="交叉验证集"></a>交叉验证集</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/77.jpg'  width="80%" height="80%"/ loading="lazy">

<p>将数据分为三部分：训练集60%（training set）、交叉验证集20%（cross validation set）和测试集20%（test set）。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/78.jpg'  width="80%" height="80%"/ loading="lazy">



<h4 id="模型选择Ⅱ"><a href="#模型选择Ⅱ" class="headerlink" title="模型选择Ⅱ"></a>模型选择Ⅱ</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/79.jpg'  width="80%" height="80%"/ loading="lazy">

<p>对于每种模型，训练出 $\Theta$，然后在<strong>交叉验证集</strong>上计算 $J_{cv}(\Theta)$，选择表现最好的参数 d，即选择某个模型。在<strong>测试集</strong>上进行误差的估计，计算  $J_{test}(\Theta)$，就可以用  $J_{test}(\Theta)$ 来衡量算法选出的模型的<strong>泛化能力</strong>了。 </p>
<h3 id="10-4-诊断偏差和方差（diagnosing-bias-vs-variance）"><a href="#10-4-诊断偏差和方差（diagnosing-bias-vs-variance）" class="headerlink" title="10.4 诊断偏差和方差（diagnosing bias vs. variance）"></a><span id="10.4">10.4 诊断偏差和方差（diagnosing bias vs. variance）</span></h3><p>当一个模型表现不好，主要有两种原因：高偏差（high bias，欠拟合）、高方差（high variance，过拟合）。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/80.jpg'  width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>多项式次数小，训练集上表现不佳，bias高。</p>
</li>
<li><p>多项式次数高，训练集上拟合得好，bias低；但交叉验证集上会过拟合，variance高。</p>
</li>
</ul>
<h4 id="如何确定模型为何表现不佳？"><a href="#如何确定模型为何表现不佳？" class="headerlink" title="如何确定模型为何表现不佳？"></a>如何确定模型为何表现不佳？</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/81.jpg'  width="80%" height="80%"/ loading="lazy">

<ul>
<li>Bias（欠拟合）：<ul>
<li>训练集上表现不好，$J_{train}(\Theta)$ 高</li>
<li>交叉验证集上表现不好，$J_{cv}(\Theta)≈J_{train}(\Theta)$</li>
</ul>
</li>
<li>Variance（过拟合）：<ul>
<li>训练集上表现很好，$J_{train}(\Theta)$ 低</li>
<li>交叉验证集上表现不好，$J_{cv}(\Theta)&gt;&gt;J_{train}(\Theta)$</li>
</ul>
</li>
</ul>
<p><em>接下来：对 bias 和 variance 的进一步解释，以及确定问题后应采取怎样的措施。</em></p>
<h3 id="10-5-正则化和偏差-x2F-方差"><a href="#10-5-正则化和偏差-x2F-方差" class="headerlink" title="10.5  正则化和偏差&#x2F;方差"></a><span id="10.5">10.5  正则化和偏差&#x2F;方差</span></h3><p><strong>正则化的线性回归</strong>，$\lambda$ 不同会得到不同的结果。太大会造成 bias，太小会造成 variance。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/82.jpg'  width="80%" height="80%"/ loading="lazy">

<p><strong>我们的模型</strong>：</p>
<ul>
<li>训练模型的过程中，添加正则项，得到 $\Theta$ 。</li>
<li>评估模型的过程中，只计算偏差，不进行优化。不用添加正则项。</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/83.jpg'  width="80%" height="80%"/ loading="lazy">

<p> <strong>自动选择</strong> $\lambda$：</p>
<ul>
<li><p>选取一系列想要尝试的取值，对于每个 $\lambda$ 分别让 $J(\Theta)$ 取到最小值，获得 $\Theta$</p>
</li>
<li><p>在交叉验证集上评估它们。选择让 $J_{cv}(\Theta^{(i)})$ 取到最小值的 $\lambda$ 。</p>
</li>
<li><p>在测试集上测试这个模型的表现。</p>
</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/84.jpg'  width="80%" height="80%"/ loading="lazy">



<p><strong>正则化的 bias&#x2F;variance：</strong></p>
<ul>
<li>$J_{train}$：$\lambda$ 小，过拟合，训练集误差小；$\lambda$ 大，欠拟合，训练集误差大。</li>
<li>$J_{cv}$：$\lambda$ 小，过拟合，交叉验证集误差大（模型扩展性差）；$\lambda$ 大，欠拟合，交叉验证集误差大。</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/85.jpg'  width="80%" height="80%"/ loading="lazy">

<p>取多个 $\lambda$ 值，绘制交叉验证集误差 $J_{cv}(\Theta)$ 的曲线。</p>
<p>在真实的数据集中，得到的曲线可能更乱，有很多的噪声。有时可以看出一个趋势，通过观察整个交叉验证误差曲线，手动或自动得出能使交叉验证误差最小的 “just right” 的 $\lambda$ 取值。</p>
<h3 id="10-6-诊断方法：学习曲线（learning-curves）"><a href="#10-6-诊断方法：学习曲线（learning-curves）" class="headerlink" title="10.6 诊断方法：学习曲线（learning curves）"></a><span id="10.6">10.6 诊断方法：学习曲线（learning curves）</span></h3><p>控制使用的训练样本数量，绘制小数据集的训练集误差、交叉验证集误差。</p>
<ul>
<li>$J_{train}$：当训练集很小，拟合起来比较容易。当训练集增大，误差就会增加。</li>
<li>$J_{cv}$：当训练集很小，模型泛化程度不会很好。使用大的训练集，模型会有更好的泛化表现。</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/86.jpg'  width="80%" height="80%"/ loading="lazy">



<p><strong>high bias 的学习曲线：</strong></p>
<ul>
<li>m 很小，拟合程度好（$J_{train}$小），泛化能力差（$J_{cv}$大）。</li>
<li>m 变大，$J_{cv}$ 和 $J_{train}$ 逐渐接近。当欠拟合时，训练集和交叉验证集上的表现都比较高，数值上相似。</li>
<li><strong>结论：如果模型处在 high bias  的情形，选用更多的训练集数据对于改善算法的表现无益。</strong></li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/87.jpg'  width="80%" height="80%"/ loading="lazy">



<p><strong>high variance 的学习曲线：</strong></p>
<ul>
<li>m 很小，拟合程度好（$J_{train}$小），泛化能力差（$J_{cv}$大）。</li>
<li>m 变大，拟合越来越难，但拟合程度也不错。泛化能力依然差。$J_{cv}$ 和 $J_{train}$ 逐渐接近。当欠拟合时，训练集和交叉验证集上的表现都比较高，数值上相似。</li>
<li><strong>结论：如果模型处在 high variance 的情形，选用更多的训练集数据，延长这两条曲线，$J_{cv}$ 和 $J_{train}$ 会逐渐接近。这对改善算法的表现是有好处的。</strong></li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/88.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="10-7-决定下一步做什么"><a href="#10-7-决定下一步做什么" class="headerlink" title="10.7 决定下一步做什么"></a><span id="10.7">10.7 决定下一步做什么</span></h3><p>回到<a href="#10.1">10.1</a>提出的改进方法：</p>
<table>
<thead>
<tr>
<th>改进方法</th>
<th>适用场合</th>
</tr>
</thead>
<tbody><tr>
<td>更多训练样本</td>
<td>high variance</td>
</tr>
<tr>
<td>更少特征</td>
<td>high variance</td>
</tr>
<tr>
<td>更多特征</td>
<td>high bias</td>
</tr>
<tr>
<td>更多多项式特征</td>
<td>high bias</td>
</tr>
<tr>
<td>增加 $\lambda$</td>
<td>high variance</td>
</tr>
<tr>
<td>减小 $\lambda$</td>
<td>high bias</td>
</tr>
</tbody></table>
<h4 id="跟神经网络的结合"><a href="#跟神经网络的结合" class="headerlink" title="跟神经网络的结合"></a>跟神经网络的结合</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/89.jpg'  width="80%" height="80%"/ loading="lazy">





<hr>
<h2 id="第十一章-机器学习系统设计-Machine-Learning-System-Design"><a href="#第十一章-机器学习系统设计-Machine-Learning-System-Design" class="headerlink" title="第十一章 机器学习系统设计(Machine Learning System Design)"></a>第十一章 机器学习系统设计(Machine Learning System Design)</h2><h3 id="11-1-确定优先执行的事情"><a href="#11-1-确定优先执行的事情" class="headerlink" title="11.1 确定优先执行的事情"></a><span id="11.1">11.1 确定优先执行的事情</span></h3><p>以垃圾邮件分类器为例：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/90.jpg'  width="80%" height="80%"/ loading="lazy">

<p>监督学习，x &#x3D; 邮件的特征，来表示一组词是否在邮件中出现。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/91.jpg'  width="80%" height="80%"/ loading="lazy">

<p>事实上，有时会随机决定去采用上面四种方法的其中一种。</p>
<h3 id="11-2-误差分析（error-analysis）"><a href="#11-2-误差分析（error-analysis）" class="headerlink" title="11.2 误差分析（error analysis）"></a><span id="11.2">11.2 误差分析（error analysis）</span></h3><p><em>在改进机器学习算法时，通常有很多不同的思想。通过误差分析，可以更系统地在众多方法中做出选择。</em></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/92.jpg'  width="80%" height="80%"/ loading="lazy">

<p>当在面对一个机器学习问题时，最好花很少的时间，<strong>先通过一个简单的算法，快速实现出来</strong>，而不是设计一个很复杂的系统。然后<strong>用交叉验证集来测试数据</strong>，画出学习曲线，进行检验误差，来确定模型是否存在高偏差或者高方差的问题，或其他问题。然后再决定是否要用更多的数据或者特征等。</p>
<p>可以把这种思想想成：在编程时避免出现过早优化的问题，我们应当<strong>用实际的证据来指导我们的决策</strong>，来决定把时间花在哪里，而不是仅凭直觉。</p>
<h4 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h4><p>误差分析：可以<strong>人工检查算法出错的地方</strong>，查看被分错了的邮件有什么共同的特征和规律。这个过程可 以启发我们怎样设计新特征，或告诉我们现有系统的优点和缺点。</p>
 <img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/93.jpg'  width="80%" height="80%"/ loading="lazy">

<h4 id="量化指标"><a href="#量化指标" class="headerlink" title="量化指标"></a>量化指标</h4><p>如果算法能够返回一个<strong>数值指标</strong>，来估计算法执行的效果，将会很有帮助。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/94.jpg'  width="80%" height="80%"/ loading="lazy">

<p>是否使用词干提取（stemming）？使用了会造成一些问题，不使用会忽略一些情况。</p>
<p>这时就可以使用<strong>交叉验证集上的错误率</strong>作为数值指标，来决定是否使用词干提取。</p>
<p>在此基础上，要不要区分大小写？也可以通过这个单一量化指标来轻松决定。</p>
<p><strong>注意：不要再测试集上做误差分析。要在交叉验证集上做误差分析。</strong></p>
<p><strong>注意：最先实现的算法，简单粗暴为主，快是第一指标，再烂都没问题。</strong></p>
<h3 id="11-3-类偏斜的误差度量（error-metrics-for-skewed-classes）"><a href="#11-3-类偏斜的误差度量（error-metrics-for-skewed-classes）" class="headerlink" title="11.3 类偏斜的误差度量（error metrics for skewed classes）"></a><span id="11.3">11.3 类偏斜的误差度量（error metrics for skewed classes）</span></h3><p>上节提到了误差分析和设定误差度量值的重要性，那就是设定某个实数来评估学习算法并衡量它的表现。但仅用准确率accuracy来评价模型，在一种条件下是不可靠的，这个问题就是偏斜类的问题。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/95.jpg'  width="80%" height="80%"/ loading="lazy">

<p>正样本的数量与负样本的数量相比，非常非常少，这种情况就叫偏斜类（skewed classes）。</p>
<p>精确度从99.2%提升到99.5%，不能说分类模型的质量提升了，因为有时模型恒定输出0，如果样本中y&#x3D;0的比率比模型的预测准确率高，也能带来数值上的提升。</p>
<p>对于偏斜类，需要有不同的误差度量值：</p>
<p><strong>查准率和查全率</strong></p>
<ul>
<li><strong>precision：查准率、精确率</strong>。在预测为1的样本中，有多少是预测正确的？（没错诊）</li>
<li><strong>recall：查全率、召回率</strong>。在真实为1的样本中，有多少是预测出来的？（没漏诊）</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/96.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="11-4-查准率和查全率之间的权衡（trading-off-precision-and-recall）"><a href="#11-4-查准率和查全率之间的权衡（trading-off-precision-and-recall）" class="headerlink" title="11.4 查准率和查全率之间的权衡（trading off precision and recall）"></a><span id="11.4">11.4 查准率和查全率之间的权衡（trading off precision and recall）</span></h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/97.jpg'  width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>给没有病的人通知得病是不好的。为了提高precision，也就是降低误诊率，可以把逻辑回归的阈值设置得高一些。</p>
<p>这会导致：higher precision（有更高的把我才预测得病），lower recall（容易漏诊）</p>
</li>
<li><p>没有给得病的人通知也是不好的。为了提高recall，也就是降低漏诊率，可以把逻辑回归的阈值设置得低一些。</p>
<p>这会导致：higher recall（有病的人容易被预测到），lower precision（没病的人也容易被预测到）。</p>
</li>
</ul>
<p>问题来了：有没有办法自动选取阈值？more generally，如果我们有不同的算法，如何比较不同的查准率和查全率？</p>
<p>在<a href="#10.2">10.2</a>，我们提出设置单个数值指标来评估模型的好坏。但现在我们有两个可以判断的数字 precision 和 recall。如何得到单个数值？</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/98.jpg'  width="80%" height="80%"/ loading="lazy">

<p>平均值是不行的。比如图中 Algorithm 3，恒输出y&#x3D;1，显然不是一个好模型。</p>
<p>F<sub>1</sub> 值（调和平均值）共同考虑了两个值，并且给小的值更高的权重。</p>
<h3 id="11-5-机器学习的数据"><a href="#11-5-机器学习的数据" class="headerlink" title="11.5 机器学习的数据"></a><span id="11.5">11.5 机器学习的数据</span></h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/99.jpg'  width="80%" height="80%"/ loading="lazy">

<p>实验证明：给算法更多的数据，模型的性能往往会变好。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/100.jpg'  width="80%" height="80%"/ loading="lazy">

<p>如何判断 feature x 是否包含了充足的信息，能够从中预测出 y？</p>
<p>判别方式：找一个人类专家，他能根据现有条件给出预测吗？</p>
<p>一个说英语的人，可以根据现有上下文填出 “two”；但一个房地产专家不能仅根据房屋面积预测房价。</p>
<p><strong>如果 feature x 确实包含了充足的的信息用来预测 y ，大量的训练数据就是有帮助的。</strong></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/101.jpg'  width="80%" height="80%"/ loading="lazy">

<p>使用一个有很多参数的学习算法（也被直接称为 low bias algorithm），在训练集上会得到不错的拟合效果，也就是 $J_{train}$ 比较小。</p>
<p>然后，用一个很大的训练集进行训练，模型就难以过拟合（low variance），在测试集上表现跟训练集上差不多，也就是 $J_{train}≈J_{test}$。</p>
<p>也就是：$J_{test}$ 比较小。</p>
<p>以上，我们证明了：做一个关键的假设<strong>如果特征值有足够的信息量，并有一类很好的学习算法保证low bias，那么——如果由大量的训练数据集，这能保证得到 low variance 的模型</strong>。</p>
<hr>
<h2 id="第十二章-支持向量机-Support-Vector-Machines）"><a href="#第十二章-支持向量机-Support-Vector-Machines）" class="headerlink" title="第十二章 支持向量机(Support Vector Machines）"></a>第十二章 支持向量机(Support Vector Machines）</h2><p>在监督学习中， 很多监督学习算法的性能都很相似，所以经常要考虑的不是选择哪种算法，而是：构建算法时所使用的数据量。这体现了应用算法时的技巧，比如所涉及的用于学习算法的特征的选择，以及正则化参数的选择，等等。</p>
<p>还有一个更加强大的算法，有广泛的应用：支持向量机（support vector machine）。与逻辑回归、神经网络相比，SVM在学习复杂的非线性方程时，能提供一种更为清晰和更强大的方式。</p>
<h3 id="12-1-优化目标（optimization-objective）"><a href="#12-1-优化目标（optimization-objective）" class="headerlink" title="12.1 优化目标（optimization objective）"></a><span id="12.1">12.1 优化目标（optimization objective）</span></h3><h4 id="从逻辑回归的损失函数定义SVM的损失函数"><a href="#从逻辑回归的损失函数定义SVM的损失函数" class="headerlink" title="从逻辑回归的损失函数定义SVM的损失函数"></a>从逻辑回归的损失函数定义SVM的损失函数</h4><p><strong>直观上</strong>：以直代曲。</p>
<p>跟逻辑回归的代价函数中的取对数部分相似，当 y&#x3D;1，$cost_1(z)$ 让 z 越大越好；当 y&#x3D;0，$cost_0(z)$ 让 z 越小越好。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/102.jpg'  width="80%" height="80%"/ loading="lazy">



<p><strong>表达式上：</strong></p>
<ul>
<li>把取对数项换成 $cost_1(z)$ 和 $cost_0(z)$ </li>
<li>去掉 $\frac{1}{m}$ （约定习惯。优化的结果不会变，依然是最佳值）</li>
<li>逻辑回归的损失函数形式上是 $A+\lambda B$，用 $\lambda$ 来权衡损失 $A$ 和正则化 $B$ 的相对权重（来决定我们是更关心第一项还是第二项的优化）。而在SVM里，使用 $CA+B$ 的形式，对 $C$ 赋值来权衡 $A$ 、$B$ 的相对权重。可以理解为$C &#x3D; \frac{1}{\lambda}$。</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/103.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="12-2-大边界的直观理解（large-margin-intuition）"><a href="#12-2-大边界的直观理解（large-margin-intuition）" class="headerlink" title="12.2 大边界的直观理解（large margin intuition）"></a><span id="12.2">12.2 大边界的直观理解（large margin intuition）</span></h3><p>有时候称 SVM 为大间距分类器（large margin classifier）。</p>
<p>从损失函数图像理解：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/104.jpg'  width="80%" height="80%"/ loading="lazy">

<p>从损失函数表达式理解：</p>
<p>当 $C$ 取一个很大的值，在优化过程中我们希望让第一项为0。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/105.jpg'  width="80%" height="80%"/ loading="lazy">

<p>逻辑回归：当$y&#x3D;1$，我们希望 $z≥0$；当$y&#x3D;0$，我们希望 $z&lt;0$。</p>
<p><strong>SVM：当$y&#x3D;1$，我们希望 $z≥1$；当$y&#x3D;0$，我们希望 $z≤-1$。</strong></p>
<p>SVM的决策边界：SVM会尽量把正样本和负样本以最大的间距分开。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/106.jpg'  width="70%" height="70%"/ loading="lazy">



<p>一个问题：只因为一个负样本分布比较偏，预测边界就从黑线转变为紫线，是不合理的。但如果把 <strong>SVM 的 $C$ 设置得非常大</strong>，因此会做这样的转变。</p>
<p><strong>如果 $C$ 不是很大，或者模型本身不是线性可分的</strong>，那就不会有这样的问题。预测边界是黑线，SVM表现不错。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/107.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="12-3-大边界分类器背后的数学原理"><a href="#12-3-大边界分类器背后的数学原理" class="headerlink" title="12.3 大边界分类器背后的数学原理*"></a><span id="12.3">12.3 大边界分类器背后的数学原理*</span></h3><p><em>SVM的优化问题和大间距分类器之间的联系</em></p>
<p><strong>回顾向量内积</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/108.jpg'  width="80%" height="80%"/ loading="lazy">

<p>$u^T v &#x3D; p·||u|| &#x3D; u_1v_1+u_2v_2，$p 是 $v$ 投影到 $u$ 上的长度。</p>
<p><strong>SVM 决策边界</strong>：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/109.jpg'  width="80%" height="80%"/ loading="lazy">

<p>SVM 损失函数第二项，优化时，做的其实是最小化参数向量 $\theta$ 的范数（长度）的平方。</p>
<p>$\theta^Tx^{(i)}&#x3D;p^{(i)}·||\theta||&#x3D;\theta_1x_1^{(i)}+\theta_2x_2^{(i)}$</p>
<p>上式告诉我们：两个条件 $\theta^Tx^{(i)} ≥1$、$\theta^Tx^{(i)} ≤1$ 可以换种方式表述，即 $ p^{(i)}·||\theta||$ 的大小。</p>
<p>改写后：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/110.jpg'  width="80%" height="80%"/ loading="lazy">

<ul>
<li>假设 $\theta_0&#x3D;0$ ，决策边界通过原点，决策边界一定与 $\theta$ 向量垂直。以 $\theta_1x_1+\theta_2x_2≥1$ 为例，$x_2 &#x3D; -\frac{\theta1}{\theta2} x_1 + a$ ，跟 $\theta$ （斜率 $\frac{\theta_2}{\theta_1}$）是垂直的。</li>
<li>左图绿色的决策边界，会导致 $x$ 到 $\theta$ 的投影长度较小。我们又要求 $ p^{(i)}·||\theta||≥1$ ，因此 $\theta$ 的范数要比较大。</li>
<li>右边绿色的决策边界，会导致 $x$ 到 $\theta$ 的投影长度比较大。要求 $ p^{(i)}·||\theta||≥1$ ，可以把 $\theta$ 优化到很小。</li>
<li>在SVM中，要求对于大多数数据点， $\theta^Tx ≥1$，也就是说，正样本、负样本的数据点投影到 $\theta$ 的值足够大，就是使决策边界周围保持大间距，让 $p^{(i)}$ 尽量大。</li>
</ul>
<h3 id="12-4-核函数Ⅰ"><a href="#12-4-核函数Ⅰ" class="headerlink" title="12.4 核函数Ⅰ"></a><span id="12.4">12.4 核函数Ⅰ</span></h3><p><em>改造 SVM，来构造复杂的非线性分类器</em></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/111.jpg'  width="80%" height="80%"/ loading="lazy">

<p>非线性决策边界的拟合，需要很多的高阶多项式项，给计算带来麻烦。</p>
<h4 id="是否有更好的特征选择方式？"><a href="#是否有更好的特征选择方式？" class="headerlink" title="是否有更好的特征选择方式？"></a>是否有更好的特征选择方式？</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/112.jpg'  width="80%" height="80%"/ loading="lazy">

<p>每个特征是 x 与 l 的相似度 $f_i &#x3D; similarity(x, l^{(i)}) &#x3D; exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2})$</p>
<p>这个相似度函数 similarity(x, l) 就是核函数，在这里是高斯核函数。</p>
<p>相似度函数（核函数）的作用：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/112.jpg'  width="80%" height="80%"/ loading="lazy">

<p>这些特征 f 的作用就是衡量 x 到 l 的相似度。越相近，f 越接近 1；越不相近，f 越接近 0.</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/114.jpg'  width="80%" height="80%"/ loading="lazy">

<p>$\sigma$ 越大，从把 x 从 l 点移走时，特征变量的值 f 下降得越慢。</p>
<h4 id="从特征到预测函数"><a href="#从特征到预测函数" class="headerlink" title="从特征到预测函数"></a>从特征到预测函数</h4><p>通过前面的核函数，可以从 x 得到特征 f 了，并且越接近 f<sub>i</sub> 对应的 l<sub>i</sub> ，f<sub>i</sub> 的输出越接近1。</p>
<p>通过学习到每个特征对应的 $\theta$，就可以进行预测，拟合复杂的非线性边界了。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/115.jpg'  width="80%" height="80%"/ loading="lazy">

<p>以图中的 $\theta$ 取值为例，接近 $l_1,l_2$ 的点，预测的结果为 1；远离 $l_1,l_2$ 的点，预测的结果为 0.</p>
<p>现在已经学习了核函数，以及如何在 SVM 使用核函数，来定义新的特征变量了。</p>
<p>还有一些问题：如何选择标记点 $l$？其他的核函数（相似度函数）是什么样的？</p>
<h3 id="12-5-核函数Ⅱ"><a href="#12-5-核函数Ⅱ" class="headerlink" title="12.5 核函数Ⅱ"></a><span id="12.5">12.5 核函数Ⅱ</span></h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/116.jpg'  width="80%" height="80%"/ loading="lazy">

<h4 id="如何选择-landmarks？"><a href="#如何选择-landmarks？" class="headerlink" title="如何选择 landmarks？"></a>如何选择 landmarks？</h4><p>一种方法：把数据集的 m 个样本都选择为 landmarks，即 $l^{(i)}&#x3D;x^{(i)}$。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/117.jpg'  width="80%" height="80%"/ loading="lazy">

<p>如上图对于每个输入 x，把它转换成一个 m 维的特征向量 $f^{(i)}$.</p>
<h4 id="SVM-的训练"><a href="#SVM-的训练" class="headerlink" title="SVM 的训练"></a>SVM 的训练</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/118.jpg'  width="80%" height="80%"/ loading="lazy">

<p>注意：</p>
<ul>
<li><p>在优化时设置 $\theta^Tf≥1$ ，在预测时用 $\theta^Tf≥0$</p>
</li>
<li><p>忽略掉 $\theta_0$后，最后一项正则化也可以用 $\theta^T\theta$ 来计算。</p>
</li>
<li><p>由于所有样本都设置为关键点，有 m &#x3D; n，正则化项求和边界没变。</p>
</li>
<li><p>在逻辑回归等算法也可以采用核函数的思想，来设置关键点，但在计算上没有 SVM 特有的优化方法。</p>
</li>
<li><p>这个函数的优化不建议自己实现，调包即可。</p>
</li>
</ul>
<h4 id="SVM-的参数，偏差和方差"><a href="#SVM-的参数，偏差和方差" class="headerlink" title="SVM 的参数，偏差和方差"></a>SVM 的参数，偏差和方差</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/119.jpg'  width="80%" height="80%"/ loading="lazy">

<ul>
<li>C（理解为 $\frac{1}{\lambda}$)<ul>
<li>C 大：拟合项权重大，正则化项权重小，容易过拟合。low bias, high variance。</li>
<li>C 小：拟合项权重小，正则化项权重大，容易欠拟合。high bias, low variance。</li>
</ul>
</li>
<li>$\sigma^2$<ul>
<li>$\sigma^2$ 大：$f$ 平缓，模型随x的改变，变化不明显，容易欠拟合。high bias, low variance。</li>
<li>$\sigma^2$ 小：$f $ 陡峭，变化明显，容易过拟合。low bias, high variance。</li>
</ul>
</li>
</ul>
<h3 id="12-6-使用支持向量机"><a href="#12-6-使用支持向量机" class="headerlink" title="12.6 使用支持向量机"></a><span id="12.6">12.6 使用支持向量机</span></h3><p><strong>不需要自己写 SVM 优化软件，但需要手动</strong>：</p>
<ul>
<li>选择参数 C</li>
<li>选择核函数<ul>
<li>无核函数（也叫线性核函数）<ul>
<li>predict “y &#x3D; 1” if $\theta^Tx≥0$ </li>
<li>线性关系（$\theta_0+\theta_1x_1+…+\theta_nx_n≥0$)</li>
<li>如果有大量的特征值和很少的训练数据集（n大，m小），就拟合一个线性的判定边界，而不去你和一个非常复杂的非线性函数，防止过拟合</li>
</ul>
</li>
<li>高斯核函数<ul>
<li>$f_i &#x3D; exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2})$，where $l^{(i)} &#x3D; x^{(i)}$</li>
<li>选择参数 $\sigma^2$ </li>
<li>如果特征值少，训练集数据多（n小，m大），用高斯核函数是一个好的选择</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/120.jpg'  width="80%" height="80%"/ loading="lazy">



<p><strong>核函数的编写</strong>：</p>
<ul>
<li>很多 SVM 优化函数需要核函数作为参数传入</li>
<li>核函数返回一个实数</li>
<li>在使用高斯核函数前需要做 feature scaling</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/121.jpg'  width="80%" height="80%"/ loading="lazy">



<p><strong>核函数的选择</strong>：</p>
<ul>
<li>核函数需要通过 mercer’s theorem 检查，确保能够正常优化</li>
<li>其他的核函数<ul>
<li>多项式核函数（Polynomial kernal）<ul>
<li>$k(x, l) &#x3D; (x^Tl+constant)^{degree}$ </li>
<li>通常用在 x 和 l 都是严格的非负数时，以确保内积一定不是负数</li>
</ul>
</li>
<li>string kernel, chi-square kernal, histogram intersection kernel, …</li>
</ul>
</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/122.jpg'  width="80%" height="80%"/ loading="lazy">



<p><strong>多分类任务</strong>：</p>
<ul>
<li>很多包都内置了多分类 SVM 功能</li>
<li>如果没有，使用 one-vs-all 思想（见<a href="#6.7">6.7</a>）：如果有 k 个类别，就训练 k 个 SVM ，用以将每个类别从其他的类别中区分开来。会得到 k 组 $\theta$，预测时取最大值的索引作为分类结果。</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/123.jpg'  width="80%" height="80%"/ loading="lazy">



<p><strong>逻辑回归 vs SVMs</strong>：</p>
<p>n 是特征的个数（$x∈\R^{n+1}$），m 是训练集样本个数</p>
<ul>
<li>n 比 m 大很多（如有10000个词特征，只有1000篇邮件来训练）：<strong>使用逻辑回归或无核函数（线性核函数）</strong></li>
<li>n 很小，m 大小适中（n&#x3D;1~1000，m&#x3D;10~10000)：<strong>高斯核函数表现好</strong></li>
<li>n 很小，m 很大（n&#x3D;1~1000，m&#x3D;~50000+）：高斯核函数会跑的很慢。如果训练样本特别多，<strong>尝试添加更多特征，然后使用逻辑回归或无核函数（线性核函数）</strong></li>
</ul>
<p><strong>在所有的场合，设计良好神经网络都可以表现的很好</strong>。缺点是比起好的SVM包，神经网络训练得比较慢。在实际应用中，对于神经网络，局部最优是一个不大不小的问题；但SVM的优化是凸函数优化，在使用SVM时不需要担心这个问题。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/124.jpg'  width="80%" height="80%"/ loading="lazy">





<hr>
<h2 id="第十三章-聚类-Clustering）"><a href="#第十三章-聚类-Clustering）" class="headerlink" title="第十三章 聚类(Clustering）"></a>第十三章 聚类(Clustering）</h2><h3 id="13-1-无监督学习简介（unsupervised-learning-introduction）"><a href="#13-1-无监督学习简介（unsupervised-learning-introduction）" class="headerlink" title="13.1 无监督学习简介（unsupervised learning introduction）"></a><span id="13.1">13.1 无监督学习简介（unsupervised learning introduction）</span></h3><p>监督学习：我们有一系列标签，然后用假设函数去拟合它。</p>
<p>无监督学习：我们的数据并不带有任何的标签。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/125.jpg'  width="90%" height="90%"/ loading="lazy">



<p>聚类算法的应用：</p>
<ul>
<li>市场分割</li>
<li>社交网络分析</li>
<li>组指计算机集群</li>
<li>天文数据分析</li>
</ul>
<h3 id="13-2-K均值算法（K-means-algorithm）"><a href="#13-2-K均值算法（K-means-algorithm）" class="headerlink" title="13.2 K均值算法（K-means algorithm）"></a><span id="13.2">13.2 K均值算法（K-means algorithm）</span></h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/126.jpg'  width="90%" height="90%"/ loading="lazy">

<p>输入：</p>
<ul>
<li>K：聚类的个数</li>
<li>训练数据集 ${x^{(1)},x^{(2)},…,x^{(m)}}$ <ul>
<li>$x^{(i)}∈\R^n$，<strong>无监督学习的x是 n 维向量而不是 n+1 维</strong></li>
</ul>
</li>
</ul>
<p><strong>随机初始化 K 个聚类中心</strong> $\mu_1, \mu_2,…,\mu_k ∈ \R^K$</p>
<p>循环：</p>
<ul>
<li>样本划分：对于每个训练集的样本，<strong>把值设置为最接近的聚类中心的值</strong></li>
<li>移动聚类中心：对于每个聚类中心，<strong>把所有值相同的训练样本求平均，得到新的聚类中心</strong><ul>
<li>如果有聚类中心没有一个训练样本，常见的做法是移除这个聚类中心</li>
</ul>
</li>
</ul>
<p>即使数据没有明确分为几簇，K-means算法还是能将数据分为几个簇。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/127.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="13-3-优化目标（optimization-objective）"><a href="#13-3-优化目标（optimization-objective）" class="headerlink" title="13.3 优化目标（optimization objective）"></a><span id="13.3">13.3 优化目标（optimization objective）</span></h3><p>了解K-means的优化函数，能帮助我们：</p>
<ul>
<li>对学习算法进行调试，确保算法正确运行</li>
<li>帮助算法找到更好的簇，并避免局部最优解（下节讲）</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/128.jpg'  width="80%" height="80%"/ loading="lazy">

<p>K-means算法的代价函数：训练样本到聚类中心的接近程度</p>
<p>K-means算法的优化目标：调整聚类中心 $\mu$ 、样本划分 $c$ ，让代价函数取到最小值</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/129.jpg'  width="80%" height="80%"/ loading="lazy">

<p>每次迭代：</p>
<ul>
<li><p>先样本划分：改变 $c^{(i)}$，不改变 $\mu_k$，优化代价函数 $J$ 关于变量 $c$</p>
</li>
<li><p>后移动聚类中心：改变 $\mu_k$，优化代价函数 $J$ 关于变量 $\mu$</p>
</li>
</ul>
<h3 id="13-4-随机初始化（random-initialization）"><a href="#13-4-随机初始化（random-initialization）" class="headerlink" title="13.4 随机初始化（random initialization）"></a><span id="13.4">13.4 随机初始化（random initialization）</span></h3><p><em>如何初始化 K-means、如何使 K-means 避开局部最优</em></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/130.jpg'  width="80%" height="80%"/ loading="lazy">

<p>从 $x^{(i)}$ 里随机找 K 个聚类中心，这就是随机初始化。随机初始化状态不同，K-means最后可能会得到不同的结果。特别地，K-means 可能会落在局部最优。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/131.jpg'  width="80%" height="80%"/ loading="lazy">

<p>我们可以以不同的初始值初始化 K-means 很多次，并执行算法很多次，以此来保证我们最终能得到一个尽可能好的局部或全局最优值。</p>
<p>具体做法如下：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/132.jpg'  width="80%" height="80%"/ loading="lazy">

<p>多次随机初始化 K-means 算法，分别求代价函数，在其中找最小的 $J$ .当 K 不是特别大（不超过10），一般都能得到比较好的结果。</p>
<h3 id="13-5-选择聚类个数K（choosing-the-number-of-clusters）"><a href="#13-5-选择聚类个数K（choosing-the-number-of-clusters）" class="headerlink" title="13.5 选择聚类个数K（choosing the number of clusters）"></a><span id="13.5">13.5 选择聚类个数K（choosing the number of clusters）</span></h3><p>这个问题没有什么好的答案，也没有能自动处理的方法。用来决定聚类数量最常用的方法仍然是：人工，人为观察可视化的图，或者观察聚类算法的输出。</p>
<p>无监督学习没有给出标签，因此并不总是有一个明确的答案。也是因为这个原因，用算法自动选择聚类个数是困难的。</p>
<p><strong>肘部法则（elbow method）</strong>：改变 K 的值，计算损失函数 $J$，选择“elbow”的点作为 K 的个数。（每次计算 $J$ 都要随机取个100次的初始聚类中心，这个计算消耗是不小的。）有时这个肘部也不太好找。总之：这是个值得尝试的方法，但不能指望它解决问题。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/133.jpg'  width="80%" height="80%"/ loading="lazy">



<p><strong>从下游找K值：</strong>聚类往往是要解决问题的，比如T恤分为 S、M、L 三个聚类，从类似的下游环节寻找聚类个数。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/134.jpg'  width="80%" height="80%"/ loading="lazy">






<hr>
<h2 id="第十四章-降维-Dimensionality-Reduction）"><a href="#第十四章-降维-Dimensionality-Reduction）" class="headerlink" title="第十四章 降维(Dimensionality Reduction）"></a>第十四章 降维(Dimensionality Reduction）</h2><h3 id="14-1-动机一：数据压缩（data-compression）"><a href="#14-1-动机一：数据压缩（data-compression）" class="headerlink" title="14.1 动机一：数据压缩（data compression）"></a><span id="14.1">14.1 动机一：数据压缩（data compression）</span></h3><p>数据压缩不仅能让我们节省存储空间，还能对学习算法进行加速。</p>
<p>我们需要一个二维向量 $x^{(i)}∈\R^2$ 来表示一个样本的两个特征值。有时候这两个特征是冗余的，特征值可能是线性的。</p>
<p>如果能通过投影绿线上的样本来近似原始的数据集，那只需要一个实数 $z^{(i)}∈\R $ 就能指定点在直线上的位置。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/135.jpg'  width="80%" height="80%"/ loading="lazy">

<p>从 3D 到 2D 的降维也是一样的：三维中的数据大概都分布在一个平面内，所以我们通过把所有数据都投影到一个二维平面上来进行降维。</p>
<p>这样，使用二维向量 $z^{(i)}∈\R^2$ 可以表示三维坐标。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/136.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="14-2-动机二：数据可视化（data-visualization）"><a href="#14-2-动机二：数据可视化（data-visualization）" class="headerlink" title="14.2 动机二：数据可视化（data visualization）"></a><span id="14.2">14.2 动机二：数据可视化（data visualization）</span></h3><p>数据可视化可以帮助我们理解数据。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/137.jpg'  width="80%" height="80%"/ loading="lazy">



<p>以图中50维的国家数据为例，如果能只用两个数字，来概述50个数字，就可以把这些国家在二维平面上表示出来。</p>
<p>当观察降维算法的输出时，z 通常不是所期望的具有物理意义的特征，我们常常要弄清楚这些特征大致意味着什么。</p>
<p>比如图中，横轴表示国家的“体量”，纵轴表示国家的“人均”。</p>
<h3 id="14-3-主成分分析问题（principal-component-analysis-problem-formulation）"><a href="#14-3-主成分分析问题（principal-component-analysis-problem-formulation）" class="headerlink" title="14.3 主成分分析问题（principal component analysis problem formulation）"></a><span id="14.3">14.3 主成分分析问题（principal component analysis problem formulation）</span></h3><p>主成分分析算法（PCA）是很常用、很流行的降维算法。</p>
<h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/138.jpg'  width="80%" height="80%"/ loading="lazy">



<p>PCA算法会找一个低维平面（图中红线），然后将数据投影在上面，使数据到低维平面的距离最小（图中蓝色小线段）。</p>
<p>更正式地说：当从n维降到k维，需要找出一个方向（找出k个向量 $u^{(i)}∈\R^n$，定义这个k维空间）。PCA能够找出最小化投影距离的方式，来对数据进行投影。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/139.jpg'  width="80%" height="80%"/ loading="lazy">



<h4 id="PCA和线性回归"><a href="#PCA和线性回归" class="headerlink" title="PCA和线性回归"></a>PCA和线性回归</h4><p>PCA和lr看上去相似，但确实是两种完全不同的算法。</p>
<p>线性回归：最小化预测值和实际值的距离。用x预测y。</p>
<p>PCA：最小化投影距离（正交距离）。都是特征x，没有需要特殊对待的y。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/140.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="14-4-主成分分析算法（principal-component-analysis-algorithm）"><a href="#14-4-主成分分析算法（principal-component-analysis-algorithm）" class="headerlink" title="14.4 主成分分析算法（principal component analysis algorithm）"></a><span id="14.4">14.4 主成分分析算法（principal component analysis algorithm）</span></h3><p>首先要进行 feature scaling、mean normalization 等数据预处理工作。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/141.jpg'  width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/142.jpg'  width="80%" height="80%"/ loading="lazy">



<p><strong>PCA 算法需要做的两件事：求出定义了降维空间的向量 $u$；求出样本在降维空间的投影 $z$。</strong></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/143.jpg'  width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/144.jpg'  width="80%" height="80%"/ loading="lazy">

<p>PCA算法：</p>
<p>先计算协方差矩阵 $\Sigma$，再计算 $\Sigma$ 的特征向量 $U$.</p>
<p>$U$ 是 (n, n) 维矩阵，它的前 $k$ 列就是要求的 k 维空间的方向向量 $u^{(1)}, u^{(2)}, …,u^{(k)}$ 。</p>
<p>把这些向量$u^{(1)}, u^{(2)}, …,u^{(k)}$ 构建成 $U_{reduce}$ 矩阵 (n, k)维，通过公式 $z &#x3D; U_{reduce}^TX$ 可以求的 k 维空间的样本 $z$ 。</p>
<p>PCA算法总览：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/145.jpg'  width="80%" height="80%"/ loading="lazy">



<p>本节没有给出数学上详细的证明，来证明 $u$、$z$ 就是让误差平方最小的投影方法。</p>
<h3 id="14-5-选择主成分的数量（choosing-the-number-of-principal-components）"><a href="#14-5-选择主成分的数量（choosing-the-number-of-principal-components）" class="headerlink" title="14.5 选择主成分的数量（choosing the number of  principal components）"></a><span id="14.5">14.5 选择主成分的数量（choosing the number of  principal components）</span></h3><p>本节将讨论如何选择降维后的维度 k。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/146.jpg'  width="80%" height="80%"/ loading="lazy">

<p>投影误差可以理解为放弃的特征值，总方差理解为全部的特征值。</p>
<p>选择k的通用原则是：选择使不等式成立的最小k。</p>
<p>意思是：保留更多的方差。</p>
<p>找k的算法的实现如下，可以使用 <code>svd(Sigma)</code> 返回的S来简化计算。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/147.jpg'  width="80%" height="80%"/ loading="lazy">

<p>只需要运行svd函数一次，就可以进行这些运算。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/148.jpg'  width="80%" height="80%"/ loading="lazy">

<p>$\frac{\sum^k_{i&#x3D;1}S_{ii}}{\sum^m_{i&#x3D;1}S_{ii}}≥0.99$这个式子也可以用来表示 k 值选的好不好。如果我们把特征从1000维降到100维，可以使用这个不等式来衡量降维对于特征保留的性能。</p>
<h3 id="14-6-降维重建（reconstruction-from-compressed-representation）"><a href="#14-6-降维重建（reconstruction-from-compressed-representation）" class="headerlink" title="14.6 降维重建（reconstruction from compressed representation）"></a><span id="14.6">14.6 降维重建（reconstruction from compressed representation）</span></h3><p>怎样从 $z∈\R$ 得到 $x∈\R^2$？</p>
<p>求$z$的公式是 $z&#x3D;U_{reduce}^Tx$，$U$不是方阵，没有逆矩阵，不能直接算。 $x_{approx}&#x3D;U_{reduce}z$。由于PCA要求投影不能跟数据点离得太远，所以 $x_{approx}$ 差不多就是 $x$。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/149.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="14-7-主成分分析法的应用建议（advice-for-applying-PCA）"><a href="#14-7-主成分分析法的应用建议（advice-for-applying-PCA）" class="headerlink" title="14.7 主成分分析法的应用建议（advice for applying PCA）"></a><span id="14.7">14.7 主成分分析法的应用建议（advice for applying PCA）</span></h3><p><em>PCA是如何提高算法执行效率的？</em></p>
<h4 id="应用PCA的注意事项"><a href="#应用PCA的注意事项" class="headerlink" title="应用PCA的注意事项"></a>应用PCA的注意事项</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/150.jpg'  width="80%" height="80%"/ loading="lazy">

<p>PCA建立了从 $x$ 到 $z$ 的映射关系，这个映射关系只能通过<strong>在训练集上运行PCA</strong>得到。</p>
<p>这个映射所做的就是计算一系列参数，比如进行特征缩放和均值归一化，还计算了$U_{reduce}$，找到合适的k值等。我们<strong>应该只在训练集上拟合这些参数</strong>，而不知交叉验证集或测试集上。</p>
<p>在训练集上找到所有参数后，就可以把这个映射用在交叉验证集或测试集的其他样本中。</p>
<p>以图中为例，<strong>只在训练集的数据上运行PCA</strong>，从 $x$ 到 $z$ 建立映射，然后在 $z$ 和 $y$ 之间拟合机器学习算法（<strong>低维数据提高了算法的运行效率</strong>），当要做预测时，就把交叉验证集和测试集中的数据也映射为 $z$（映射用的矩阵 $U_{reduce}$也<strong>只能从训练集得来</strong>），然后通过机器学习算法得出预测结果。</p>
<h4 id="PCA的应用场景"><a href="#PCA的应用场景" class="headerlink" title="PCA的应用场景"></a>PCA的应用场景</h4><p><strong>PCA的主要应用场景：</strong></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/151.jpg'  width="80%" height="80%"/ loading="lazy">

<p>不同的应用场景要选择不同的k值。</p>
<p><strong>PCA的错误应用场景：</strong></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/152.jpg'  width="80%" height="80%"/ loading="lazy">

<p>PCA能给数据降维，减少特征数量。所以有人会认为PCA可以用来防止过拟合。</p>
<p>如果这样做了，可能效果也会很好，但这不是解决过拟合的好方法。更好的解决过拟合的方法是：使用正则化。</p>
<p>原因：PCA不使用标签y，仅使用输入的x，去寻找接近的低维数据。PCA扔掉一些信息来减少数据的维度，并且不关心y的值。PCA更有可能丢失一些有价值的信息。</p>
<p><strong>PCA的另一个误用：</strong></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/153.jpg'  width="80%" height="80%"/ loading="lazy">

<p>经常在一个项目开始时，一些人会写一个项目计划，其中包括PCA在内的四步。</p>
<p>在写下这样的计划之前，要先问这个问题：如何直接去做而不使用PCA会怎样？</p>
<p>只有在使用原始数据 $x^{(i)}$ 达不到预期效果时（运行太慢，需要提高效率；内存不够、硬盘不够，需要压缩数据），才考虑使用PCA和 $z^{(i)}$。</p>
<hr>
<h2 id="第十五章-异常检测-Anomaly-Detection）"><a href="#第十五章-异常检测-Anomaly-Detection）" class="headerlink" title="第十五章 异常检测(Anomaly Detection）"></a>第十五章 异常检测(Anomaly Detection）</h2><p>这是机器学习算法的一个常见应用，它的有趣之处在于：虽然主要用在非监督学习问题，但从某个角度看，跟有监督学习问题是非常相似的。</p>
<h3 id="15-1-问题的动机（problem-motivation）"><a href="#15-1-问题的动机（problem-motivation）" class="headerlink" title="15.1 问题的动机（problem motivation）"></a><span id="15.1">15.1 问题的动机（problem motivation）</span></h3><p>从给定无标签的数据集，对数据建模 $p(x)$，对于新数据，如果 $p(x{test})＜\epsilon$ ，就将它标记为 anomaly。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/154.jpg'  width="80%" height="80%"/ loading="lazy">

<p>应用举例：</p>
<ul>
<li>网站欺骗检测。$x^{(i)}$ 是用户 $i$ 的行为，包含如登录次数、点击速度等信息；对数据建模得到 $p(x)$；对 $p(x{test})＜\epsilon$ 的 anomaly 的用户进行身份验证，或开启防御动作。</li>
<li>工业生产领域。如找到异常的机器。$x^{(i)}$ 是计算机 $i$ 的状态，包含如内存占用、CPU占用等信息；对数据建模得到 $p(x)$；对 $p(x{test})＜\epsilon$ 的 anomaly 的计算机进行监视，防止异常出现。</li>
</ul>
<h3 id="15-2-高斯分布（gaussian-distribution）"><a href="#15-2-高斯分布（gaussian-distribution）" class="headerlink" title="15.2 高斯分布（gaussian distribution）"></a><span id="15.2">15.2 高斯分布（gaussian distribution）</span></h3><h4 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h4><p>高斯分布也叫正态分布 $x\sim N(\mu,\sigma^2)$，$\mu$ 是中心值， $\sigma$ 是标准差（$\sigma^2$是方差）。</p>
<p>公式：</p>
<div>
$$
p(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$
</div>

<p>$\sigma$ 越小，图像越窄；无论参数如何变，图像的积分一定是1 。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/155.jpg'  width="80%" height="80%"/ loading="lazy">



<h4 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h4><p>假设 $x^{(i)}$ 符合高斯分布，但不知道 $\mu$ 和 $\sigma$ 的值。</p>
<ul>
<li>$\mu&#x3D;\frac{1}{m}\sum^m_{i&#x3D;1}x^{(i)}$，均值</li>
<li>$\sigma^2&#x3D;\frac{1}{m}\sum^m_{i&#x3D;1}(x^{(i)}-\mu)^2$，方差，也是对 $\mu$ 和 $\sigma^2$ 的极大似然估计</li>
</ul>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/156.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="15-3-异常检测算法（algorithm）"><a href="#15-3-异常检测算法（algorithm）" class="headerlink" title="15.3 异常检测算法（algorithm）"></a><span id="15.3">15.3 异常检测算法（algorithm）</span></h3><p>每个特征都随机取值，都服从一个高斯分布，等同于一个 $x_1$ 到 $x_n$ 的<strong>独立同分布</strong>。</p>
<div>
$$
p(x) = \prod^n_{j=1}p(x_j;\mu_j,\sigma_j^2)
$$
</div>


<p>异常检测算法总览：</p>
<p>先选一些符合数据分布特征的例子；根据这个数据集训练高斯分布的参数；进行估计。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/157.jpg'  width="80%" height="80%"/ loading="lazy">



<p>独立同分布相乘的直观理解：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/158.jpg'  width="80%" height="80%"/ loading="lazy">

<p>粉色是 anomaly 区域。</p>
<h3 id="15-4-开发和评价一个异常检测系统（developing-and-evaluating-an-anomaly-detection-system）"><a href="#15-4-开发和评价一个异常检测系统（developing-and-evaluating-an-anomaly-detection-system）" class="headerlink" title="15.4 开发和评价一个异常检测系统（developing and evaluating an anomaly detection system）"></a><span id="15.4">15.4 开发和评价一个异常检测系统（developing and evaluating an anomaly detection system）</span></h3><p><em>如何开发一个异常检测的应用来解决实际问题；如何评估一个异常检测算法。</em></p>
<p>之前讲到了用实数评估的好处。如果在异常检测算法也有这样的实数是比较好的。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/159.jpg'  width="80%" height="80%"/ loading="lazy">

<p>训练集是non-anomalous的，在交叉验证集、测试集中，假设是有标签的并且含有anomaly数据（y&#x3D;1代表anomaly）。图中展示了两种可行的划分方法（第二种把同样的数据同时用在交叉验证集和测试集是不推荐的）：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/160.jpg'  width="80%" height="80%"/ loading="lazy">



<p>异常检测算法的评估：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/161.jpg'  width="80%" height="80%"/ loading="lazy">

<p>首先在<strong>训练集</strong>上拟合函数 $p(x)$。</p>
<p>因为数据很倾斜（y&#x3D;1的情况很少），使用这些指标来评估：真&#x2F;假阳&#x2F;阴性，查准率和查全率，F1分。</p>
<p>可以在<strong>交叉验证集</strong>上尝试多个 $\epsilon$ 的值，并选择让F1分最高的那个，或在其他方面有良好表现的那个。</p>
<p><strong>交叉验证集的作用就是可以尝试不同的特征组合、参数值，来选择最佳的配置。</strong></p>
<p>得到了配置，就在<strong>测试集</strong>上进行最终的评估。</p>
<h3 id="15-5-异常检测与监督学习对比（anomaly-detection-vs-supervised-learning）"><a href="#15-5-异常检测与监督学习对比（anomaly-detection-vs-supervised-learning）" class="headerlink" title="15.5 异常检测与监督学习对比（anomaly detection vs. supervised learning）"></a><span id="15.5">15.5 异常检测与监督学习对比（anomaly detection vs. supervised learning）</span></h3><p><em>在上一节，我们使用了带标签的数据集来评估异常检测算法，这与监督学习有什么异同？为什么不用逻辑回归或神经网络来预测 y&#x3D;0 或 y&#x3D;1？分别在什么情况下使用两者？</em></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/162.jpg'  width="80%" height="80%"/ loading="lazy">

<p>异常检测中，<strong>有很少的正样本和很多的负样本</strong>，当在估计 $p(x)$ 的值，拟合所有高斯参数的过程中，我们只需要负样本就可以了。所以如果有大量的负样本，仍然可以拟合出很好的 $p(x)$。</p>
<p>异常检测中，有<strong>很多种类的正样本难以观测</strong>（比如有不同的原因导致发动机宕机）。如果只有很少的正样本，<strong>很难充分学习到正样本的种类是什么</strong>；并且，未出现的异常也很难被预测到。所以，放弃对正样本建模，只对负样本建模。</p>
<p>监督学习中，在正常范围内<strong>有足够的正、负样本</strong>。监督学习算法查看大量正负样本，并且能够进行种类的区分。</p>
<p>应用对比上，区别还是在数据集中<strong>正样本的数量</strong>大小。如果给异常检测的正样本收集了足够多的数据，就可以用监督学习算法来预测。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/163.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="15-6-选择特征（choosing-what-features-to-use）"><a href="#15-6-选择特征（choosing-what-features-to-use）" class="headerlink" title="15.6 选择特征（choosing what features to use）"></a><span id="15.6">15.6 选择特征（choosing what features to use）</span></h3><p><em>选择用什么特征来实现异常检测算法。</em></p>
<h4 id="如何调整特征分布"><a href="#如何调整特征分布" class="headerlink" title="如何调整特征分布"></a>如何调整特征分布</h4><p>特征选择的原则：最好<strong>让数据更接近高斯分布</strong>。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/164.jpg'  width="80%" height="80%"/ loading="lazy">



<h4 id="如何选择特征"><a href="#如何选择特征" class="headerlink" title="如何选择特征"></a>如何选择特征</h4><p>跟监督学习类似，先训练一个算法，在交叉验证集上人工查看预测出错的部分，并考虑能否找到一些其他的特征来表达这些判断出错的样本的特性。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/165.jpg'  width="80%" height="80%"/ loading="lazy">

<p>如图，只用 $x_1$ 作为特征，发现了异常点。再给数据建模添加特征 $x_2$ 后，就发现异常产生的原因了。</p>
<p>举例：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/166.jpg'  width="80%" height="80%"/ loading="lazy">

<p>CPU负载和网络负载在有些时候是线性的，也有些时候CPU负载高而网络负载低（如本机死循环）。添加了特征 $x_5$ 或 $x_6$ ，就能捕捉这种情况了。</p>
<h3 id="15-7-多元高斯分布-（multivariate-gaussian-distribution）"><a href="#15-7-多元高斯分布-（multivariate-gaussian-distribution）" class="headerlink" title="15.7 多元高斯分布*（multivariate gaussian distribution）"></a><span id="15.7">15.7 多元高斯分布*（multivariate gaussian distribution）</span></h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/167.jpg'  width="80%" height="80%"/ loading="lazy">

<p>绿点离样本分布有距离，应该被判断为anomaly，但异常检测算法计算 $p(x^{(i)})$ ，看上去检测不出来这个异常。异常检测算法学习到的是粉色范围，而不是蓝色范围。</p>
<p>为了解决这个问题，使用多元高斯分布。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/168.jpg'  width="80%" height="80%"/ loading="lazy">

<p>$\Sigma\in \R^{n\times n}$ 是协方差矩阵，参见 <a href="#14.4">14.4PCA算法的协方差矩阵</a>。PCA第一步是均质归一化，就不用减去平均值了。</p>
<p>使用这个矩阵一次算出所有特征的概率 $p(x)$。公式：</p>
<div>
$$
p(x;\mu,\Sigma)=\frac{1}{(2\pi)^\frac{n}{2}|\Sigma|^\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^T\epsilon^{-1}(x-\mu))
$$
</div>


<p>增大 $\Sigma$ 的某个对角线值（增大某个特征分布的方差），对应x下降得更缓慢；减小 $\Sigma$  的某个对角线值（减小某个特征分布的方差），对应x下降得更缓慢。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/169.jpg'  width="80%" height="80%"/ loading="lazy">

<p>通过多元高斯分布，可以描述斜着的样本分布.越接近1，越跟横轴长度相似：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/170.jpg'  width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/171.jpg'  width="80%" height="80%"/ loading="lazy">

<p>也可以改变 $\mu$ 的值，来改变山顶的位置：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/172.jpg'  width="80%" height="80%"/ loading="lazy">



<h3 id="15-8-使用多元高斯分布进行异常检测-（anomaly-detection-using-the-multivariate-gaussian-distribution）"><a href="#15-8-使用多元高斯分布进行异常检测-（anomaly-detection-using-the-multivariate-gaussian-distribution）" class="headerlink" title="15.8 使用多元高斯分布进行异常检测*（anomaly detection using the multivariate gaussian distribution）"></a><span id="15.8">15.8 使用多元高斯分布进行异常检测*（anomaly detection using the multivariate gaussian distribution）</span></h3><h4 id="步骤和公式"><a href="#步骤和公式" class="headerlink" title="步骤和公式"></a>步骤和公式</h4><p>从数据集得到多元高斯分布的参数 $\mu$ 和 $\Sigma$：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/173.jpg'  width="80%" height="80%"/ loading="lazy">

<p>计算 $p(x)$，得到椭圆形的分布估计：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/174.jpg'  width="80%" height="80%"/ loading="lazy">



<h4 id="多元高斯分布模型和原始模型的区别"><a href="#多元高斯分布模型和原始模型的区别" class="headerlink" title="多元高斯分布模型和原始模型的区别"></a>多元高斯分布模型和原始模型的区别</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/175.jpg'  width="80%" height="80%"/ loading="lazy">

<p>原始模型不包含特征间的关系。特征的分布如果是独立的，概率分布函数就可以拆开相乘。这种拆开乘的原始模型，实际上就是某种特殊情况下（<strong>特征间不相关，即$\Sigma$ 的非主对角线都是0</strong>）的多元高斯分布。此时分布图象关于x，y轴是“正”的。</p>
<p>多元高斯模型的 $\Sigma$ <strong>非主对角线不为0，也就是特征间有相关</strong>，不能写成拆开相乘的形式。这时的<strong>分布图象是“斜”的</strong>。</p>
<p>如何在两个模型之间做选择：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/176.jpg'  width="80%" height="80%"/ loading="lazy">

<p>原始模型：</p>
<ul>
<li>手动组合异常，来捕捉异常样本（如CPU负荷低网络负荷高，需要创建新的特征，见<a href="#15.6">15.6</a>）</li>
<li>计算简单，可以计算巨大的特征数量</li>
<li>参数不多训练容易，既使数据样本很少，也能拟合出好模型</li>
</ul>
<p>多于高斯模型：</p>
<ul>
<li><p>可以自动捕捉特征间的关系</p>
</li>
<li><p>有 $\Sigma^T$ 操作，计算要求高 </p>
</li>
<li><p>必须 $m&gt;n$ ，即样本数量多于特征数量，来保证 $\Sigma$ 可逆；$\Sigma\in\R^{n\times n}$ 的参数很多，实践上最好 $m&gt;&gt;n$</p>
</li>
</ul>
<p>多元高斯分布 $\Sigma$ 不可逆的原因：</p>
<ul>
<li>不满足 $m&gt;n$</li>
<li>有线性相关的特征，造成冗余</li>
</ul>
<hr>
<h2 id="第十六章-推荐系统-Recommender-Systems）"><a href="#第十六章-推荐系统-Recommender-Systems）" class="headerlink" title="第十六章 推荐系统(Recommender Systems）"></a>第十六章 推荐系统(Recommender Systems）</h2><p>推荐系统是机器学习的一个重要应用。推荐系统的性能提升在学界仅占很小的讨论份额，但在企业里是很重要的。</p>
<p>特征学习是机器学习中很重要的思想。对于某些问题，一些算法可以自动学习一系列的特征，在有些环境下我们可以<strong>编写一个算法来学习使用哪些特征</strong>，推荐系统就是一个例子。通过推荐系统，可以了解一些特征学习的思想。</p>
<h3 id="16-1-问题规划（problem-formulation）"><a href="#16-1-问题规划（problem-formulation）" class="headerlink" title="16.1 问题规划（problem formulation）"></a><span id="16.1">16.1 问题规划（problem formulation）</span></h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/177.jpg'  width="80%" height="80%"/ loading="lazy">

<ul>
<li><p>$n_u$：user 的数量</p>
</li>
<li><p>$n_m$：movie 的数量</p>
</li>
<li><p>$r(i, j)$：user i 是否给 movie j 打过分</p>
</li>
<li><p>$y(i, j)$：user i 给 movie j 打了几分（$r(i, j)&#x3D;1$ 时有定义）</p>
</li>
</ul>
<p>推荐系统就是开发一个算法，能够为我们填补这些缺失值。然后在用户还没看过的电影中，预测用户喜欢的电影，并推荐给该用户。</p>
<h3 id="16-2-基于内容的推荐算法（content-based-recommendations）"><a href="#16-2-基于内容的推荐算法（content-based-recommendations）" class="headerlink" title="16.2 基于内容的推荐算法（content-based recommendations）"></a><span id="16.2">16.2 基于内容的推荐算法（content-based recommendations）</span></h3><p>基于内容的意思是：我们假设已有电影内容的特征向量 $x^{(i)}$（描述电影类型）。</p>
<p>基于内容的推荐算法就相当于线性回归：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/178.jpg'  width="80%" height="80%"/ loading="lazy">

<p>每一部电影有一个题材向量 $x$，$x_1$ 评价电影的爱情要素，$x_2$ 评价电影的动作要素，$x_3$ 是偏置项1。</p>
<p>每一个用户 $j$ 有一个评分向量 $\theta^{(j)}\in \R^{(n+1)}$。根据该用户的过往评分拟合出这个向量。</p>
<p>对于新电影，用 $(\theta^{(j)} )^T x^{(i)}$ 预测用户 $j$ 对电影 $i$ 的评分。</p>
<p>更普遍的线性回归过程：为了简化表达式，将评分过的电影数量 $m^{(j)}$ 去掉。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/179.jpg'  width="80%" height="80%"/ loading="lazy">

<p>线性回归的优化目标：<strong>所求是每一个用户的喜好</strong> $\theta^{(j)}$。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/180.jpg'  width="80%" height="80%"/ loading="lazy">

<p>梯度下降：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/181.jpg'  width="80%" height="80%"/ loading="lazy">



<p>对于许多电影来说，我们并没有内容的特征向量 $x^{(i)}$，下节将介绍不是基于内容的推荐系统算法。</p>
<h3 id="16-3-协同过滤（collaborative-filtering）"><a href="#16-3-协同过滤（collaborative-filtering）" class="headerlink" title="16.3 协同过滤（collaborative filtering）"></a><span id="16.3">16.3 协同过滤（collaborative filtering）</span></h3><p>基于内容的推荐系统是我们已经拥有了内容向量（如一个电影拥有多少爱情元素，拥有多少动作元素），但实际上我们<strong>很难得到这个内容向量</strong>。 并且，我们通常需要除了爱情、动作之外的其他特征，<strong>如何得到这些新特征</strong>？</p>
<p>协同过滤算法有一个很有趣的特性：特征学习。这种算法能够自行学习所要使用的特征。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/182.jpg'  width="80%" height="80%"/ loading="lazy">

<p>我们不知道内容向量，但每个用户告诉了我们对每种电影的喜爱程度 $\theta^{(j)}$。对于电影 $x^{(1)}$，我们知道 $\theta^{(1)},\theta^{(2)}$ 喜欢并且 $\theta^{(3)},\theta^{(4)}$ 不喜欢，那么可以推断 $x^{(1)}$ 是爱情片而不是动作片，并可以根据 $(\theta^{(j)})^T x^{(1)}\approx r(1, j)$ 计算出 $x^{(1)}$ 的具体值。</p>
<p><strong>所求是每一部电影的内容向量</strong> $x^{(i)}$：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/183.jpg'  width="80%" height="80%"/ loading="lazy">



<p>这节和上节的算法可以形成协同过滤的迭代过程：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/184.jpg'  width="80%" height="80%"/ loading="lazy">

<p>从一个初始用户喜爱向量 $\theta$ 开始，可以不断进行梯度优化，得到更准确的用户喜好 $\theta$、电影类型 $x$，并且得到好的推荐系统。</p>
<h3 id="16-4-协同过滤算法（collaborative-filtering-algorithm）"><a href="#16-4-协同过滤算法（collaborative-filtering-algorithm）" class="headerlink" title="16.4 协同过滤算法（collaborative filtering algorithm）"></a><span id="16.4">16.4 协同过滤算法（collaborative filtering algorithm）</span></h3><p>在前两节，我们学习了：</p>
<ul>
<li>给出电影的特征，可以用来学习用户的参数 $\theta$</li>
<li>给出用户的参数，可以用来学习电影的特征 $x$</li>
</ul>
<p>有一种更高效的算法，我们不需要不停的交替计算 $x$ 和 $\theta$，而是能同时计算 $x$ 和 $\theta$。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/185.png'  width="80%" height="80%"/ loading="lazy">

<p>取消了偏置项，即 $x_0,\theta_0$。因为算法要学习很多的特征，如果需要一个特征永远为1，可以选择靠自己去获得这个参数，比如把 $x_1$ 学习为1。让 $x$ 和 $\theta$ 自己学习线性关系，不需要人为加截距项。</p>
<p>协同过滤算法：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/186.png'  width="80%" height="80%"/ loading="lazy">

<ul>
<li>用小的随机数初始化，有点像神经网络的初始化</li>
<li>进行梯度下降，最小化 $J$</li>
<li>用 $\theta^Tx$ 进行预测</li>
</ul>
<h3 id="16-5-向量化：低秩矩阵分解（vectorization-low-rank-matrix-factorization）"><a href="#16-5-向量化：低秩矩阵分解（vectorization-low-rank-matrix-factorization）" class="headerlink" title="16.5 向量化：低秩矩阵分解（vectorization : low rank matrix factorization）"></a><span id="16.5">16.5 向量化：低秩矩阵分解（vectorization : low rank matrix factorization）</span></h3><p>低秩矩阵分解就是协同过滤的向量化求解过程：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/187.png'  width="80%" height="80%"/ loading="lazy">

<p>通过电影内容向量 $x$ 可以度量电影之间的相关性：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/188.png'  width="80%" height="80%"/ loading="lazy">



<h3 id="16-6-实现细节：均值归一化（implementational-detail-mean-normalization）"><a href="#16-6-实现细节：均值归一化（implementational-detail-mean-normalization）" class="headerlink" title="16.6 实现细节：均值归一化（implementational detail : mean normalization）"></a><span id="16.6">16.6 实现细节：均值归一化（implementational detail : mean normalization）</span></h3><p>如果一个用户没有给任何的电影评分，那在梯度下降最小化 $J$ 的过程中，会学习到 $\theta^{(j)} &#x3D; 0$。如果用 $\theta^Tx$ 做预测，会将所有的电影分数预测为0分，并且不会给用户做任何推荐。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/189.png'  width="80%" height="80%"/ loading="lazy">



<p>均值归一化让我们解决这类问题。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/190.png'  width="80%" height="80%"/ loading="lazy">

<p>给评分矩阵 $Y$ 减去评分均值 $\mu$，然后将新矩阵进行协同过滤，在预测阶段再把 $\mu$ 加回到预测的评分上。</p>
<p>对于没有评分的用户，会预测为平均分，推荐给他大家都觉得好的电影。</p>
<p>对于没有人评分的电影，不推荐给任何用户。</p>
<hr>
<h2 id="第十七章-大规模机器学习-Large-Scale-Machine-Learning）"><a href="#第十七章-大规模机器学习-Large-Scale-Machine-Learning）" class="headerlink" title="第十七章 大规模机器学习(Large Scale Machine Learning）"></a>第十七章 大规模机器学习(Large Scale Machine Learning）</h2><p>处理大数据集的算法。</p>
<h3 id="17-1-大型数据集的学习（learning-with-large-datasets）"><a href="#17-1-大型数据集的学习（learning-with-large-datasets）" class="headerlink" title="17.1 大型数据集的学习（learning with large datasets）"></a><span id="17.1">17.1 大型数据集的学习（learning with large datasets）</span></h3><p>如果数据集 m&#x3D;1亿，梯度下降的每一步都要对 m&#x3D;1亿 项求和，计算代价太大了。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/191.png'  width="80%" height="80%"/ loading="lazy">

<p>在高偏差的情况下，让数据集增大效果不好；在高方差的情况下，增大数据集一般是有效的。</p>
<p>详见 <a href="#10.6">10.6学习曲线</a></p>
<h3 id="17-2-随机梯度下降（stochastic-gradient-descent）"><a href="#17-2-随机梯度下降（stochastic-gradient-descent）" class="headerlink" title="17.2 随机梯度下降（stochastic gradient descent）"></a><span id="17.2">17.2 随机梯度下降（stochastic gradient descent）</span></h3><p>batch gradient descent：批量梯度下降，每次要同时考虑所有的训练样本。</p>
<p>如果数据集很大，对计算时间、内存容量都有很高的要求。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/192.png'  width="80%" height="80%"/ loading="lazy">



<p>stochastic gradient descent：随机梯度下降，先打乱数据集，然后用单个数据样本进行梯度下降。内层循环遍历所有数据样本，外层循环控制遍历次数，一般取1~10。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/193.png'  width="80%" height="80%"/ loading="lazy">



<p>批量梯度下降每次迭代取平均的梯度，直接向最优点收敛（红线）；随机梯度下降	每一次迭代都会更快，每次只需保证能拟合某一个训练样本就可以了，每一步不一定都向最优点收敛，但总体上也能到达最优点（粉线）。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/194.png'  width="80%" height="80%"/ loading="lazy">



<h3 id="17-3-小批量梯度下降（mini-batch-gradient-descent）"><a href="#17-3-小批量梯度下降（mini-batch-gradient-descent）" class="headerlink" title="17.3 小批量梯度下降（mini-batch gradient descent）"></a><span id="17.3">17.3 小批量梯度下降（mini-batch gradient descent）</span></h3><p>批量梯度下降迭代慢、收敛快；随机梯度下降迭代快、收敛慢。小批量梯度下降是两者的折中方案。</p>
<p>一次取 b 个样本进行小批量梯度下降，b 一般取 2~100。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/195.png'  width="80%" height="80%"/ loading="lazy">



<p>小批量梯度下降可能会比随机梯度下降算法更好，仅当：有一个好的向量化方法。在这种情况下，对一批 b 个样本的偏导求和能以更向量化的方式执行，这将使我们在 b 个样本中实现部分并行计算（同时计算 b 个）。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/196.png'  width="80%" height="80%"/ loading="lazy">



<h3 id="17-4-随机梯度下降收敛（stochastic-gradient-descent-convergence）"><a href="#17-4-随机梯度下降收敛（stochastic-gradient-descent-convergence）" class="headerlink" title="17.4 随机梯度下降收敛（stochastic gradient descent convergence）"></a><span id="17.4">17.4 随机梯度下降收敛（stochastic gradient descent convergence）</span></h3><p><em>在运行随机梯度下降算法时，如何确保调试过程已经完成，并且已经收敛到合适的位置呢？怎样调整随机梯度下降种学习速率 $\alpha$ 的值？</em></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/197.png'  width="80%" height="80%"/ loading="lazy">

<p>在批量梯度下降中，通过绘制 $J(\theta)$ 的图像来判断收敛。原因是批量梯度下降的每一次迭代，代价函数都是减小的。需要在算法运行过程中扫描一遍整个数据集来计算当前的 $J(\theta)$ ，所以数据集很大的时候也不好用。</p>
<p>随机梯度下降每次只考虑一个单独的样本，每次让算法前进一点，就不需要停下算法来计算 $J(\theta)$ 了。只需关注单个样本 $(x^{(i)},y^{(i)})$ ，在用该样本更新 $\theta$ 之前，计算 $cost(\theta, (x^{(i)},y^{(i)}))$ 。并且在一定量的数据样本的迭代后，打印 $cost$ 的平均值。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/198.png'  width="80%" height="80%"/ loading="lazy">

<p>图左上：学习率大和小。学习率小的收敛结果可能更好。</p>
<p>图右上：计算 $cost$ 均值的数据量越大，曲线越平滑。缺点是求均值的数据量大，得到的反馈有延迟。</p>
<p>图左下：少的数据量求平均，看上去没有在学习收敛（蓝线）；但大的数据量求均值，可以看出函数实际上是在收敛的过程中的（红线）。如果大学习量求均值，函数也不收敛，就是算法出了问题（粉线）。</p>
<p>图右下：$cost$ 在增加，函数发散了。需要用更小的学习率 $\alpha$。	</p>
<p>关于学习率：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/199.png'  width="80%" height="80%"/ loading="lazy">

<p>随机梯度下降的结果是在最小值附近震荡，如果想让随机梯度下降更好地收敛到全局最小值（震荡的幅度小），可以让学习率 $\alpha$ 随时间变化逐渐减小。但同时，也会带来新的确定参数的工作。</p>
<h3 id="17-5-在线学习（online-learning）"><a href="#17-5-在线学习（online-learning）" class="headerlink" title="17.5 在线学习（online learning）"></a><span id="17.5">17.5 在线学习（online learning）</span></h3><p><em>我们有连续一波数据或连续的数据流，想要用算法来学习。</em></p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/200.png'  width="80%" height="80%"/ loading="lazy">

<p>跟随机梯度下降很相似，区别是：在线学习不使用固定的数据样本 $(x^{(i)},y^{(i)})$ ，而是根据目前的数据流  $(x, y)$ 来训练模型。这要求数据流比较多，这样就不用重复学习同样的样本了。</p>
<p>还有一个好处，就是这样可以适应用户喜好随着时代的变化。</p>
<p>另一个应用场景：预测点击率，并进行推送。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/201.png'  width="80%" height="80%"/ loading="lazy">



<h3 id="17-6-映射化简和数据并行（map-reduce-and-data-parallelism）"><a href="#17-6-映射化简和数据并行（map-reduce-and-data-parallelism）" class="headerlink" title="17.6 映射化简和数据并行（map-reduce and data parallelism）"></a><span id="17.6">17.6 映射化简和数据并行（map-reduce and data parallelism）</span></h3><p><em>前面的梯度下降算法都能在单个计算机上运行。但有时候可能数据太多了，不想把所有数据都在一台电脑上跑一遍。</em></p>
<p>map-reduce：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/203.png'  width="80%" height="80%"/ loading="lazy">

<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/202.png'  width="80%" height="80%"/ loading="lazy">

<p>批量梯度下降：</p>
<ul>
<li>在单个主机上计算 $\theta_j :&#x3D;\theta_j-\alpha\frac{1}{400}\sum_{i&#x3D;1}^{400}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$</li>
</ul>
<p>map-reduce：</p>
<ul>
<li>将数据集分成几部分，在不同的主机上计算 $temp_j^{(t)}&#x3D;\sum_{}^{}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$</li>
<li>然后在整合服务器上计算 $\theta_j :&#x3D;\theta_j-\alpha\frac{1}{400}\sum temp_j^{(t)}$。</li>
</ul>
<p>实际上，很多学习算法都可以表示成对训练集函数的求和，而在大数据集上运行所消耗的运算量就在于需要对非常大的训练集进行求和。所以<strong>只要学习算法可以表示为对训练集的求和，或者学习算法的主要工作可以表示成对训练集的求和，那么就可以使用 map-reduce 将学习算法的适用范围扩大到非常大的数据集。</strong></p>
<p>举例：如果使用高级优化函数来训练logistic算法，需要计算两个重要的量：一个是对于高级学习算法，需要提供一个过程来计算优化目标的代价函数 $J_{train}(\theta)$ ，第二个是计算偏导数 $\frac{\partial}{\partial \theta_j}J_{train}(\theta)$。将这两个求和过程分散给多个主机，然后把结果整合起来，就获得总的损失和总的偏导项，接着将这两个值交给高级优化算法。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/204.png'  width="80%" height="80%"/ loading="lazy">



<p>也有时候可以在单个电脑上进行 map-reduce。有的电脑又多个CPU，CPU又有多个核心。如果有一个很大的数据集，可以在单机上分给不同的核心来计算。</p>
<p>也有些线性代数库，可以自动进行多核运算。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/205.png'  width="80%" height="80%"/ loading="lazy">





<hr>
<h2 id="第十八章-应用实例：图片文字识别-Application-example-Photo-OCR）"><a href="#第十八章-应用实例：图片文字识别-Application-example-Photo-OCR）" class="headerlink" title="第十八章 应用实例：图片文字识别(Application example : Photo OCR）"></a>第十八章 应用实例：图片文字识别(Application example : Photo OCR）</h2><p>一个复杂的机器学习系统是如何组织起来的。</p>
<p>机器学习pipeline的相关知识，以及如何分配资源。</p>
<p>机器学习中一些有用的想法和概念，如：如何将机器学习运用到计算机视觉问题中；人工数据合成的概念。</p>
<h3 id="18-1-问题描述和流程（problem-description-and-pipeline）"><a href="#18-1-问题描述和流程（problem-description-and-pipeline）" class="headerlink" title="18.1 问题描述和流程（problem description and pipeline）"></a><span id="18.1">18.1 问题描述和流程（problem description and pipeline）</span></h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/206.png'  width="80%" height="80%"/ loading="lazy">

<p>pipeline：</p>
<ol>
<li>找出有文字的图像区域</li>
<li>进行字母分离</li>
<li>分类器识别字母</li>
</ol>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/207.png'  width="80%" height="80%"/ loading="lazy">



<p>在机器学习实践中，很重要的一步就是设计机器学习的pipeline，如何将整个问题分为一系列不同的模块。</p>
<h3 id="18-2-滑动窗口（sliding-windows）"><a href="#18-2-滑动窗口（sliding-windows）" class="headerlink" title="18.2 滑动窗口（sliding windows）"></a><span id="18.2">18.2 滑动窗口（sliding windows）</span></h3><h4 id="pipeline第一步：检测文字区域"><a href="#pipeline第一步：检测文字区域" class="headerlink" title="pipeline第一步：检测文字区域"></a>pipeline第一步：检测文字区域</h4><p><strong>行人检测任务：</strong></p>
<p>训练阶段，矩形的长宽比例比较固定。可以构建监督学习模型，根据带有标签的数据集，学习预测是否含有行人。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/208.png'  width="80%" height="80%"/ loading="lazy">

<p>预测阶段，使用滑动窗口：建立不同尺寸的滑动窗口，按照一定的步长，取一系列图像，判断是否含有行人。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/209.png'  width="80%" height="80%"/ loading="lazy">



<p><strong>文本检测任务：</strong></p>
<p>模型训练阶段，也可以训练类似的监督学习模型：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/210.png'  width="80%" height="80%"/ loading="lazy">

<p>预测阶段：使用小的滑动窗口，预测图中各个区域有文字的概率（越亮代表有文字的可能性大）然后进行放大算子，将白色区域扩大。并对长宽比进行筛选。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/211.png'  width="80%" height="80%"/ loading="lazy">



<h4 id="pipeline第二步：字母分割"><a href="#pipeline第二步：字母分割" class="headerlink" title="pipeline第二步：字母分割"></a>pipeline第二步：字母分割</h4><p>训练监督学习模型，识别<strong>字母分割的部分</strong></p>
<p>在第一步检测到文字的区域进行滑动窗口，预测在哪里进行了字母的分割。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/212.png'  width="80%" height="80%"/ loading="lazy">



<h4 id="pipeline第三步：字母识别"><a href="#pipeline第三步：字母识别" class="headerlink" title="pipeline第三步：字母识别"></a>pipeline第三步：字母识别</h4><p>监督学习多分类问题，不赘述。</p>
<h3 id="18-3-获取大量数据和人工数据合成（getting-lots-of-data-artificial-data-synthesis）"><a href="#18-3-获取大量数据和人工数据合成（getting-lots-of-data-artificial-data-synthesis）" class="headerlink" title="18.3 获取大量数据和人工数据合成（getting lots of data : artificial data synthesis）"></a><span id="18.3">18.3 获取大量数据和人工数据合成（getting lots of data : artificial data synthesis）</span></h3><p>一个得到高性能机器学习系统的方法：做一个低偏差的机器学习算法，并且使用庞大的训练集去训练它。</p>
<p>通过人工数据合成，得到大量的训练数据。</p>
<h4 id="其一：从零开始产生数据集"><a href="#其一：从零开始产生数据集" class="headerlink" title="其一：从零开始产生数据集"></a>其一：从零开始产生数据集</h4><p>用不同的字体生成字符，然后粘贴到不同的背景下，应用模糊算子、仿射变换、缩放旋转等操作，制作成为新的训练样本。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/213.png'  width="80%" height="80%"/ loading="lazy">

<p>左边是真实数据集，右边是人造数据集。</p>
<h4 id="从已有数据扩充数据集"><a href="#从已有数据扩充数据集" class="headerlink" title="从已有数据扩充数据集"></a>从已有数据扩充数据集</h4><p>将已有数据样本进行扭曲等操作，扩充数据集。在语音识别中，也可以认为给“干净”的音频加上各种背景噪声，扩充数据集。</p>
<p>注意，要添加“有意义”的噪声，即在真实异常状况下会出现的噪声。添加纯净的随机噪声往往是没用的。比如最好改变文字的形状，只加高斯噪声改变图片某处的亮度没什么用。</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/214.png'  width="80%" height="80%"/ loading="lazy">



<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/215.png'  width="80%" height="80%"/ loading="lazy">

<ul>
<li>扩充数据集有用吗？在扩充数据集之前，首先要保证我们的分类器偏差较低。这样，更多的数据才会起作用。标准的做法是绘制学习曲线，确保这是<strong>低偏差、高方差</strong>的分类器。如果分类器的偏差太高，可以尝试增加特征的数量，或者增加神经网络隐藏神经元的数量，直到偏差降低。然后再花精力生成人工训练集。</li>
<li>获得10倍于当前数据集的数据，需要花费多少努力？其实扩充数据集没有想象中难。<ul>
<li>本节讲的，人工生成</li>
<li>收集数据或打标签</li>
<li>众包</li>
</ul>
</li>
</ul>
<h3 id="18-4-上限分析：劲往何处使（ceiling-analysis-what-part-of-the-pipeline-to-work-on-next）"><a href="#18-4-上限分析：劲往何处使（ceiling-analysis-what-part-of-the-pipeline-to-work-on-next）" class="headerlink" title="18.4 上限分析：劲往何处使（ceiling analysis : what part of the pipeline to work on next）"></a><span id="18.4">18.4 上限分析：劲往何处使（ceiling analysis : what part of the pipeline to work on next）</span></h3><p>工作流中的哪一部分是最值得花时间去研究的？</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/216.png'  width="80%" height="80%"/ loading="lazy">

<p>对学习系统使用一个数值评价度量：准确度accuracy。先看整个系统的表现（72%），然后人工按顺序给出每个模块的正确答案，然后看整个系统的准确度表现提升。这样，我们可以看出各个模块的上升空间有多大。</p>
<p>如图，就算 character segmentation 模块到达了完美，总体性能也只能提升 1%。</p>
<p>另一个例子，身份识别的pipeline如下：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/217.png'  width="80%" height="80%"/ loading="lazy">

<p>上限分析如下：</p>
<img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/218.png'  width="80%" height="80%"/ loading="lazy">

<p>可以看到，完美的 face detection、eyes segmentation、logistic regression 会对模型有较大的改进；preprocess 对模型的改进很小。</p>
<hr>
<h2 id="第十九章-结束-Conclusion）"><a href="#第十九章-结束-Conclusion）" class="headerlink" title="第十九章 结束(Conclusion）"></a>第十九章 结束(Conclusion）</h2><h3 id="19-1-总结与致谢（summary-and-thank-you）"><a href="#19-1-总结与致谢（summary-and-thank-you）" class="headerlink" title="19.1 总结与致谢（summary and thank you）"></a><span id="19.1">19.1 总结与致谢（summary and thank you）</span></h3><img src='https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ml-notes-img/ml2014ang/219.png'  width="80%" height="80%"/ loading="lazy">]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>课程笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>【我看UE源码】（0）系列开篇 &amp; 全目录</title>
    <url>/2022/09/07/ue-0/</url>
    <content><![CDATA[<h4 id="1-系列开篇"><a href="#1-系列开篇" class="headerlink" title="1 系列开篇"></a>1 系列开篇</h4><h5 id="关于系列"><a href="#关于系列" class="headerlink" title="关于系列"></a>关于系列</h5><p>（2022&#x2F;6&#x2F;19）</p>
<p>最近刚开始接触虚幻引擎的源码，感到难以入门。网上的资料繁多，但也不太成体系，看起来、找起来都不方便。这是因为游戏引擎太过庞大，各位博主的切入点或者专研的模块都不一样。</p>
<p>我语言和工程的基础都弱，看很多博文都看不懂，自己看引擎更是连语法都搞不明白。唯一能做的就是广泛地搜索、从能看懂的一点点开始了。</p>
<p>在学习过程中，有必要按照我自己的切入点来整理一下。不仅方便日后回忆和查阅，而且也想给博客开启一个很长的系列，提醒自己坚持下去。</p>
<p>现在的想法是，看到什么就写什么，编号一路加上去。然后在下面的目录里面分类整理。因为从头对某个模块建立认识也不是一鼓作气就能完成的，大概也会是个来来回回的过程。</p>
<p>此外，放一下<a href="https://www.cnblogs.com/timlly/p/13512787.html">向往</a>大佬的<a href="https://www.cnblogs.com/timlly/p/13512787.html">剖析虚幻渲染体系</a>，我个人觉得这是目前最成体系、最干货也相当适合入门的渲染模块的文章了（然而我连第一篇都看不完，令人感叹）。当然人家是大佬回头写教程，而我从头开始看，肯定几十篇都不及人家一篇。不过也算是找个榜样，在写作这方面激励自己了。</p>
<h5 id="关于目录"><a href="#关于目录" class="headerlink" title="关于目录"></a>关于目录</h5><p>其实目前我对怎么去写这个目录也没什么好的想法，看了一些比较成体系的博客的目录，如：</p>
<ul>
<li><a href="https://www.cnblogs.com/ghl_carmack/p/5677090.html">UE4入门与精通</a>，虽然只写了蓝图、反射的内容，但分了UE的使用和源码两部分，源码也分了不少模块</li>
<li><a href="https://www.cnblogs.com/shiroe/p/14695535.html">【UE4 C++】博客目录 &#x2F; 学习笔记汇总</a>，有一些基础、语法相关的东西</li>
<li><a href="https://zhuanlan.zhihu.com/p/34256771">《Exploring in UE4》开篇</a>，更多高级Gameplay模块的知识</li>
</ul>
<p>总之就先随便列一下，后面再更新就好。</p>
<h4 id="2-目录"><a href="#2-目录" class="headerlink" title="2 目录"></a>2 目录</h4><h5 id="（-1）-一切之前的事情"><a href="#（-1）-一切之前的事情" class="headerlink" title="（-1） 一切之前的事情"></a>（-1） 一切之前的事情</h5><ul>
<li><a href="https://acbgzm.github.io/2022/06/18/ue-sourcecode--1/">【我看UE源码】（-1）一切之前的事情</a></li>
</ul>
<h5 id="（0）系列开篇-amp-目录"><a href="#（0）系列开篇-amp-目录" class="headerlink" title="（0）系列开篇 &amp; 目录"></a>（0）系列开篇 &amp; 目录</h5><ul>
<li><a href="https://acbgzm.github.io/2022/09/07/ue-0/">【我看UE源码】（0）系列开篇 &amp; 目录</a> 也就是本文，是系列博文的目录，有规划、记录问题的作用。参考比较成体系的博客列表。持续更新</li>
</ul>
<h5 id="（1）UE-C"><a href="#（1）UE-C" class="headerlink" title="（1）UE C++"></a>（1）UE C++</h5><blockquote>
<p>UE4基础，各种更偏向C++（尤其是语法）的东西</p>
</blockquote>
<p>目前想写以下几篇：</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> 从UE4的一例Cast到C++类型转换</li>
<li><input disabled="" type="checkbox"> UE4 sparse array的位运算一则</li>
<li><input disabled="" type="checkbox"> UE4的排序用的是什么算法？</li>
<li><input disabled="" type="checkbox"> 委托到底是什么？</li>
</ul>
<h5 id="（2）UE引擎基础"><a href="#（2）UE引擎基础" class="headerlink" title="（2）UE引擎基础"></a>（2）UE引擎基础</h5><blockquote>
<p>UE4基础，不那么偏向语法的东西。如反射、GC、蓝图、多线程等等，支持高级模块的一些内容</p>
</blockquote>
<h5 id="（3）GamePlay"><a href="#（3）GamePlay" class="headerlink" title="（3）GamePlay"></a>（3）GamePlay</h5><blockquote>
<p>可能是更偏向Gameplay架构的一些东西，大概就是看看大钊的那些文章</p>
</blockquote>
<h5 id="（4）UI"><a href="#（4）UI" class="headerlink" title="（4）UI"></a>（4）UI</h5><p>目前想写以下几篇：</p>
<ul>
<li><input disabled="" type="checkbox"> UMG和Slate</li>
<li><input disabled="" type="checkbox"> Slate的几个过程</li>
<li><input disabled="" type="checkbox"> slow path和fast path</li>
<li><input disabled="" type="checkbox"> prepass阶段，计算size和process invalidation</li>
<li><input disabled="" type="checkbox"> draw阶段、paint阶段的缓存技术</li>
<li><input disabled="" type="checkbox"> 非常奇怪的LayerId分发规则</li>
<li><input disabled="" type="checkbox"> render阶段，slate合批优化技术</li>
<li><input disabled="" type="checkbox"> 源码欠考虑的部分，各种错误复现</li>
<li><input disabled="" type="checkbox"> 常见的UI优化方式</li>
</ul>
<h5 id="（4）-（？）"><a href="#（4）-（？）" class="headerlink" title="（4）~（？）"></a>（4）~（？）</h5><blockquote>
<p>各个模块，UI、网络、渲染、物理、AI、动画等</p>
</blockquote>
]]></content>
      <categories>
        <category>UE源码</category>
      </categories>
      <tags>
        <tag>UnrealEngine</tag>
        <tag>游戏开发</tag>
      </tags>
  </entry>
  <entry>
    <title>GAMES101 - 现代计算机图形学入门 - 闫令琪</title>
    <url>/2022/03/15/games101/</url>
    <content><![CDATA[<p>早在两年前毕业后的暑假，我就尝试学了一下GAMES101，到第三课矩阵变换的地方就败北了。今年，为了补一补游戏开发的基础知识，才终于坚持着全看完，断断续续用了一个半月。两年过去了，一些概念和计算理解起来也容易了。</p>
<p>我想这应该是我上过的最好的中文课了吧？首先干货很多，不仅讲各种经典方法，还讲到了“现代”的发展，期间还有很多的支线专题、基础知识；然后听起来很舒适，很课会给我“秀&gt;教”的感觉，但GAMES101没有。图形学很难、很交叉，能有闫令琪这种强度的人，制作这么好的中文入门课程，我感到由衷的感激。</p>
<p>笔记已经全部回顾并精心整理了。之前有一些开发工作，没有多余的脑子做作业，后面做完了作业再总结一下吧。</p>
<hr>
<h3 id="Lecture-01-Overviews-of-Computer-Graphics"><a href="#Lecture-01-Overviews-of-Computer-Graphics" class="headerlink" title="Lecture 01 - Overviews of Computer Graphics"></a>Lecture 01 - Overviews of Computer Graphics</h3><h4 id="1-引入"><a href="#1-引入" class="headerlink" title="1 - 引入"></a>1 - 引入</h4><h5 id="图形学的应用"><a href="#图形学的应用" class="headerlink" title="图形学的应用"></a>图形学的应用</h5><p>游戏：</p>
<ul>
<li>游戏中，如何评价画面好不好？<ul>
<li>画面亮，说明全局光照做得好，看起来更舒服</li>
</ul>
</li>
<li>《无主之地3》卡通渲染中的卡通是什么样的风格，怎么做？</li>
</ul>
<p>影视：</p>
<ul>
<li><p>电影中的特效（special effects）属于图形学的一部分，是最简单的图形学应用</p>
<ul>
<li>特效是在日常生活看不到的，不会产生违和感</li>
<li>最困难的是日常生活最常见的东西</li>
</ul>
</li>
<li><p>《阿凡达》把人的面部捕捉做得很好</p>
</li>
</ul>
<p>动画：</p>
<ul>
<li>《疯狂动物城》的每根毛发都被渲染出来，每根头发都跟光线进行作用<ul>
<li>图形学的几何（毛发的表示）、渲染（计算光照并显示）</li>
</ul>
</li>
<li>《冰雪奇缘2》各种技能是怎样做出来的，风吹动衣服、头发如何运动<ul>
<li>图形学中的模拟&#x2F;动画</li>
</ul>
</li>
</ul>
<p>设计：</p>
<ul>
<li><p>CAD（Computer-Aided Design），给汽车建模并实时切换光照查看效果</p>
<ul>
<li>几何（汽车的各个面）、光照（计算不同光照的效果）、模拟（模拟车撞墙的实验）</li>
</ul>
</li>
<li><p>家装网站，渲染室内设计图片</p>
</li>
</ul>
<p>其他：</p>
<ul>
<li><p>可视化，将各种视觉信息展现出来，是图形学的重大应用</p>
</li>
<li><p>VR&#x2F;AR</p>
</li>
<li><p>数字绘画（Photoshop）</p>
</li>
<li><p>仿真&#x2F;模拟&#x2F;动画&#x2F;特效</p>
</li>
<li><p>GUI</p>
</li>
<li><p>字体设计，矢量图和点阵</p>
</li>
</ul>
<h5 id="图形学的挑战"><a href="#图形学的挑战" class="headerlink" title="图形学的挑战"></a>图形学的挑战</h5><p>基础挑战：</p>
<ul>
<li><p>图形学是创建逼真的虚拟世界并与之交互（Creates and interacts with realistic virtual world）</p>
</li>
<li><p>输入：研究好真实世界的所有方面的规律，才能将真实世界表示清楚</p>
</li>
<li><p>输出：使用新的计算方法、显示设备、显示技术</p>
</li>
</ul>
<p>技术挑战：</p>
<ul>
<li>Math of (perspective) projections, curves, surfaces</li>
<li>Physics of lighting and shading</li>
<li>Representing &#x2F; operating shapes in 3D</li>
<li>Animation &#x2F; simulation</li>
</ul>
<h5 id="CG和CV"><a href="#CG和CV" class="headerlink" title="CG和CV"></a>CG和CV</h5><ul>
<li>CV：需要计算机进行猜测；希望计算机分析、理解</li>
<li>CG：创建世界</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/2.png" style="zoom:80%;" / loading="lazy">



<h5 id="图形学依赖于"><a href="#图形学依赖于" class="headerlink" title="图形学依赖于"></a>图形学依赖于</h5><ul>
<li>基础数学 - 线性代数、微积分、统计</li>
<li>基础物理 - 光学、力学</li>
<li>其他 - 信号处理、数值分析（渲染的整个过程就是在解一个递归定义的积分；模拟&#x2F;仿真很多是在解有限元问题和扩散方程）</li>
<li>一些美感</li>
</ul>
<h4 id="2-具体内容"><a href="#2-具体内容" class="headerlink" title="2 - 具体内容"></a>2 - 具体内容</h4><h5 id="Rasterization"><a href="#Rasterization" class="headerlink" title="Rasterization"></a>Rasterization</h5><p>光栅化：把3D空间的几何形体显示在屏幕上。是实时计算机图形学的主要应用</p>
<ul>
<li>实时：30fps级别</li>
<li>OpenGL，Shader等是如何运作的</li>
</ul>
<h5 id="Curves-and-Meshes"><a href="#Curves-and-Meshes" class="headerlink" title="Curves and Meshes"></a>Curves and Meshes</h5><p>如何表示曲线和曲面，如何细分曲面，形变时如何变化等</p>
<h5 id="Ray-Tracing"><a href="#Ray-Tracing" class="headerlink" title="Ray Tracing"></a>Ray Tracing</h5><p>光线追踪常用于动画、电影等离线的场合，较慢但生成质量更高的画面，注意两者的trade-off</p>
<h5 id="Animation-x2F-Simulation"><a href="#Animation-x2F-Simulation" class="headerlink" title="Animation &#x2F; Simulation"></a>Animation &#x2F; Simulation</h5><p>动画，各种模拟的效果</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/1.png" style="zoom:300%;" / loading="lazy"></li>
</ul>
<hr>
<h3 id="Lecture-02-Review-of-Linear-Algebra"><a href="#Lecture-02-Review-of-Linear-Algebra" class="headerlink" title="Lecture 02 - Review of Linear Algebra"></a>Lecture 02 - Review of Linear Algebra</h3><h4 id="1-Vectors（向量）"><a href="#1-Vectors（向量）" class="headerlink" title="1 - Vectors（向量）"></a>1 - Vectors（向量）</h4><h5 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h5><ul>
<li>写作 $\vec a$ 或 $\bold a$ ，或用起点和终点表示为 $\overrightarrow{AB} &#x3D; B - A$ </li>
<li>是一个矢量，有方向和长度两个属性，表示向量的方向和数值大小</li>
<li>没有绝对的开始位置，平移后仍然是同一个向量，只看相对位置</li>
</ul>
<h5 id="Vector-Normalization"><a href="#Vector-Normalization" class="headerlink" title="Vector Normalization"></a>Vector Normalization</h5><ul>
<li>向量的长度：$||\vec a||$ </li>
<li>单位向量 $\hat a$ <ul>
<li>长度是 1</li>
<li>找到任意向量的单位向量（normalization）：$\hat a&#x3D;\vec a &#x2F; ||\vec a||$ </li>
<li>通常作为图形学中方向的表示方法，只关心它的方向、不关心长度</li>
</ul>
</li>
</ul>
<h5 id="向量加法"><a href="#向量加法" class="headerlink" title="向量加法"></a>向量加法</h5><ul>
<li>在几何意义上，向量相加可以联想平行四边形法则（起点相同）或三角形法则（首尾相连）</li>
<li>在代数意义上，向量相加就是直接把坐标相加</li>
</ul>
<h5 id="Cartesian-Coordinates"><a href="#Cartesian-Coordinates" class="headerlink" title="Cartesian Coordinates"></a>Cartesian Coordinates</h5><p>在笛卡尔坐标系中，默认向量起点从零点开始，可以只用两个数就表示一个向量</p>
<ul>
<li><div> $\bf A = \begin{pmatrix}x\\y\end{pmatrix}$，<u>图形学中默认一个向量是列向量</u>  </div> </li>
<li><p>$\bold {A} ^T  &#x3D; (x, y)$ </p>
</li>
<li><p>$||\bold A|| &#x3D; \sqrt{x^2+y^2}$ </p>
</li>
<li><p>$\vec a + \vec b &#x3D; (x_1+x_2, y_1+y_2)$</p>
</li>
</ul>
<h5 id="向量乘法"><a href="#向量乘法" class="headerlink" title="向量乘法"></a>向量乘法</h5><h6 id="向量点乘"><a href="#向量点乘" class="headerlink" title="向量点乘"></a>向量点乘</h6><ul>
<li><p>$\vec a \cdot \vec b &#x3D; ||\vec a||\ ||\vec b||\cos \theta$ ，<u>向量点乘的结果是一个数</u> </p>
</li>
<li><p>计算向量夹角</p>
<ul>
<li>$\cos \theta &#x3D; \frac{\vec a \cdot \vec b}{||\vec a||\ ||\vec b||}$ </li>
<li>对于单位向量，$\cos \theta &#x3D; \hat a \cdot \hat b$</li>
</ul>
</li>
<li><p>性质</p>
<ul>
<li>交换律：$\vec a \cdot \vec b &#x3D; \vec b \cdot \vec a\ $ </li>
<li>结合律：$(k\vec a)\cdot \vec b &#x3D; \vec a \cdot (k\vec b) &#x3D; k(\vec a\cdot\vec b)$  </li>
<li>分配律：$\vec a \cdot(\vec b + \vec c) &#x3D; \vec a \cdot \vec b + \vec a \cdot \vec c$</li>
</ul>
</li>
<li><p>笛卡尔坐标系中的向量点乘，就是对应元素相乘再加起来</p>
<ul>
<li><div>2D：$\vec a \cdot \vec b=\begin{pmatrix} {x_a}\\{y_a} \end{pmatrix} \cdot \begin{pmatrix} {x_b}\\{y_b} \end{pmatrix} = x_ax_b + y_ay_b$ </div></li>
<li><div>3D：$\vec a \cdot \vec b=\begin{pmatrix} {x_a}\\{y_a}\\{z_a} \end{pmatrix} \cdot \begin{pmatrix} {x_b}\\{y_b}\\{z_b} \end{pmatrix} = x_ax_b + y_ay_b + z_az_b$ </div></li>
</ul>
</li>
<li><p>点乘在图形学中的应用</p>
<ul>
<li>找到两个向量间的夹角，如光线和物体表面的夹角</li>
<li>找到一个向量在另一个向量上的投影，如计算阴影<ul>
<li>计算 $\vec b$ 在 $\vec a$ 上的投影<ul>
<li>方向跟 $\vec a$ 相同，$\vec {b_\perp}&#x3D;  k\hat a$ </li>
<li>长度 $k &#x3D; ||\vec {b_\perp}|| &#x3D; ||\vec {b}\cos \theta||$</li>
</ul>
</li>
</ul>
</li>
<li>在向量投影基础上的计算<ul>
<li>在垂直、平行方向分解一个向量</li>
<li>计算两个向量的 “接近” 程度：点乘的结果大，说明夹角小<ul>
<li>可用于判断是否能看到镜面反射、高光等</li>
</ul>
</li>
<li>点乘的符号表示向量 <strong>“前”与“后”</strong> 的信息：点乘的结果为负，两个向量方向基本相反</li>
</ul>
</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/3.png" style="zoom: 50%;" / loading="lazy"> 
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/4.png" style="zoom: 50%;" / loading="lazy"> 
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/5.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h6 id="向量叉乘"><a href="#向量叉乘" class="headerlink" title="向量叉乘"></a>向量叉乘</h6><ul>
<li><p><u>向量叉乘的结果是一个向量</u> </p>
<ul>
<li>方向垂直于 $\vec a$ 和 $\vec b$ 所在的平面，具体方向根据右手螺旋定则确定<ul>
<li>$\vec a \times \vec b&#x3D;-\vec b\times \vec a$ ，不满足交换律</li>
<li>$||\vec a \times \vec b|| &#x3D; ||\vec a||\ ||\vec b||\sin \phi$</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/6.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>性质</p>
<ul>
<li>可以确定一个三维坐标系，如果满足 $\vec x \times \vec y &#x3D; +\vec z$，就是右手坐标系</li>
<li>计算性质<ul>
<li>满足分配律、结合律，不满足交换律</li>
<li>向量叉乘自己，$\sin \phi&#x3D;0$，得到的是零向量</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/7.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>计算，向量坐标形式和矩阵乘法形式</p>
<ul>
<li>注意图中向量的对偶矩阵（dual matrix），它跟向量有同样的叉乘意义</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/8.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>叉乘在图形学中的应用</p>
<ul>
<li>判定 <strong>“左”和“右”</strong><ul>
<li>右手系中，叉乘 $\vec a \times \vec b$ 是正向量，说明 $\vec b$ 在 $\vec a$ 的左侧</li>
<li>左手系中，叉乘 $\vec a \times \vec b$ 是正向量，说明 $\vec b$ 在 $\vec a$ 的右侧</li>
</ul>
</li>
<li>判定 <strong>“内”和“外”</strong><ul>
<li>点 $P$ 是否在三角形内部？$\overrightarrow {AB} \times\overrightarrow {AP}$ 结果向外，说明 $P$ 在 $\overrightarrow {AB}$  的左侧，同理判断 $P$ 点跟三条边的位置关系</li>
<li>检查 $P$ 点是否一直在所有边的同一边，从而确定 $P$ 点是否在多边形的内部</li>
<li>这是三角形光栅化的基础，判断三角形覆盖了哪些像素、进而对像素进行着色</li>
<li>如果刚好同方向（叉乘结果是零向量），对于corner case可以自行处理</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/9.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>定义一个正交坐标系</p>
<ul>
<li>三个坐标轴互相垂直</li>
<li>任意向量 $\vec p$ 可以对应到坐标系中的三个轴上（使用投影计算长度）</li>
<li>$\vec p &#x3D; (\vec p\cdot\hat u)\hat u + (\vec p\cdot\hat v)\hat v + (\vec p\cdot\hat w)\hat w$</li>
</ul>
</li>
</ul>
<h4 id="2-Matrices（矩阵）"><a href="#2-Matrices（矩阵）" class="headerlink" title="2 - Matrices（矩阵）"></a>2 - Matrices（矩阵）</h4><h5 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h5><ul>
<li><p>把一些数组合成一个 $m \times n$ 的结构</p>
</li>
<li><p>在图形学中，矩阵普遍被用于表示<strong>变换</strong>（移动、旋转、缩放、剪切）</p>
</li>
</ul>
<h5 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h5><ul>
<li>矩阵的行列需要匹配，才能相乘；$(M\times N)(N\times P)&#x3D;(M\times P)$ </li>
<li>对于相乘后矩阵的每个值，可以这样计算：$(i, j)$ 位置的值是 $A$ 矩阵第 $i$ 行和 $B$ 矩阵第 $j$ 列的两个向量的点积</li>
<li>没有任何交换律；满足结合律和分配律</li>
</ul>
<h5 id="矩阵和向量相乘"><a href="#矩阵和向量相乘" class="headerlink" title="矩阵和向量相乘"></a>矩阵和向量相乘</h5><ul>
<li>始终认为：矩阵在左、向量在右且是列向量，$(M\times N)(N\times 1)&#x3D;(M\times 1)$ </li>
<li><div>如：进行 $x$ 轴镜像变换的矩阵 $\begin{pmatrix} -1&0\\0&1 \end{pmatrix}  \begin{pmatrix} x\\y \end{pmatrix} = \begin{pmatrix} -x\\y \end{pmatrix}$ </div></li>
</ul>
<h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><h6 id="矩阵转置"><a href="#矩阵转置" class="headerlink" title="矩阵转置"></a>矩阵转置</h6><ul>
<li><div>$\begin{pmatrix} 1&2\\3&4 \\ 5&6 \end{pmatrix} ^T = \begin{pmatrix} 1&3&5\\2&4&6\end{pmatrix}$ </div>
</li>
<li><p>$(AB)^T &#x3D; B^T A^T$</p>
</li>
</ul>
<h6 id="单位矩阵、逆矩阵"><a href="#单位矩阵、逆矩阵" class="headerlink" title="单位矩阵、逆矩阵"></a>单位矩阵、逆矩阵</h6><ul>
<li><div>$I_{3\times 3} = \begin{pmatrix} 1&0&0\\0&1&0 \\ 0&0&1 \end{pmatrix}$ ；单位矩阵本身不做操作，主要定义逆矩阵</div></li>
<li>$AA^{-1}&#x3D; A^{-1}A &#x3D; I$ </li>
<li>$(AB)^{-1} &#x3D; B^{-1}A^{-1}$</li>
</ul>
<h4 id="3-向量乘法的矩阵表示"><a href="#3-向量乘法的矩阵表示" class="headerlink" title="3 - 向量乘法的矩阵表示"></a>3 - 向量乘法的矩阵表示</h4><h5 id="点乘"><a href="#点乘" class="headerlink" title="点乘"></a>点乘</h5><div>$\vec a\cdot\vec b=\vec a^T\vec b=\begin{pmatrix} x_a & y_a & z_a \end{pmatrix} \begin{pmatrix} x_b \\ y_b \\ z_b \end{pmatrix} = x_ax_b+y_ay_b+z_az_b$  </div>



<h5 id="叉乘"><a href="#叉乘" class="headerlink" title="叉乘"></a>叉乘</h5><div>$\vec a \times \vec b = A^*b = \begin{pmatrix} 0&-z_a&y_a \\ z_a & 0 & -x_a \\ -y_a&x_a&0 \end{pmatrix} \begin{pmatrix} x_b \\ y_b \\ z_b \end{pmatrix}$</div> 

<hr>
<h3 id="Lecture-03-Transformation"><a href="#Lecture-03-Transformation" class="headerlink" title="Lecture 03 - Transformation"></a>Lecture 03 - Transformation</h3><h4 id="1-变换的应用"><a href="#1-变换的应用" class="headerlink" title="1 - 变换的应用"></a>1 - 变换的应用</h4><p>为什么学习变换、2D变换（旋转&#x2F;缩放&#x2F;切变）、齐次坐标、变换的组合、3D变换</p>
<ul>
<li>模型变换</li>
<li>视图变换</li>
<li>IK（位置改变时，反推模型的变换）</li>
<li>光栅化过程（3D到2D的投影）</li>
</ul>
<h4 id="2-二维空间的变换"><a href="#2-二维空间的变换" class="headerlink" title="2 - 二维空间的变换"></a>2 - 二维空间的变换</h4><h5 id="Scale"><a href="#Scale" class="headerlink" title="Scale"></a>Scale</h5><ul>
<li><div>$\left \{ \begin{array}{l} x' = s_xx \\ y' = s_yy \end{array}  \right.$ </div></li>
<li><div>$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} s_x&0 \\ 0&s_y \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$ </div></li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/10.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h5 id="Reflection"><a href="#Reflection" class="headerlink" title="Reflection"></a>Reflection</h5><ul>
<li><div>$\left \{ \begin{array}{l} x' = -x \\ y' = y \end{array}  \right.$ </div></li>
<li><div>$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} -1&0 \\ 0&1 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$ </div></li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/11.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h5 id="Shear"><a href="#Shear" class="headerlink" title="Shear"></a>Shear</h5><ul>
<li><div>$\left \{ \begin{array}{l} x' = x+ay \\ y' = y \end{array}  \right.$ </div></li>
<li><div>$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} 1&a \\ 0&1 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$ </div></li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/12.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h5 id="Rotate-about-the-origin-0-0-CCW-by-default"><a href="#Rotate-about-the-origin-0-0-CCW-by-default" class="headerlink" title="Rotate (about the origin (0, 0), CCW by default)"></a>Rotate (about the origin (0, 0), CCW by default)</h5><ul>
<li><div>$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} \cos\theta&-\sin\theta \\ \sin\theta&\cos\theta \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$ </div></li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/13.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>推导方法：</p>
<ul>
<li><div>设 $\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} a&b \\ c&d \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$  ，即 $\left \{ \begin{array}{l} x' = ax+by \\ y' = cx+dy \end{array}  \right.$ </div>
</li>
<li><div>带入 $\left \{ \begin{array}{l} (1, 0)\rightarrow (\cos\theta, \sin\theta) \\ (0, 1)\rightarrow (-\sin\theta, \cos\theta) \end{array}\right.$ 两个变换关系</div>
</li>
<li><div>解得 $\left \{ \begin{array}{l} a = \cos\theta \\ b = -\sin\theta \\ c = \sin\theta\\d = \cos\theta \end{array}  \right.$ </div>
</li>
<li><div>$R_\theta =\begin{pmatrix}\cos\theta &-\sin\theta \\\sin\theta & \cos\theta \end{pmatrix}$，$R_{-\theta} =\begin{pmatrix}\cos\theta &\sin\theta \\-\sin\theta & \cos\theta \end{pmatrix}$ </div>
</li>
<li><p>旋转矩阵是正交矩阵</p>
<ul>
<li>计算上，旋转 $-\theta$ 的矩阵就是旋转 $\theta$ 的转置</li>
<li>从定义上看，旋转 $-\theta$ 正好是旋转 $\theta$ 的逆操作</li>
<li>即：<u>旋转中，旋转矩阵的逆就是旋转矩阵的转置</u>，称这个矩阵为<strong>正交矩阵</strong></li>
<li>$R_{-\theta}&#x3D;R_{\theta}^{-1}$</li>
</ul>
</li>
</ul>
<h5 id="几种变换的共同点"><a href="#几种变换的共同点" class="headerlink" title="几种变换的共同点"></a>几种变换的共同点</h5><p>都是线性变换：能写成 矩阵×坐标 的形式。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/14.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h4 id="3-Homogeneous-coordinates（齐次坐标）"><a href="#3-Homogeneous-coordinates（齐次坐标）" class="headerlink" title="3 - Homogeneous coordinates（齐次坐标）"></a>3 - Homogeneous coordinates（齐次坐标）</h4><p>为了更方便地表示非线性变换，引入齐次坐标的概念。</p>
<h5 id="特殊的变换：Translation"><a href="#特殊的变换：Translation" class="headerlink" title="特殊的变换：Translation"></a>特殊的变换：Translation</h5><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/15.png" style="zoom: 50%;" / loading="lazy">
</li>
<li><div>$\left \{ \begin{array}{l} x' = x+t_x \\ y' = y+t_y \end{array}  \right.$ </div></li>
<li><p>平移变换不属于线性变换的范畴，无法写成之前的矩阵相乘的形式</p>
</li>
<li><div>$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} a&b \\ c&d \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} t_x \\ t_y \end{bmatrix}$ </div></li>
<li><p>不想将平移变换当成一个特殊的变换，有没有办法表示所有的变换？（注意，trade-off或者NFL定律表明，引入齐次坐标同样会带来问题）</p>
</li>
</ul>
<h5 id="Homogeneous-Coordinates（齐次坐标）"><a href="#Homogeneous-Coordinates（齐次坐标）" class="headerlink" title="Homogeneous Coordinates（齐次坐标）"></a>Homogeneous Coordinates（齐次坐标）</h5><p>给2D的点和向量都增加第三个维度（w-coordinate）的信息：</p>
<ul>
<li>2D point：$(x, y, 1)^T$ </li>
<li>2D vector：$(x, y, 0)^T$</li>
</ul>
<p>用矩阵表示平移变换：</p>
<ul>
<li><div>$ \begin{pmatrix} x'\\y'\\w' \end{pmatrix} = \begin{pmatrix} 1&0&t_x\\0&1&t_y \\ 0&0&1 \end{pmatrix} \cdot \begin{pmatrix} x\\y\\1 \end{pmatrix} = \begin{pmatrix} x+t_x\\y+t_y\\1 \end{pmatrix}$ </div></li>
</ul>
<p>为什么点的w-coordinate是1，而矩阵是0？</p>
<ul>
<li>解释上，<u>向量具有平移不变性</u>：向量经过上面的平移变换后，可以发现，由于第三维是0，平移对向量不起作用</li>
<li>理解上，这也定义了 point 和 vector 之间的一些运算<ul>
<li>vector + vector &#x3D; vector</li>
<li>point - point &#x3D; vector（把1减没了）</li>
<li>point + vector &#x3D; point</li>
<li>point + point &#x3D; ??<ul>
<li><div>在齐次坐标中， $\begin{pmatrix} x\\y\\w \end{pmatrix}$ 就是2D点 $ \begin{pmatrix} x/w\\y/w\\1 \end{pmatrix} ,w\ne 0$ </div></li>
<li>因此 point + point &#x3D; 中点</li>
</ul>
</li>
</ul>
</li>
<li>引入齐次坐标，不仅保证了向量的平移不变性，而且保证了运算的正确性</li>
</ul>
<p>齐次坐标的代价是什么？</p>
<ul>
<li>增加了几个数，但2D仿射变换的最后一行都是 $(0,0,1)$ ，存储简单</li>
</ul>
<h5 id="Affine-Transformations（仿射变换）"><a href="#Affine-Transformations（仿射变换）" class="headerlink" title="Affine Transformations（仿射变换）"></a>Affine Transformations（仿射变换）</h5><p>定义仿射变换</p>
<ul>
<li><div>affine map = linear map + translation：$\begin{pmatrix} x' \\ y' \end{pmatrix} = \begin{pmatrix} a&b \\ c&d \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix} + \begin{pmatrix} t_x \\ t_y \end{pmatrix}$ </div>
</li>
<li><div>使用齐次坐标表示：$\begin{pmatrix} x' \\ y' \\1 \end{pmatrix} = \begin{pmatrix} a&b&t_x \\ c&d&t_y \\ 0&0&1\end{pmatrix}\begin{pmatrix} x \\ y \\ 1\end{pmatrix}$ </div>
</li>
<li><p>最后一行永远是 $(0,0,1)$，平移永远写在最后一列的前两个数，$a,b,c,d$  是线性变换</p>
</li>
</ul>
<p><u>所有的2D仿射变换都写成了 矩阵 × 向量 的形式</u>：</p>
<ul>
<li><div><b>Scale</b>：$\bold S(s_x, s_y) = \begin{pmatrix} s_x&0&0 \\ 0&s_y&0 \\ 0&0&1\end{pmatrix}$ </div></li>
<li><div><b>Rotation</b>：$\bold R(\alpha)=\begin{pmatrix} \cos\alpha&-\sin\alpha&0 \\ \sin\alpha&\cos\alpha&0 \\ 0&0&1\end{pmatrix}$ </div></li>
<li><div><b>Translation</b>：$\bold T(t_x,t_y)=\begin{pmatrix} 1&0&t_x \\ 0&1&t_y \\ 0&0&1\end{pmatrix}$ </div></li>
</ul>
<h4 id="4-Composing-Tranforms"><a href="#4-Composing-Tranforms" class="headerlink" title="4 - Composing Tranforms"></a>4 - Composing Tranforms</h4><h5 id="Inverse-Tranform"><a href="#Inverse-Tranform" class="headerlink" title="Inverse Tranform"></a>Inverse Tranform</h5><p>一个变换的逆变换，在数学上对应乘以变换矩阵的逆矩阵 $\bold M^{-1}$</p>
<h5 id=""><a href="#" class="headerlink" title=""></a></h5><h5 id="变换的组合"><a href="#变换的组合" class="headerlink" title="变换的组合"></a>变换的组合</h5><p>一个简单的变换：先旋转再平移</p>
<ul>
<li><p>一个复杂的变换可以通过一系列简单的变换组合而成</p>
</li>
<li><p>变换的顺序是重要的（默认旋转绕原点，先平移再旋转就会转到错误的位置）</p>
</li>
<li><p>矩阵不满足交换律</p>
<ul>
<li>$R_{45}\cdot T_{(1,0)} \ne T_{(1,0)}\cdot R_{45}$</li>
</ul>
</li>
<li><p>矩阵的乘法顺序是从右向左</p>
<ul>
<li><div>先旋转、再平移  $T_{(1,0)}\cdot R_{45}\begin{bmatrix} x \\ y \\ 1\end{bmatrix} = \begin{bmatrix} 1&0&1 \\ 0&1&0 \\ 0&0&1\end{bmatrix} \begin{bmatrix} \cos{45^{\circ}}&-\sin{45^{\circ}}&0 \\ \sin{45^{\circ}}&\cos{45^{\circ}}&0 \\ 0&0&1\end{bmatrix} \begin{bmatrix} x \\ y \\ 1\end{bmatrix}$ </div></li>
</ul>
</li>
</ul>
<p>推广两个变换到多个变换：</p>
<ul>
<li><div>$A_n(...A_2(A_1(x))) = \bold A_n \cdot \cdot \cdot \bold A_2\cdot \bold A_1 \cdot\begin{pmatrix}x\\y\\1\end{pmatrix}$ </div>
</li>
<li><p>应用顺序依然是从右到左</p>
</li>
<li><p>矩阵乘法没有交换律，但满足结合律</p>
<ul>
<li>可以先把 $\bold A_n \cdot \cdot \cdot \bold A_2\cdot \bold A_1 $ 求出一个矩阵，然后把这个矩阵应用到向量 </li>
<li>求得的这个矩阵依然是一个 3×3 的矩阵，即<u>可以用一个矩阵表示很复杂的变换</u>，变换多了不会让矩阵变复杂</li>
</ul>
</li>
</ul>
<h5 id="变换的分解"><a href="#变换的分解" class="headerlink" title="变换的分解"></a>变换的分解</h5><p>如何绕任意一个点旋转？</p>
<ul>
<li>由于旋转矩阵是绕原点旋转的，不能用单个旋转矩阵表示绕任意一个点旋转的变换</li>
<li>先移到原点、再旋转、再平移回去</li>
<li>写成矩阵形式，依然是从右到左</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/16.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h4 id="5-3D-Transforms"><a href="#5-3D-Transforms" class="headerlink" title="5 - 3D Transforms"></a>5 - 3D Transforms</h4><h5 id="三维空间的仿射变换"><a href="#三维空间的仿射变换" class="headerlink" title="三维空间的仿射变换"></a>三维空间的仿射变换</h5><p>简单扩展之前2D空间的表示方法（homogeneous coordinates）</p>
<ul>
<li>3D point &#x3D; $(x,y,z,1)^T$ </li>
<li>3D vector &#x3D; $(x,y,z,0)^T$ </li>
<li>In general, $(x,y,z,w),w\ne 0$ is the 3D point: $(x&#x2F;w,y&#x2F;w,z&#x2F;w)$</li>
</ul>
<p>3D Transformations，使用齐次坐标表示：</p>
<ul>
<li><div>$\begin{pmatrix} x' \\ y' \\ z' \\1 \end{pmatrix} = \begin{pmatrix} a&b&c&t_x \\ d&e&f&t_y \\ g&h&i&t_z \\ 0&0&0&1 \end{pmatrix} \cdot \begin{pmatrix} x \\ y \\ z \\ 1\end{pmatrix}$ </div></li>
<li>3×3 的矩阵是 3D 空间的线性变换，最后一列是平移，最后一行永远是 $(0,0,0,1)$</li>
</ul>
<p>一个问题：这个 4×4 矩阵应用在向量上，<u>平移 和 线性变换 的顺序是什么</u>？</p>
<ul>
<li>先线性变换、再平移</li>
<li>回顾2D的情况<ul>
<li><div>affine map = linear map + translation：$\begin{pmatrix} x' \\ y' \end{pmatrix} = \begin{pmatrix} a&b \\ c&d \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix} + \begin{pmatrix} t_x \\ t_y \end{pmatrix}$ </div></li>
<li><div>而不是 $\begin{pmatrix} x' \\ y' \end{pmatrix} = \begin{pmatrix} a&b \\ c&d \end{pmatrix}\begin{pmatrix} x+t_x \\ y+t_y \end{pmatrix} $ </div></li>
<li><div>使用 homogeneous coordinates 来表示这个变换：$\begin{pmatrix} x' \\ y' \\1 \end{pmatrix} = \begin{pmatrix} a&b&t_x \\ c&d&t_y \\ 0&0&1\end{pmatrix}\begin{pmatrix} x \\ y \\ 1\end{pmatrix}$ </div></li>
</ul>
</li>
<li>在3D中，也是一样的，<strong>先应用线性变换、再平移</strong></li>
</ul>
<hr>
<h3 id="Lecture-04-Transformation-Cont"><a href="#Lecture-04-Transformation-Cont" class="headerlink" title="Lecture 04 - Transformation Cont."></a>Lecture 04 - Transformation Cont.</h3><p>3D变换、Viewing 变换（视图&#x2F;相机变换，投影变换（正交变换、透视变换））</p>
<h4 id="1-3D-Transformations"><a href="#1-3D-Transformations" class="headerlink" title="1 - 3D Transformations"></a>1 - 3D Transformations</h4><h5 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h5><ul>
<li>3D point &#x3D; $(x,y,z,1)^T$ </li>
<li>3D vector &#x3D; $(x,y,z,0)^T$ </li>
<li><div>齐次坐标表示3D变换：$\begin{pmatrix} x' \\ y' \\ z' \\1 \end{pmatrix} = \begin{pmatrix} a&b&c&t_x \\ d&e&f&t_y \\ g&h&i&t_z \\ 0&0&0&1 \end{pmatrix} \cdot \begin{pmatrix} x \\ y \\ z \\ 1\end{pmatrix}$ ，先应用线性变换、再加上平移量</div></li>
</ul>
<h5 id="3D空间的若干变换"><a href="#3D空间的若干变换" class="headerlink" title="3D空间的若干变换"></a>3D空间的若干变换</h5><h6 id="缩放"><a href="#缩放" class="headerlink" title="缩放"></a>缩放</h6><ul>
<li><div>Scale：$\bold S(s_x, s_y, s_z) = \begin{pmatrix} s_x&0&0&0 \\ 0&s_y&0&0\\ 0&0&s_z&0 \\ 0&0&0&1 \end{pmatrix}$  </div></li>
</ul>
<h6 id="平移"><a href="#平移" class="headerlink" title="平移"></a>平移</h6><ul>
<li><div>Translation：$\bold T(t_x, t_y, t_z) = \begin{pmatrix} 1&0&0&t_x \\ 0&1&0&t_y\\ 0&0&1&t_z \\ 0&0&0&1 \end{pmatrix}$ </div></li>
</ul>
<h6 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h6><ul>
<li><p>绕 $x, y, z$ 轴旋转</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/17.png" style="zoom: 50%;" / loading="lazy"> 
</li>
<li><p>绕哪个轴旋转，对应的坐标就不会改变</p>
</li>
<li><div>$\bold R_x(\alpha) = \begin{pmatrix} 1&0&0&0 \\ 0&\cos\alpha&-\sin\alpha&0\\ 0&\sin\alpha&\cos\alpha&0 \\ 0&0&0&1 \end{pmatrix}$ </div>
</li>
<li><div>$\bold R_y(\alpha) = \begin{pmatrix} \cos\alpha&0&\sin\alpha&0 \\ 0&1&0&0\\ -\sin\alpha&0&\cos\alpha&0 \\ 0&0&0&1 \end{pmatrix}$ ，注意到是 $\vec z\times \vec x = \vec y$，跟 $x$ 和 $z$ 轴是相反的</div>
</li>
<li><div>$\bold R_z(\alpha) = \begin{pmatrix} \cos\alpha&-\sin\alpha&0&0 \\ \sin\alpha&\cos\alpha&0&0\\ 0&0&1&0 \\ 0&0&0&1 \end{pmatrix}$ </div></li>
</ul>
</li>
<li><p>由 $\bold R_x,\bold R_y,\bold R_z$ 组合的旋转</p>
<ul>
<li>定义 $\bold R_{xyz}(\alpha,\beta,\gamma)&#x3D;\bold R_x(\alpha)\bold R_y(\beta)\bold R_z(\gamma)$ </li>
<li>$\alpha,\beta,\gamma$ 称为欧拉角</li>
<li>对应飞机上下抬头（Pitch），左右转向（Yaw），歪过来转（Roll）</li>
</ul>
</li>
<li><p>任意旋转</p>
<ul>
<li><div>Rodrigues' Rotation Formula：$\bold R(\bold n, \alpha) = \cos(\alpha)\bold I + (1- \cos(\alpha))\bold{nn}^T +\sin\alpha \underbrace{\begin{pmatrix} 0&-n_z&n_y \\ n_z&0&-n_x \\ -n_y&n_x&0\end{pmatrix}}_{\bold N} $ </div>

<ul>
<li>这个矩阵是向量 $n$ 的对偶矩阵，查看第二课向量叉乘部分，对偶矩阵理解为一个向量的矩阵表示形式</li>
</ul>
</li>
<li>罗德里格斯旋转公式表达了<u>绕任意旋转轴 $\bold n$ 旋转任意角度 $\alpha$ 的变换</u>。注意，用一个向量 $\bold n $ 表示这个旋转轴，即默认这个轴过原点</li>
<li>如何过任意轴旋转？结合上节2D旋转的做法，<u>先平移到过原点的轴旋转、再进行罗德里格斯旋转、再平移回去</u></li>
</ul>
</li>
<li><p>四元数的概念：为了旋转之间的插值而提出的概念。旋转矩阵是不适合作为插值的，四元数可以跟旋转矩阵互相转化、并解决这个问题。</p>
</li>
</ul>
<h4 id="2-Viewing-的概念"><a href="#2-Viewing-的概念" class="headerlink" title="2 - Viewing 的概念"></a>2 - Viewing 的概念</h4><p>Viewing 分为 View&#x2F;Camera Transformation（视图变换） 和 Projection Tranformation（投影变换），可以通过现实生活中拍照的过程来理解：</p>
<ul>
<li>找到好位置，摆姿势：model transformation 模型变换</li>
<li>摄像机找好角度：view transformation 视图变换</li>
<li>拍照：projection transformation 投影变换</li>
</ul>
<h4 id="3-Viewing-之-View-x2F-Camera-Transformation"><a href="#3-Viewing-之-View-x2F-Camera-Transformation" class="headerlink" title="3 - Viewing 之 View &#x2F; Camera Transformation"></a>3 - Viewing 之 View &#x2F; Camera Transformation</h4><ul>
<li><p>如何定义一个相机？</p>
<ul>
<li>相机的位置 $\vec e$ </li>
<li>相机看向的方向 $\hat g$ </li>
<li>相机的上方向 $\hat t$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/18.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>相机的特性：跟物体、环境相对运动一致时，得到的结果相同</p>
</li>
<li><p>因此，为了简化操作，把相机放在标准位置上：</p>
<ul>
<li>位于原点，朝 $-z$ 方向看，向上方向是 $y$ </li>
<li>其他物体随着相机移动</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/19.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>进行相机标准化变换 $M_{view}$ </p>
<ul>
<li>原本相机在 $\vec e$ ，看向 $\hat g$ ，上方向 $\hat t$  ；目标相机在 $0$，看向 $-Z$，上方向 $Y$ </li>
<li>平移 $\vec e$ 到 $(0,0,0)$ </li>
<li>观察方向 $\hat g$ 旋转到 $-Z$ </li>
<li>上方向 $\hat t$ 旋转到 $Y$ </li>
<li>$\hat g\times \hat t$ 旋转到 $X$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/20.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>写成矩阵形式</p>
<ul>
<li><p>$M_{view}&#x3D;R_{view}T_{view}$ ，先平移再旋转（齐次坐标本来就是这样）</p>
</li>
<li><div>平移到原点：$T_{view}=\begin{bmatrix}1&0&0&-x_e \\ 0&1&0&-y_e \\ 0&0&1&-z_e \\ 0&0&0&1 \end{bmatrix}$ </div>
</li>
<li><p>旋转到正确方向，直接旋转不好写，但从目标方向旋转到原方向好些</p>
<ul>
<li><p>逆旋转：$X \rightarrow \hat g\times\hat t$，$Y\rightarrow \hat t$，$Z\rightarrow -\hat g$ </p>
</li>
<li><p>计算方法：带入 $(1,0,0) \rightarrow \hat g\times\hat t$，$(0,1,0)\rightarrow \hat t$，$(0,0,1)\rightarrow -\hat g$ </p>
</li>
<li><div>$R^{-1}_{view}=\begin{bmatrix}x_{\hat g\times \hat t}&x_t&x_{-g}&0 \\ y_{\hat g\times \hat t}&y_t&y_{-g}&0 \\ z_{\hat g\times \hat t}&z_t&z_{-g}&0 \\ 0&0&0&1 \end{bmatrix}$ </div>
</li>
<li><p>结合第三课中，旋转矩阵的性质：<u>旋转矩阵是正交矩阵，逆旋转变换的矩阵是旋转矩阵的转置</u>，得到旋转矩阵</p>
</li>
<li><div>$R_{view}=\begin{bmatrix}x_{\hat g\times \hat t}&y_{\hat g\times \hat t}&z_{\hat g\times \hat t}&0 \\ x_t&y_t&z_t&0 \\ x_{-g}&y_{-g}&z_{-g}&0 \\ 0&0&0&1 \end{bmatrix}$ </div></li>
</ul>
</li>
<li><p>由 $M_{view}&#x3D;R_{view}T_{view}$，得到相机的视图变换</p>
</li>
<li><p>其他物体也跟相机一起做相同的变换，也称为 Model View Transformation（模型视图变换）</p>
</li>
</ul>
</li>
</ul>
<h4 id="4-Viewing-之-Projection-transformation"><a href="#4-Viewing-之-Projection-transformation" class="headerlink" title="4 - Viewing 之 Projection transformation"></a>4 - Viewing 之 Projection transformation</h4><h5 id="透视投影的两种方式"><a href="#透视投影的两种方式" class="headerlink" title="透视投影的两种方式"></a>透视投影的两种方式</h5><ul>
<li><p>Orthographic projection：正交投影，常用于工程制图，不体现近大远小</p>
</li>
<li><p>Perspective projection：透视投影，鸽子为什么这么大</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/22.png" style="zoom: 67%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/21.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h5 id="Orthographic-Projection"><a href="#Orthographic-Projection" class="headerlink" title="Orthographic Projection"></a>Orthographic Projection</h5><h6 id="简单理解的做法"><a href="#简单理解的做法" class="headerlink" title="简单理解的做法"></a>简单理解的做法</h6><ul>
<li>相机位于原点，看向 $-Z$，上方向 $Y$ </li>
<li>丢掉 $Z$ 坐标，得到 $X,Y$ 平面上的图</li>
<li>标准化到 $[-1, 1]$  </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/23.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h6 id="图形学的做法"><a href="#图形学的做法" class="headerlink" title="图形学的做法"></a>图形学的做法</h6><ul>
<li>用三个轴各一个区间，定义一个3D空间的立方体 $[l,r]\times [b,t]\times [f,n]$ <ul>
<li>left, right, bottom, top, far, near</li>
<li>由于右手坐标系、沿着 $-Z$ 方向看，导致 far &lt; near</li>
<li>OpenGL等左手系在此处更容易理解，但带来 $X \times Y \ne Z$</li>
</ul>
</li>
<li>将这个立方体映射到 canonical（正则、规范、标准） 立方体 $[-1,1]^3$ </li>
<li>具体做法：把立方体的中点移动到原点，然后把三个轴都标准化到 $[-1,1]$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/24.png" style="zoom: 50%;" / loading="lazy"> </li>
<li><div>$M_{ortho}=\begin{bmatrix}\frac{2}{r-l}&0&0&0 \\ 0&\frac{2}{t-b}&0&0 \\ 0&0&\frac{2}{n-f}&0 \\ 0&0&0&1 \end{bmatrix} \begin{bmatrix}1&0&0&-\frac{r+l}{2} \\ 0&1&0&-\frac{t+b}{2} \\ 0&0&1&-\frac{n+f}{2} \\ 0&0&0&1 \end{bmatrix}$ </div>

<ul>
<li>写成齐次坐标的变换矩阵形式</li>
<li>先把中点平移到 $(0,0,0)$，然后把三个轴覆盖的长度都变为 $2$，对应 $[-1,1]$</li>
</ul>
</li>
<li>做完这个变换，所有的物体都会位于$[-1,1]^3$ 并被拉伸，在之后会做<strong>视口变换</strong>让它们恢复正常的形状</li>
</ul>
<h5 id="Perspective-Projection"><a href="#Perspective-Projection" class="headerlink" title="Perspective Projection"></a>Perspective Projection</h5><h6 id="透视投影的步骤"><a href="#透视投影的步骤" class="headerlink" title="透视投影的步骤"></a>透视投影的步骤</h6><ul>
<li>回顾：<ul>
<li>$(x,y,z,1)$，$(kx,ky,kz,k\ne0)$，$(zx,zy,z^2,z\ne0)$ 都表示3D中的同一个点 $(x,y,z)$ </li>
<li>如 $(1,0,0,1)$ 和 $(2,0,0,2)$ 表示同一个点</li>
</ul>
</li>
<li>透视投影中，把一个平面投影到另一个平面上，透视投影变换就是把两个屏幕对应点的映射关系</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/25.png" style="zoom: 50%;" / loading="lazy"></li>
<li>一种方法是直接求左图线表示的映射关系</li>
<li>另一种方法：先把左图的形状“挤压”成右图的矩形体（$M_{persp\rightarrow ortho}$），然后对于矩形体应用之前的正交投影<ul>
<li>在第一步操作下，near平面上的点不会变化，其他平面上的中心点也不会发生变化</li>
</ul>
</li>
<li>由于相机位于原点、过原点的直线延申得来两个平面，可以构建相似三角形<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/26.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>由相似三角形，可以得出任意一点 $(x,y,z)$ 挤压到 $(x’,y’,z’)$ 的计算方法 $y’&#x3D;\frac{n}{z}y,x’&#x3D;\frac{n}{z}x$ <ul>
<li>$z’\ne z$。除了near和far面的其他面，挤压过程中 $z$ 会发生变化，但目前不清楚变化方式</li>
<li>对于相似三角形，我的理解：<ul>
<li>这个图有一个误导： $(x,y,z)$ 经过挤压变成了near平面上的 $(x’,y’,z’)$ </li>
<li>事实上：相似三角形展示了 $x$ 和 $y$ 在挤压中的变化方式，也就是经过挤压，都会跟near平面的 $x’,y’$ 相同，并且对于每个 $x,y$ 都是如此，可以看上面的挤压图中的连线</li>
<li>也就是相似三角形并不展示 $z$ 的变化，$(x,y,z)$ 平移到下方 $y’$ 高度上的某个点，而不是平移到 $(x’,y’,n)$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/413.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="计算“挤压”的变换矩阵"><a href="#计算“挤压”的变换矩阵" class="headerlink" title="计算“挤压”的变换矩阵"></a>计算“挤压”的变换矩阵</h6><ul>
<li><div>以齐次坐标表示每个点“挤压”的变化：$\begin{pmatrix} x \\ y \\ z \\ 1\end{pmatrix} \rightarrow \begin{pmatrix} nx/z \\ ny/z \\ \text{unknown} \\ 1\end{pmatrix} ==\begin{pmatrix} nx \\ ny \\ \text{still unknown} \\ z\end{pmatrix}$ </div>

<ul>
<li>使用到之前说的，乘一个数依然是同一个点</li>
</ul>
</li>
<li><div>“挤压”的变换矩阵的作用就是：$M_{persp->ortho}^{(4\times 4)}\begin{pmatrix} x \\ y \\ z \\ 1\end{pmatrix} = \begin{pmatrix} nx \\ ny \\ \text{still unknown} \\ z\end{pmatrix}$ </div>
</li>
<li><div>目前可求得：$M_{persp->ortho}^{(4\times 4)}=\begin{pmatrix}n&0&0&0 \\ 0&n&0&0 \\ ?&?&?&? \\ 0&0&1&0 \end{pmatrix}$ </div>

<ul>
<li>这不是仿射变换，最后一行不一定是 $(0,0,0,1)$。在这里是 $(0,0,1,0)$</li>
</ul>
</li>
<li><p>已知 near 和 far 两个平面的变换关系</p>
<ul>
<li><div>在 near 平面上的任意点，运算完后不会发生任何变化，$\begin{pmatrix} x \\ y \\ n \\ 1\end{pmatrix} \rightarrow \begin{pmatrix} x \\ y\\ n\\1\end{pmatrix} ==\begin{pmatrix} nx \\ ny\\ n^2\\n\end{pmatrix}$ </div>

<ul>
<li><div>因此，矩阵的第三行 $\begin{pmatrix}A&B&C&D\end{pmatrix} \begin{pmatrix}x\\y\\n\\1\end{pmatrix} = n^2$ ，得 $\left \{ \begin{array}{l} A = 0 \\ B=0 \\Cn+D=n^2 \end{array}  \right.$ </div></li>
</ul>
</li>
<li><div>在 far 平面上的任意点，运算完后 $z$ 不会发生变化，$\begin{pmatrix} x \\ y \\ f \\ 1\end{pmatrix} \rightarrow \begin{pmatrix} nx/f \\ ny/f\\ f\\1\end{pmatrix} ==\begin{pmatrix} nx \\ ny\\ f^2\\f\end{pmatrix}$ </div></li>
<li><div>或者 far平面的中心点，运算完后不发生任何变化，$\begin{pmatrix} 0 \\ 0 \\ f \\ 1\end{pmatrix} \rightarrow \begin{pmatrix} 0 \\ 0\\ f\\1\end{pmatrix} ==\begin{pmatrix} 0 \\ 0 \\ f^2\\f\end{pmatrix}$ </div>

<ul>
<li><div>矩阵第三行 $\begin{pmatrix}0&0&C&D\end{pmatrix} \begin{pmatrix}x\\y\\f\\1\end{pmatrix} = f^2$ ，得 $\left \{ \begin{array}{l} A = 0 \\ B=0 \\Cf+D=f^2 \end{array}  \right.$ </div></li>
</ul>
</li>
</ul>
</li>
<li><div>联立 $\left \{ \begin{array}{l} Cf+D=f^2 \\Cn+D=n^2 \end{array}  \right.$ ，得 $\left \{ \begin{array}{l} C=n+f \\D=-nf \end{array}  \right.$ </div></li>
<li><div>$M_{persp->ortho}^{(4\times 4)}=\begin{pmatrix}n&0&0&0 \\ 0&n&0&0 \\ 0&0&n+f&-nf \\ 0&0&1&0 \end{pmatrix}$ </div></li>
<li><div>一点思考：$M_{persp->ortho}\begin{pmatrix}x\\y\\z\\1\end{pmatrix} =\begin{pmatrix}nx\\ny\\nz+fz-nf\\z\end{pmatrix}=\begin{pmatrix}nx/z\\ny/z\\n+f-nf/z\\1\end{pmatrix} $ </div>

<ul>
<li>$z\rightarrow n+f-\frac{nf}{z}$，求解一元二次方程（见草稿纸）后发现，在“挤压”变换中 $z$ 变小（<strong>被推向far</strong>）</li>
<li>也可以直接带几个数进去算</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/414.png" style="zoom: 80%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h6 id="完整的-Perspective-Projection"><a href="#完整的-Perspective-Projection" class="headerlink" title="完整的 Perspective Projection"></a>完整的 Perspective Projection</h6><div>$M_{persp}=M_{ortho}M_{persp\rightarrow ortho} = \begin{bmatrix}\frac{2}{r-l}&0&0&0 \\ 0&\frac{2}{t-b}&0&0 \\ 0&0&\frac{2}{n-f}&0 \\ 0&0&0&1 \end{bmatrix} \begin{bmatrix}1&0&0&-\frac{r+l}{2} \\ 0&1&0&-\frac{t+b}{2} \\ 0&0&1&-\frac{n+f}{2} \\ 0&0&0&1 \end{bmatrix} \begin{bmatrix}n&0&0&0 \\ 0&n&0&0 \\ 0&0&n+f&-nf \\ 0&0&1&0 \end{bmatrix}$ </div>

<hr>
<h3 id="Lecture-05-Rasterization-1-Triangles"><a href="#Lecture-05-Rasterization-1-Triangles" class="headerlink" title="Lecture 05 - Rasterization 1 (Triangles)"></a>Lecture 05 - Rasterization 1 (Triangles)</h3><p>上节课讲的是MVP的过程：</p>
<ul>
<li>Model Transformation，放置物体</li>
<li>View Transformation，相机移动到原点、其他物体一起移动</li>
<li>Projection Transformation，物体投影到 $[1-,1]^3$ 的立方体空间中<ul>
<li>Orthograpic Projection</li>
<li>Perspective Projection</li>
</ul>
</li>
</ul>
<p>MVP后，场景中的所有物体都可以投影到 $[1-,1]^3$ 的立方体空间中。下一步该做什么？</p>
<p>将物体画在屏幕上，这一步叫光栅化（Rasterization）。</p>
<h4 id="1-透视投影其他"><a href="#1-透视投影其他" class="headerlink" title="1 - 透视投影其他"></a>1 - 透视投影其他</h4><h5 id="如何定义一个透视投影的frustum"><a href="#如何定义一个透视投影的frustum" class="headerlink" title="如何定义一个透视投影的frustum"></a>如何定义一个透视投影的frustum</h5><ul>
<li>可以指定 near，far 平面分别的 left，right，bottom，top</li>
<li>也可以用相机的两个参数指定一个平面<ul>
<li>vertical <strong>field-of-view</strong>：相机垂直可视角度</li>
<li><strong>aspect ratio</strong>：一个平面的宽高比</li>
<li>有这两个值，当然也可以得到 horizontal fov</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/27.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>有了这两个概念，也可以转换到 left，right，bottom，top<ul>
<li>通过下图的两个关系，可以求得 top、right，进而求得 bottom、left（假设是中心对称的）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/28.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="2-视口变换（把立方体画在屏幕上）"><a href="#2-视口变换（把立方体画在屏幕上）" class="headerlink" title="2 - 视口变换（把立方体画在屏幕上）"></a>2 - 视口变换（把立方体画在屏幕上）</h4><h5 id="概念理解"><a href="#概念理解" class="headerlink" title="概念理解"></a>概念理解</h5><h6 id="屏幕"><a href="#屏幕" class="headerlink" title="屏幕"></a>屏幕</h6><ul>
<li>屏幕是一个二维数组，存的是像素<ul>
<li>pixel 是 picture element 的缩写</li>
<li>在本课中，把 pixel 抽象理解成一个小方块，是颜色划分的最小单位</li>
<li>一个像素内只有唯一的颜色，用RGB表示</li>
</ul>
</li>
<li>数组的大小：分辨率，如 1920×1080</li>
<li>屏幕是一个典型的光栅成像设备（raster display）<ul>
<li>raster &#x3D;&#x3D; screen in German</li>
<li>rasterize &#x3D;&#x3D; 把东西画在屏幕上</li>
</ul>
</li>
</ul>
<h6 id="屏幕空间（定义方法跟虎书有点差别）"><a href="#屏幕空间（定义方法跟虎书有点差别）" class="headerlink" title="屏幕空间（定义方法跟虎书有点差别）"></a>屏幕空间（定义方法跟虎书有点差别）</h6><ul>
<li>屏幕左下角是原点，右、上分别是 $x$，$y$ 方向</li>
<li>像素坐标定义为 $(x, y)$ ，所有的像素从 $(0,0)$ 到 $(\text{width}-1,\text{height}-1)$ </li>
<li>用整数的坐标来描述像素，但像素的中心是 $(x+0.5, y+0.5)$ </li>
<li>整个屏幕的像素覆盖的坐标范围从 $(0,0)$ 到 $(\text{width},\text{height})$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/29.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h5 id="视口变换"><a href="#视口变换" class="headerlink" title="视口变换"></a>视口变换</h5><p>需要做的是：从$[-1,1]^3$ 到 $[0,\text{width}]\times[0,\text{height}]$ 的转换</p>
<ul>
<li>先不管 $z$，先做从$[-1,1]^2$ 到 $[0,\text{width}]\times[0,\text{height}]$ <ul>
<li><p>$x,y$ 方向的长度先从 2 都 scale 到 width、height，再平移到 width、height 的中点</p>
</li>
<li><p>这里是从中点在原点，平移到左下角在原点，因此是正的</p>
</li>
<li><div>$M_{viewport}=\begin{pmatrix}\frac{\text{width}}{2}&0&0&\frac{\text{width}}{2} \\ 0&\frac{\text{height}}{2}&0&\frac{\text{height}}{2} \\ 0&0&1&0 \\ 0&0&0&1 \end{pmatrix}$ </div>
</li>
<li><p>这个变化称为<strong>视口变换 Viewport Transformation</strong>。做完这一步后，3D空间任意物体的 $x,y$ 都在2D屏幕上了</p>
</li>
</ul>
</li>
</ul>
<p>得到了2D空间的物体，下一步需要把物体打散成像素，真正画在屏幕上。</p>
<h4 id="3-光栅化（Rasterizing）"><a href="#3-光栅化（Rasterizing）" class="headerlink" title="3 - 光栅化（Rasterizing）"></a>3 - 光栅化（Rasterizing）</h4><h5 id="光栅显示设备介绍"><a href="#光栅显示设备介绍" class="headerlink" title="光栅显示设备介绍"></a>光栅显示设备介绍</h5><ul>
<li><p>Oscilloscope，示波器</p>
<ul>
<li>成像原理：跟阴极射线管（Cathode Ray Tube）电视机原理相同，电子发射后经过偏转、打在屏幕上的某个位置；配合隔行扫描等优化技术</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/30.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>Frame Buffer：显示内存中的一块区域</p>
<ul>
<li>显卡内存的一块区域映射到屏幕上。也可以生成不同图像，存在显存不同区域，然后指定显示器显示哪张</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/31.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>平板显示设备 LCD，OLED等：计算器，手机等</p>
<ul>
<li>LCD（Liquid Crystal Display，液晶显示器）：液晶通过自己的不同排布，影响光的偏振方向</li>
<li>光经过竖直的光栅后是不能再通过水平光栅的，但液晶可以通过排布，将光扭曲成水平的，进而通过水平光栅</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/32.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>LED（Light emitting diode，发光二极管）</p>
</li>
<li><p>Electrophoretic 电子墨水屏：Kindle</p>
<ul>
<li>经过不同电压，可以把黑白墨水进行排布，刷新率低</li>
</ul>
</li>
</ul>
<h5 id="三角形"><a href="#三角形" class="headerlink" title="三角形"></a>三角形</h5><h6 id="为什么是三角形"><a href="#为什么是三角形" class="headerlink" title="为什么是三角形"></a>为什么是三角形</h6><ul>
<li>△是最基础的图形，可以表示复杂的图形</li>
<li>有一些独特的性质<ul>
<li>一定在一个平面上</li>
<li>有清晰定义的内、外部，可以用叉乘判断点是否在三角形内部（之前讲的）</li>
<li>定义三角形的三个顶点后，内部可以通过任意点跟顶点的位置关系，得到渐变的关系<ul>
<li>用于中心坐标的插值方法</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><u>通过前几步MVP变换、视口变换，物体的 $x,y$ 都投影到了屏幕空间中，每个3D中的三角形，都可以在屏幕中找到它的3个顶点</u>。怎样把三角形变为像素？</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/33.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
<h6 id="判断像素和三角形的位置关系"><a href="#判断像素和三角形的位置关系" class="headerlink" title="判断像素和三角形的位置关系"></a>判断像素和三角形的位置关系</h6><p>像素是正方形。考虑像素的<strong>中心点</strong>跟三角形的位置关系</p>
<h5 id="一个简单的光栅化方法：Sampling（采样）"><a href="#一个简单的光栅化方法：Sampling（采样）" class="headerlink" title="一个简单的光栅化方法：Sampling（采样）"></a>一个简单的光栅化方法：Sampling（采样）</h5><h6 id="采样的概念"><a href="#采样的概念" class="headerlink" title="采样的概念"></a>采样的概念</h6><ul>
<li>给定一个连续的函数，在不同的地方，问函数的值是多少</li>
<li>采样就是把一个函数<u>离散化</u>的过程</li>
<li><code>for(int x=0;x&lt;xmax;++x)  output[x] = f(x);</code> </li>
<li>此处，利用像素的中心，对屏幕空间进行采样</li>
<li>采样是图形学的重要概念。还会对 time(1D)，area(2D)，direction(2D)，volume(3D) 进行采样</li>
</ul>
<h6 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h6><p>定义一个函数 <code>inside(t, x, y)</code>，判断像素 $(x,y)$ 的中心 $(x+0.5,y+0.5)$ 是否在三角形 $t$ 内</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/34.png" style="zoom: 50%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/35.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<pre class="language-c++" data-language="c++"><code class="language-c++">for(int x&#x3D;0;x&lt;xmax;++x)
    for(int y&#x3D;0;y&lt;ymax;++y)
        image[x][y] &#x3D; inside(tri, x+0.5, y+0.5);</code></pre>



<p>使用向量叉乘的方法实现 <code>inside(t, x, y)</code></p>
<ul>
<li>把三角形安排成某种顺序，比如△ABC，循环成ABCABC，找到 $\vec {AB},\vec{BC},\vec{CA}$ </li>
<li>跟  $\vec {AP},\vec{BP},\vec{CP}$ 叉乘，如果结果全是正向量或全是负向量，说明点 $P$ 在△ABC内部</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/36.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
<p>如果点正好在三角形的边界上怎么办？</p>
<ul>
<li>比如边上的点是否属于三角形；两个三角形交界处的点，是都属于还是都不属于，还是属于一边</li>
<li>对于图形学遇到的 Edge Cases，要么不做处理，要么特殊处理</li>
<li>对于本课，不做处理</li>
<li>在OpenGL，规定上、左边算在内部，下、右边不算</li>
</ul>
<p><u>对于一个三角形，只检查它 Bounding Box 范围内的像素</u> </p>
<ul>
<li><p>一个简单方法是取轴向包围盒AABB，即对三个点取 $x,y$ 的最大、最小值</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/37.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>还有方法对于每行找最左和最右，不多考虑任何一个像素</p>
<ul>
<li>适合于又细又斜的三角形，AABB很大</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/38.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h6 id="实际的光栅化"><a href="#实际的光栅化" class="headerlink" title="实际的光栅化"></a>实际的光栅化</h6><p>理论上，光栅化就是：<strong>对于每个可能的像素，检测其是否位于三角形内部</strong> </p>
<p>实际上屏幕的光栅化</p>
<ul>
<li><p>手机</p>
<ul>
<li><p>iPhone的一个像素分为红绿蓝三个条；Galaxy的一个像素是 bayer pattern，绿色点更多（因为人眼对绿色更敏感）</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/39.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>打印机</p>
<ul>
<li>减色系统，cmyk</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/40.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>在本课中，依然认为：<u>每个像素的内部，是一个颜色均匀的小方块</u> </p>
<h6 id="一个小问题"><a href="#一个小问题" class="headerlink" title="一个小问题"></a>一个小问题</h6><ul>
<li><p>光栅化图形学中，有一个严重问题：<strong>锯齿 Jaggies</strong> </p>
</li>
<li><p>产生的原因：像素本身有一定大小，并且像素的<u>采样率</u>对于信号是不够高的，产生了信号的走样问题</p>
</li>
<li><p>带来了抗锯齿&#x2F;反走样技术</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/41.png" style="zoom: 50%;" / loading="lazy"> </li>
<li><p>（为什么通过采样的概念来分析这个问题？）</p>
</li>
</ul>
<hr>
<h3 id="Lecture-06-Rasterization-2-Anti-aliasing-and-Z-Buffering"><a href="#Lecture-06-Rasterization-2-Anti-aliasing-and-Z-Buffering" class="headerlink" title="Lecture 06 - Rasterization 2 (Anti-aliasing and Z-Buffering)"></a>Lecture 06 - Rasterization 2 (Anti-aliasing and Z-Buffering)</h3><p>Rasterization</p>
<ul>
<li><p>Viewing</p>
<ul>
<li>View &#x2F; Camera + Projection(Orthographic, Perspective) + Viewport</li>
</ul>
</li>
<li><p>Rasterizing triangles</p>
<ul>
<li>Point-in-triangle test</li>
<li>Aliasing</li>
</ul>
</li>
<li><p>Antialiasing</p>
<ul>
<li>Sampling theory</li>
<li>Antialiasing in practice</li>
</ul>
</li>
<li><p>Visibility &#x2F; Occlusion</p>
<ul>
<li>Z-buffering</li>
</ul>
</li>
</ul>
<h4 id="1-信号处理概念简述"><a href="#1-信号处理概念简述" class="headerlink" title="1 - 信号处理概念简述"></a>1 - 信号处理概念简述</h4><h5 id="1-1-采样和-Aliasing"><a href="#1-1-采样和-Aliasing" class="headerlink" title="1.1 采样和 Aliasing"></a>1.1 采样和 Aliasing</h5><h6 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h6><p>采样是计算机图形学的常用方法，在时域、空间概念上都会用到。可以理解视频为每帧进行一次采样，然后排列起来。</p>
<p><u>Sampling Artifacts</u> 概念为：在图形学中，一切看上去不对的结果。</p>
<p>由采样引起的 Artifacts，称为 “Aliasing”：</p>
<ul>
<li>Jaggies，空间中采样出现的锯齿问题</li>
<li>Moire Pattern，摩尔纹，对图片下采样</li>
<li>Wagon wheel effect，时间中采样，人眼的采样率跟不上物体的运动速度，导致看到相反的旋转方向</li>
</ul>
<p>Aliasing Artifacts 问题的成因：</p>
<ul>
<li><u>信号变化得太快，而采样速度太慢</u></li>
</ul>
<h6 id="-1"><a href="#-1" class="headerlink" title=""></a></h6><h6 id="抗锯齿-x2F-反走样技术"><a href="#抗锯齿-x2F-反走样技术" class="headerlink" title="抗锯齿 &#x2F; 反走样技术"></a>抗锯齿 &#x2F; 反走样技术</h6><p>根据之前的步骤，视口变换将 $[-1,1]^3$ 的物体画在屏幕空间中；然后进行光栅化，对可能在三角形内的像素中心点进行采样。在采样后，出现了 Jaggies（锯齿），这个问题叫做 Aliasing。</p>
<p>针对这个问题，使用抗锯齿 &#x2F; 反走样技术。</p>
<p>一种技术：<u>采样之前做模糊（低通滤波），然后再做采样</u> </p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/42.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>先模糊、再采样的方法称为 Blurred Aliasing</li>
<li>如果反过来，先采样、再模糊，达不到期望的效果</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/43.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>更本质上的问题：</p>
<ul>
<li>为什么采样速度跟不上信号变化的速度，就会产生走样？</li>
<li>为什么先采样、再模糊，达不到反走样的效果？</li>
</ul>
<p>对这些问题进行分析，需要频域方面的知识</p>
<h5 id="1-2-傅里叶变换，频域和时域"><a href="#1-2-傅里叶变换，频域和时域" class="headerlink" title="1.2 傅里叶变换，频域和时域"></a>1.2 傅里叶变换，频域和时域</h5><h6 id="频域（Frequency-Domain）"><a href="#频域（Frequency-Domain）" class="headerlink" title="频域（Frequency Domain）"></a>频域（Frequency Domain）</h6><ul>
<li>通过正弦、余弦波函数的系数 $f$，可以定义函数的频率、周期<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/44.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
</li>
<li>在微积分中，可以对一个函数进行<strong>傅里叶级数展开</strong> <ul>
<li>任何一个周期函数，都可以写成一系列正弦、余弦函数的线性组合，以及一个常数项</li>
<li>图中展示了一个像城墙一样的函数的拟合过程，从上到下函数依次加入4个正&#x2F;余弦函数项，左边是各个函数的图像，右边是加起来的拟合结果</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/45.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>傅里叶级数展开跟<strong>傅里叶变换</strong>的概念是相似的<ul>
<li>一个函数可以通过复杂的变化，转变成另一个函数，并且能通过逆变换再转换回来</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/46.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>通过傅里叶级数展开，可以发现，任何一个函数都可以分解成<u>不同的频率函数的线性组合</u>，具体来说是不同频率从低到高的组合</li>
<li>由此，傅里叶变换是把函数变成不同频率的段，并把不同频率的段显示出来</li>
</ul>
</li>
<li>采样频率和函数频率<ul>
<li>从上到下，频率越来越高的五个函数，对于每个函数进行相同频率采样点的采样</li>
<li>发现：频率越高，越无法通过采样，将函数“恢复”，这就代表函数丢失了一部分高频信息</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/47.png" style="zoom: 50%;" / loading="lazy"> </li>
<li><u>采样频率跟不上函数变换频率，就不能通过采样，将函数的信息恢复出来</u> </li>
<li>此外可以发现，使用同样的采样率，采样不同的函数（如下图蓝、黑两个函数），会得到完全一致的采样结果</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/48.png" style="zoom: 50%;" / loading="lazy"> </li>
<li><u>用同样的采样方法采样不同的函数，得出的结果无法区分</u>，这就是<strong>走样 &#x2F; 混叠（aliases）</strong>的概念</li>
</ul>
</li>
<li>补充：时域和频域的理解<ul>
<li>傅里叶变换把函数的时域和频域联系起来</li>
<li>关于时域和频域，可以参考<a href="https://www.zhihu.com/question/21040374/answer/37911622">这一个问题下的回答</a> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/415.gif" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="1-3-滤波，卷积，平均"><a href="#1-3-滤波，卷积，平均" class="headerlink" title="1.3 滤波，卷积，平均"></a>1.3 滤波，卷积，平均</h5><h6 id="滤波，高频和低频"><a href="#滤波，高频和低频" class="headerlink" title="滤波，高频和低频"></a>滤波，高频和低频</h6><p>Filtering（<strong>滤波</strong>）：把特定频段的频率删掉</p>
<p><u>傅里叶变换可以把函数从时域变到频域</u>。傅里叶变换让我们看到任何信号（包括图像）在各个不同的频率是什么样子，称为<strong>频谱</strong>。</p>
<p>频谱图：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/49.png" style="zoom: 33%;" / loading="lazy"> </li>
<li><u>频谱图中间的频率低，周围的频率高</u> </li>
<li>对于大多数图片，信息都集中在低频</li>
<li>由于图片本身不是周期性重复的信号，就认为它是水平、竖直堆叠复制的，在两张图边界部分会产生极其高的高频，因此出现了右图水平、竖直两条线。对于图片内部的分析，可以忽略这两条线</li>
</ul>
<p>高通滤波：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/50.png" style="zoom: 33%;" / loading="lazy"> </li>
<li>把低频过滤掉，只留高频，再进行逆傅里叶变换，发现原图像留下了内容上的边缘（理解为原图边界处发生剧烈的变化，信号蕴含高频的信息）</li>
<li>称为<strong>高通滤波</strong></li>
</ul>
<p>低通滤波：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/51.png" style="zoom: 33%;" / loading="lazy"> </li>
<li>反过来留下低频、过滤高频，称为<strong>低通滤波</strong>，也就是舍弃了图片变化大的部分（比如边界）</li>
<li>逆傅里叶变换的图像的意义是进行了模糊处理，整个图片的变化减小</li>
<li>逆傅里叶变换的图像有些水波纹，这是不完美的低通滤波产生的问题</li>
</ul>
<p>某一频段滤波：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/52.png" style="zoom: 33%;" / loading="lazy"> </li>
<li>去掉高频、也去掉低频，留下某一频段的信息，对应原图剩下了不是很明显的边界特征</li>
<li>去掉了最外面的边界（对应高频信息）、去掉了内部的色块（对应低频信息）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/53.png" style="zoom: 33%;" / loading="lazy"> </li>
<li>对于上一步，留下更高频的信息，对应到图片上也更接近高频的边界</li>
</ul>
<p>这是数字图像处理的内容，最近对于图像的处理已经更多使用深度学习技术，而不直接做频率上的变化了</p>
<h6 id="Filtering滤波-x3D-Convolution卷积（-x3D-Averaging平均）"><a href="#Filtering滤波-x3D-Convolution卷积（-x3D-Averaging平均）" class="headerlink" title="Filtering滤波 &#x3D; Convolution卷积（ &#x3D; Averaging平均）"></a>Filtering滤波 &#x3D; Convolution卷积（ &#x3D; Averaging平均）</h6><ul>
<li>平均<ul>
<li>低通滤波对应图像模糊，也就是<u>图像像素上的“平均”操作</u></li>
</ul>
</li>
<li>卷积<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/54.png" style="zoom: 33%;" / loading="lazy"> </li>
<li>把窗口在信号上移动，窗口的权重跟窗口覆盖的信号值进行点乘</li>
<li>卷积得到的是：<u>信号每个位置在周围的加权“平均”</u></li>
</ul>
</li>
<li>卷积定理<ul>
<li>时域上，对两个信号做卷积，对应到两个信号各自的频域上，是各自频域上的乘积（<strong>时域的卷积是频域的乘积</strong>）<ul>
<li>在时域上的乘积，意味着是在频域上的卷积</li>
</ul>
</li>
<li>卷积操作<ul>
<li>对于一张图，可以直接用卷积的滤波器（卷积核），对图进行卷积操作</li>
<li>也可以用傅里叶变换，将图变到频域上，再把卷积滤波器变到频域上，把两者相乘得到频域的结果；再做逆傅里叶变换，变回时域上的图像</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/55.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>如图，可以直接使用卷积核，对图做卷积</li>
<li>也可以把图、卷积核都做傅里叶变换到频域上，然后在频域上把两个频谱相乘，得到的结果再逆傅里叶变换成图</li>
</ul>
</li>
<li>卷积核<ul>
<li>上图中，卷积核的元素都是1，它的作用是：不改变颜色，但对于 $3\times3$ 范围内做平均。观察卷积结果，发现图像被模糊</li>
<li>在频域上可以看出，这个滤波器基本上是<u>低通滤波</u>（保留频域图中间的低频部分）。跟图片相乘，只留下图片的低频信息</li>
<li>说明：<strong>卷积 &#x3D; 低通滤波 &#x3D; 平均</strong></li>
<li>box filter也称为低通滤波器<ul>
<li>如果用更大的box，像素的取平均范围变大，对应更模糊的图像，也就是更低频率的低通滤波</li>
<li>也就是，box越大，对应的频域图像越小</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/56.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="1-4-回到采样和-Aliasing"><a href="#1-4-回到采样和-Aliasing" class="headerlink" title="1.4 回到采样和 Aliasing"></a>1.4 回到采样和 Aliasing</h5><h6 id="Sampling-采样-x3D-重复频域上的内容"><a href="#Sampling-采样-x3D-重复频域上的内容" class="headerlink" title="Sampling 采样 &#x3D; 重复频域上的内容"></a>Sampling 采样 &#x3D; 重复频域上的内容</h6><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/57.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>左边都是时域，右边都是频域</li>
<li>(a) 的傅里叶变换结果是 (b)</li>
<li>(c) <strong>冲激函数</strong>的定义是，只在某一些位置上有值 <ul>
<li>(d) 是经过傅里叶变换的冲激函数，依然有冲激函数的性质</li>
</ul>
</li>
<li>用 (c) 乘以 (a) 函数，会得到一个个离散的点，也就是 (e)，这就是采样的过程<ul>
<li><u>时域上：给定一个原始信号 (a)，乘上冲激函数 (c)，得到采样的结果</u></li>
</ul>
</li>
<li>把 (b) 和 (d) 进行卷积，同样得到频域上的采样结果 (f)<ul>
<li><strong>时域上的卷积相当于频域上的乘积</strong> </li>
<li><u>频域上：给定一个原始频域 (b)，跟冲激函数的频域 (d) 做卷积，得到采样的结果</u></li>
</ul>
</li>
<li>发现：时域上 (e) 作为采样的结果，频域上 (f) 其实是原始的频谱 (b) <strong>复制粘贴</strong>了很多份 </li>
<li><strong>采样就是在重复一个原始信号的频谱</strong></li>
</ul>
<h6 id="Aliasing-走样"><a href="#Aliasing-走样" class="headerlink" title="Aliasing 走样"></a>Aliasing 走样</h6><ul>
<li>为什么会产生走样现象？<ul>
<li>采样就是在重复一个原始信号的频谱，以不同频率采样，就对应不同间隔（步长）的重复。</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/58.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>图中是频域上的复制关系，时域上采样率高，也就是周期小，到了频域上 $f$ 更大，也就是搬移间隔大</li>
<li>采样率低时，搬移的间隔小，频谱的复制密集，叠在了一起</li>
<li>这种情况叫 <strong>走样 &#x2F; 混叠 Aliasing</strong></li>
</ul>
</li>
</ul>
<h4 id="2-反走样方法-antialiasing"><a href="#2-反走样方法-antialiasing" class="headerlink" title="2 - 反走样方法 antialiasing"></a>2 - 反走样方法 antialiasing</h4><h5 id="理论解决思路"><a href="#理论解决思路" class="headerlink" title="理论解决思路"></a>理论解决思路</h5><ul>
<li>从根本上解决：增加采样率<ul>
<li>增加屏幕的分辨率，像素小、采样率高，频谱的搬移间隔大，不容易出现混叠</li>
</ul>
</li>
<li>antialiasing方法<ul>
<li>先做模糊，再做采样<ul>
<li>模糊：低通滤波，去掉高频信息，让频谱覆盖的范围小一些</li>
<li>采样：频谱范围变小，不容易发生混叠</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/59.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>实际操作中，怎样进行模糊操作？<ul>
<li>用一定大小的低通滤波器进行卷积即可。前面提到，box filter就是低通滤波器</li>
<li>具体来说，使用 1-pixel box filter，对于每个像素，计算它的平均值（因为光栅化后，像素可能被覆盖一部分，1-pixel box filter可以求出像素的平均颜色）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/60.png" style="zoom: 50%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/61.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="实际方法"><a href="#实际方法" class="headerlink" title="实际方法"></a>实际方法</h5><ul>
<li><p>理论上，对于每个像素求覆盖的区域，然后取平均，但很难实现</p>
</li>
<li><p>近似方法：<strong>MultiSampling Anti-Aliasing（MSAA，多重采样抗锯齿）</strong> </p>
<ul>
<li><p>认为一个像素划分为很多小的像素</p>
<ul>
<li>对于每个小像素的中心，判断其是否在图形内</li>
<li>然后把每个像素中的所有小像素计算结果取平均，作为整个像素的结果</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/62.png" style="zoom: 50%;" / loading="lazy">  
</li>
<li><p>如图使用 $2\times2$ ，一像素有 3 个小像素位于三角形内，则认为该像素的值是 75%</p>
</li>
</ul>
</li>
<li><p>注意：MSAA实际只对应抗锯齿中的模糊操作，并没有采样。MSAA只是近似得出一个合理的覆盖率，并不是提高了分辨率，而直接解决 aliasing 问题。</p>
</li>
<li><p>其他</p>
<ul>
<li><p>NFL定律中，MSAA带来的cost是什么？</p>
<ul>
<li><p>增大计算量</p>
</li>
<li><p>工业界并不是完全如同 $4\times4$ 做像素的划分，而是按更合理的图案分割，有些点还可以被临近不同的像素复用</p>
</li>
<li><p>因此，采用 $4\times4$ 的抗锯齿，不会直接让帧率变小 4 倍</p>
</li>
</ul>
</li>
<li><p>其他的抗锯齿方案</p>
<ul>
<li>FXAA（Fast Approximate AA，快速近似抗锯齿）：<ul>
<li>不增加样本数，而是是一个图像的后处理</li>
<li>得到有锯齿的图 -&gt; 把锯齿边界找到 -&gt; 换成没有锯齿的边界</li>
</ul>
</li>
<li>TAA（Temporal AA，时间抗锯齿）<ul>
<li>复用上一帧像素的信息</li>
<li>相当于把MSAA对应的样本分布在时间上，对于运动的物体在之后实时光线追踪再说（因为实时光线追踪核心思想跟TAA一样）</li>
</ul>
</li>
</ul>
</li>
<li><p>超分辨率 Super resolution &#x2F; Super sampling</p>
<ul>
<li>把小分辨率的图转换成高分辨率的图，同时不看到锯齿</li>
<li>跟MSAA都是解决采样样本不足（采样率不够）的问题，本质相同</li>
<li>一种解决方法：DLSS（Deep Learning Super Sampling）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>aliasing现象出现的原因是信号变换得太快，而采样率太慢。为了解决这个问题，要么提高采样率（用更高分辨率、划分出更多像素），要么使用“先做模糊再采样”的antialiasing方法。</p>
<p>傅里叶变换建立了时域到频域的联系，我们得知：时域上的卷积相当于频域上的乘积。</p>
<p>而对于采样，我们用冲激函数跟带采样函数相乘，在频域上得知：采样其实是搬移一个信号的频谱。采样率低，对应时域上周期长、频域上搬移间隔（频率）小，容易在频域上发生混叠。</p>
<p>通过卷积&#x3D;低通滤波&#x3D;平均的操作，让图片变得模糊，可以过滤掉高频的信息。也就是让频域更窄，搬移后不容易混叠。</p>
<p>而先采样、再模糊就不能做到antialiasing了，因为采样后的信号发生混叠，模糊只会去掉混叠后的大信号的高频信息。</p>
<p>对于光栅化中的锯齿问题，针对每个像素，先模糊（把像素内部对应的图形进行模糊），再采样（这个模糊后的值作为像素的值）。比如MSAA方法，把每个像素划分成小像素，分别计算小像素是否在三角形内部，然后计算大像素在三角形内的比例（模糊）作为采样结果（采样）。</p>
<h4 id="3-深度测试-Z-buffering"><a href="#3-深度测试-Z-buffering" class="headerlink" title="3 - 深度测试 Z-buffering"></a>3 - 深度测试 Z-buffering</h4><h5 id="可见性和遮挡问题"><a href="#可见性和遮挡问题" class="headerlink" title="可见性和遮挡问题"></a>可见性和遮挡问题</h5><p>把一个三角形画在屏幕上，需要先进行光栅化，划分成像素格子；然后对像素中心进行采样，查看它是否在三角形内；为了解决反走样，往往使用先做模糊、再采样的方式。</p>
<p>场景中的三角形离相机的距离各不相同，会产生 Visibility &#x2F; Occlusion 可见性和遮挡问题。</p>
<ul>
<li><p>在画家算法（Painter’s Algorithm）中，体现在每个三角形的渲染顺序上</p>
<ul>
<li>把所有的三角形进行深度排序，按照从远到近的顺序绘制所有三角形，也就从远到近完成整幅画</li>
<li>适用于三角形进行深度排序的场景。n个三角形时，复杂度是 O(nlogn)</li>
<li>有不能解决的情况：</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/64.png" style="zoom: 33%;" / loading="lazy"> </li>
<li>无法定义深度顺序，就不能应用画家算法</li>
</ul>
</li>
<li><p>图形学中，使用 Z-buffering（深度缓存）解决这个问题</p>
<ul>
<li>不好对每个三角形进行深度的排序，但可以<strong>对每个像素记录像素表示的几何的最近距离</strong> </li>
<li>同时渲染两张图<ul>
<li>frame buffer，存储颜色值，相当于最后的渲染结果</li>
<li>depth buffer（z-buffer），存能看到物体的最浅深度，利用这个信息维护遮挡关系</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/65.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>在之前，规定相机放在原点、朝 $-z$ 方向看，因此越近的物体深度值越大。在此处为了简化计算，认为：<u>相机看到的深度理解为相机到物体的距离，永远是正的</u>。小的 $z$ 值表示深度小、离得近<ul>
<li>查看上图的右边depth buffer，离相机越近，深度值越小，颜色越黑；离相机越远，深度值越大，颜色越白</li>
</ul>
</li>
<li>绘制过程：对于一个像素，刚开始画了地板，就把地板对应的深度记下来；后面放上了物体，物体上的三角形覆盖了这个像素，物体在像素上的深度比地板更小，因此该像素在左图 frame buffer 绘制物体、在右图 z-buffer 记录物体的深度</li>
</ul>
</li>
</ul>
<h5 id="Z-Buffer-Algorithm"><a href="#Z-Buffer-Algorithm" class="headerlink" title="Z-Buffer Algorithm"></a>Z-Buffer Algorithm</h5><ul>
<li><p>初始化 depth buffer 为 ∞</p>
</li>
<li><p>在每个三角形光栅化过程中：</p>
</li>
</ul>
<pre class="language-c++" data-language="c++"><code class="language-c++">for(each triangle T)					&#x2F;&#x2F; rasterization
    for(each sample(x, y, z) in T)		&#x2F;&#x2F; sampling 已经光栅化到屏幕上了，(x,y)是在屏幕上的坐标
		if(z &lt; zbuffer[x, y])			&#x2F;&#x2F; closest sample so far
			framebuffer[x, y] &#x3D; rgb;	&#x2F;&#x2F; update color
			zbuffer[x, y] &#x3D; z;			&#x2F;&#x2F; update depth
        else ;							&#x2F;&#x2F; do nothing, this sample is occluded</code></pre>

<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/66.png" style="zoom: 50%;" / loading="lazy"> 
</li>
<li><p>n个三角形、分别覆盖常数个像素个数，复杂度是 c*O(n)</p>
<ul>
<li>只是对每个像素记录了最小深度，并没有实际上排序</li>
</ul>
</li>
<li><p>深度缓存的一个性质：跟三角形的绘画顺序是没有关系的</p>
<ul>
<li>此处假设两个物体的某个像素的深度值永远不相等，因为运算得来的浮点数基本上不可能相等</li>
<li>在实际场景中确实会出现深度相等的情况，此处先不提</li>
</ul>
</li>
<li><p>深度缓存算法是目前被广泛采用的算法，被用在各种GPU硬件中</p>
<ul>
<li>为了反走样，采用MSAA算法，对于每个像素取很多个采样点，Z-Buffer会对每个采样点进行深度的记录</li>
</ul>
</li>
<li><p>Z-Buffer处理不了透明物体的深度。对透明物体需要特殊处理</p>
</li>
</ul>
<hr>
<h3 id="Lecture-07-Shading-1-Illumination-Shading-and-Graphics-Pipeline"><a href="#Lecture-07-Shading-1-Illumination-Shading-and-Graphics-Pipeline" class="headerlink" title="Lecture 07 - Shading 1 (Illumination, Shading and Graphics Pipeline)"></a>Lecture 07 - Shading 1 (Illumination, Shading and Graphics Pipeline)</h3><p>目前以完成的步骤：</p>
<ul>
<li>model transformation，生成一个3D世界，得到图1</li>
<li>进行Camera&#x2F;View transformation，摄像机移到原点，物体也跟着移动，得到图2</li>
<li>进行Projection transformation（orthographic，perspective），物体投影到 $[1-,1]^3$ 的立方体空间中</li>
<li>进行Viewport transformation，舍弃z轴信息，物体进入2D屏幕，得到图3</li>
<li>Rasterization（sampling，Antialiasing，Z-Buffer），得到图4</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/67.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>到此，不同三角形被画在屏幕上，填充对应的像素。这些像素的颜色应该是什么？（Shading）</p>
<h4 id="1-Shading"><a href="#1-Shading" class="headerlink" title="1 - Shading"></a>1 - Shading</h4><h5 id="Shading基础"><a href="#Shading基础" class="headerlink" title="Shading基础"></a>Shading基础</h5><h6 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h6><ul>
<li>shading的本意：绘画中不同的<u>明暗和颜色</u>。（The darkening or coloring of an illustration or diagram with parallel lines or a block of color.）</li>
<li>对于本课：shading 是<strong>对于不同物体，应用不同材质的过程</strong></li>
</ul>
<h6 id="一个简单的着色模型：Blinn-Phong-反射模型"><a href="#一个简单的着色模型：Blinn-Phong-反射模型" class="headerlink" title="一个简单的着色模型：Blinn-Phong 反射模型"></a>一个简单的着色模型：Blinn-Phong 反射模型</h6><ul>
<li>Specular highlights：高光，表面很光滑，光线向镜面反射方向附近反射</li>
<li>Diffuse reflection：漫反射，表面不光滑，光线被反射到四面八方</li>
<li>Ambient lighting：间接光照，不直接接触光，接受环境的反射光</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/68.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
<h4 id="2-Blinn-Phong-反射模型"><a href="#2-Blinn-Phong-反射模型" class="headerlink" title="2 - Blinn-Phong 反射模型"></a>2 - Blinn-Phong 反射模型</h4><h5 id="Blinn-Phong-反射模型总览"><a href="#Blinn-Phong-反射模型总览" class="headerlink" title="Blinn-Phong 反射模型总览"></a>Blinn-Phong 反射模型总览</h5><h6 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h6><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/69.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>考虑任何一个点（shading point）上的着色结果<ul>
<li>shading point 位于一个物体表面上，在小范围内是一个平面</li>
<li>Blinn-Phong 模型给定 shading point 的一些属性。根据这些属性，有的点会发生漫反射，有的点发生镜面反射。模型最终求的是这个点的光照情况</li>
<li>从光源到 shading point，光的能量可能会衰减，但求得该点接收的能量后，就不考虑从该点到观测点的能量传递过程了</li>
</ul>
</li>
</ul>
<h6 id="定义量"><a href="#定义量" class="headerlink" title="定义量"></a>定义量</h6><ul>
<li>定义一些方向<ul>
<li>定义一个法线方向 $\vec n$ </li>
<li>定义观测方向是从 shading point 指向相机的方向，$\vec v$ </li>
<li>定义光照方向是从 shading point 指向光源的方向，$\vec l$ </li>
<li>（只定义方向，以上都是单位向量）</li>
</ul>
</li>
<li>物体表面相关的属性<ul>
<li>有多亮（shininess，跟亮度不同），颜色等</li>
</ul>
</li>
<li>shading ≠ shadow<ul>
<li>Shading is Local，考虑一个点的着色情况，就只考虑它自己和几个方向；不考虑其他物体的存在</li>
<li><u>只考虑 shading point 点和其他几个方向，不考虑这个点是否在阴影内</u> </li>
<li>可以看到图中物体接收左上方的光照，有了明暗的变化，但体现不出来阴影</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/70.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="Blinn-Phong-模型之-Diffuse-Reflection（漫反射）"><a href="#Blinn-Phong-模型之-Diffuse-Reflection（漫反射）" class="headerlink" title="Blinn-Phong 模型之 Diffuse Reflection（漫反射）"></a>Blinn-Phong 模型之 Diffuse Reflection（漫反射）</h5><h6 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h6><ul>
<li>漫反射：<strong>光线打到物体的某个点时，光线被均匀地反射到各个方向去</strong> <ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/71.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>光的接收<ul>
<li>当物体表面的朝向和光照方向有一定的夹角，得到的明暗是不一样的</li>
<li>考虑<strong>光是一种能量</strong>，看到的物体的明暗相当于物体接收到了多少能量<ul>
<li>考虑 shading point 周围单位面积内接收到了多少能量</li>
<li>理解为四季的变换，南北半球接收太阳直射的能量不同，体现为冬季和夏季</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/72.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>Lambert’s 余弦定理：<u>接收到的能量跟光照方向、法线方向夹角的余弦呈正比</u> <ul>
<li>光照方向跟法线方向接近垂直，光是平行打向表面的，shading point 上接收不到多少能量</li>
<li>光照方向跟法线方向相同，光垂直打向表面，shading point 接收到最多的能量</li>
</ul>
</li>
</ul>
</li>
<li>光的发散<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/73.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>认为点光源产生了光，无时无刻不在向四面八方辐射能量</li>
<li>观测方法：考虑某个时刻发出的所有能量。<u>在任何一个时刻，点光源辐射出的能量集中在一个球壳上</u>；在下一个时刻，光集中在更外面的球壳上</li>
<li><u>能量是守恒的，近的球壳和远的球壳具有相同的总能量</u> <ul>
<li>小的球壳上，每一个点的能量更多</li>
<li>传播过程中，球壳表面积变大，每一个点对应的能量越少</li>
<li>暂时定义一个强度 $I$，对于距离为 $1$ 的球壳，总能量 $4\pi I$；对于距离为 $r$ 的球壳，$4\pi r^2 I’&#x3D; 4\pi I$，$I’&#x3D;I&#x2F;r^2$ </li>
<li><u>光线在传播过程中，单位面积在任何位置能接收到的能量（也就是强度 $I$），跟光线的传播距离的平方呈反比</u></li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h6><p>$L_d &#x3D; k_d \frac{I}{r^2}\max(0, \bold {n\cdot l})$</p>
<ul>
<li>结合起来<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/74.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>由 Light Falloff，如果知道一个点光源，并且知道 shading point 离点光源的距离，就可以知道有多少光传播到了点光源附近</li>
<li>由 Lambert’s cosine law，可以知道这些光如何在 shading point 上被吸收</li>
<li>$L_d &#x3D; k_d (I&#x2F;r^2)\max(0, \bold {n\cdot l})$ <ul>
<li>$r$：shading point 到光源的距离；$I$：单位面积上的光能量；$I&#x2F;r^2$：到达 shading point 处的光能量</li>
<li>$\bold {n\cdot l}$：两个单位向量的点乘（也就是余弦），如果得出负数就取 0，物理意义上只考虑反射、不考虑从下方照来的光的折射</li>
<li>$(I&#x2F;r^2)\max(0, \bold {n\cdot l})$ ：接收了多少能量（到达该点的总能量，减去被吸收的能量，剩下的就是能发射出去的能量，也就是吸收了多少能量）</li>
<li>$k_d$：点的反射率。点本身有明暗，点吸收一部分能量、反射另一部分。如果把这个量定义成R&#x2F;G&#x2F;B三个通道上的明暗，也就定义了这个点的颜色</li>
<li>最终得出 $L_d$：对于一个发生漫反射的点，最终我们能看到多少能量，也就是这个点的明暗</li>
</ul>
</li>
<li>由于漫反射出来的能量均匀打到各个方向，不管从哪个方向观察它，都得到相同的能量，看到一模一样的结果。<u>漫反射跟观测方向 $\vec v$ 完全无关</u> </li>
<li>漫反射的结果<ul>
<li>可以看出，光源位于左上方</li>
<li>同一个石膏球，右下方法线方向跟光照方向接近垂直或反向，余弦值小，反射的能量少，看起来暗</li>
<li>不同的石膏球，对于不同的反射率 $k_d$，反射率越高，反射的能量越多，看起来越亮</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/75.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Blinn-Phong-模型之-Specular-Term（高光）"><a href="#Blinn-Phong-模型之-Specular-Term（高光）" class="headerlink" title="Blinn-Phong 模型之 Specular Term（高光）"></a>Blinn-Phong 模型之 Specular Term（高光）</h5><h6 id="概念和计算"><a href="#概念和计算" class="headerlink" title="概念和计算"></a>概念和计算</h6><ul>
<li><p>高光说明物体比较光滑，<strong>光线反射的方向接近镜面反射的方向</strong>。由于观察的方向 $\bold v$ 跟镜面反射方向 $\bold R$ 接近，因此能看到高光。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/76.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
</li>
<li><p>Blinn-Phong 模型做出一个假设：观察方向 $\bold v$ 跟镜面反射方向 $\bold R$ 接近，就是法线方向 $\bold n$ 跟半程向量 $\bold h$ 接近</p>
<ul>
<li>半程向量：两个向量的角平分线方向，这些向量都是单位向量</li>
<li>$\bold h &#x3D; \text{bisector}(\bold v, \bold l)\frac{\bold v+ \bold l}{||\bold v+ \bold l||}$ </li>
<li>通过向量点乘判断接近，单位向量接近则点乘结果接近1</li>
<li>这样就不需要计算反射方向 $\bold R$ 了。计算 $\bold R$ 跟 $\bold v$ 是否接近的模型是 Phong 模型，Blinn-Phong 模型在此处做了计算上的简化，因为计算 $\bold h$ 比计算 $\bold R$ 简单</li>
</ul>
</li>
<li><p>$L_s&#x3D;k_s(I&#x2F;r^2)\text{max}(0, cos\alpha)^p&#x3D;k_s(I&#x2F;r^2)\text{max}(0, \bold n\cdot\bold h)^p$ </p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/77.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>$k_s$ 是镜面反射系数<ul>
<li>通常认为镜面反射都是白色，$k_s$ 就是白色</li>
</ul>
</li>
<li>$(I&#x2F;r^2)$ <ul>
<li>跟漫反射一样，$(I&#x2F;r^2)$ 考虑多少能量到达 shading point</li>
<li>但比起漫反射，不再考虑多少能量被吸收，也就是考虑入射角度跟平面的夹角。此处没考虑，是因为 blinn-phong 作为经验模型，把这一点简化了<ul>
<li>我的理解，可以是因为平面很光滑，能量基本上都被反射了，没有被吸收。只考虑观测方向能否保证我们看到高光</li>
</ul>
</li>
</ul>
</li>
<li>$\text{max}(0, \bold n\cdot\bold h)^p$<ul>
<li>$\text{max}(0, \bold n\cdot\bold h)$ 间接衡量观察方向跟反射方向的的接近程度，用半程向量更好算</li>
<li>$p$ 次方提高夹角余弦的衰减程度，达到角度差一点、就看不到高光的效果。可以控制看到高光的大小。$p$ 通常取 $[100,200]$ <ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/78.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>漫反射+高光的结果</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/79.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>对于每一列，镜面反射系数 $k_s$ 增大，表示它的亮度</li>
<li>对于每一行，从左到右 $p$ 增大，偏转角度越来越严格、高光越来越小</li>
</ul>
</li>
</ul>
<h5 id="Blinn-Phong-模型之-Ambient-Term（环境光）"><a href="#Blinn-Phong-模型之-Ambient-Term（环境光）" class="headerlink" title="Blinn-Phong 模型之 Ambient Term（环境光）"></a>Blinn-Phong 模型之 Ambient Term（环境光）</h5><h6 id="概念和计算-1"><a href="#概念和计算-1" class="headerlink" title="概念和计算"></a>概念和计算</h6><ul>
<li>Blinn-Phong 模型认为任何一个点接收来自环境中的光永远都是相同的，强度为 $I_a$、来自四面八方</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/80.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>$L_a &#x3D; k_aI_a$ </li>
<li>环境光跟各种方向都无关，可以理解为一个常数——也就是一个颜色</li>
<li>环境光的作用：每个地方都有一个基础的颜色，没有任何一个地方是黑的</li>
<li>如果想很精确地计算环境光，需要用到全局光照，比较麻烦</li>
</ul>
<h5 id="Blinn-Phong-反射模型总结"><a href="#Blinn-Phong-反射模型总结" class="headerlink" title="Blinn-Phong 反射模型总结"></a>Blinn-Phong 反射模型总结</h5><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/81.png" style="zoom: 50%;" / loading="lazy">  
</li>
<li><p>环境光项：一个常数颜色</p>
</li>
<li><p>漫反射项：跟观测方向无关、跟光照和法线角度有关</p>
</li>
<li><p>高光项：在比较少的地方产生的、（通常是）白色亮点</p>
</li>
<li><p>$\begin{array}{ll}L &#x3D; L_a + L_d + L_s &#x3D; k_aI_a+k_d (I&#x2F;r^2)\max(0, \bold {n\cdot l})+k_s(I&#x2F;r^2)\text{max}(0, \bold n\cdot\bold h)^p\end{array}$</p>
</li>
</ul>
<p>到此，得到了着色模型，考虑任何一个点（shading point）的上色情况。接下来，对每个点做一次着色操作，这样整个场景的所有点就能看到了。</p>
<ul>
<li>实际上，如果一个地方凹下去、环境光会暗一些，但Blinn-Phong模型环境光都是一样的，简化了类似的细节。对于复杂的环境光计算方法，在后面的全局光照部分讲</li>
<li>“能量”这个说法其实不合理，<u>Blinn-Phong 模型只考虑从光源发射到物体的“能量”损失，观测时不管离多远看上去都是一样的颜色</u>。但事实上，离得远会看起来暗，这些关于radiance的问题会在后面说</li>
</ul>
<hr>
<h3 id="Lecture-08-Shading-2-Shading-Pipeline-and-Texture-Mapping"><a href="#Lecture-08-Shading-2-Shading-Pipeline-and-Texture-Mapping" class="headerlink" title="Lecture 08 - Shading 2 (Shading, Pipeline and Texture Mapping)"></a>Lecture 08 - Shading 2 (Shading, Pipeline and Texture Mapping)</h3><h4 id="1-Shading-1"><a href="#1-Shading-1" class="headerlink" title="1 - Shading"></a>1 - Shading</h4><p>Blinn-Phong 反射模型定义了每个 shading point 的着色方法：</p>
<p>$\begin{array}{ll}L &#x3D; L_a + L_d + L_s &#x3D; k_aI_a+k_d (I&#x2F;r^2)\max(0, \bold {n\cdot l})+k_s(I&#x2F;r^2)\text{max}(0, \bold n\cdot\bold h)^p\end{array}$ </p>
<h4 id="2-Shading-Frequencies-着色频率"><a href="#2-Shading-Frequencies-着色频率" class="headerlink" title="2 - Shading Frequencies 着色频率"></a>2 - Shading Frequencies 着色频率</h4><h5 id="不同的着色频率"><a href="#不同的着色频率" class="headerlink" title="不同的着色频率"></a>不同的着色频率</h5><ul>
<li><p>着色：应用在一个 shading point 上</p>
</li>
<li><p>着色频率：把着色应用在哪些点上。同一个模型，着色频率不同、结果也大不同 </p>
<ul>
<li>Flat shading（逐平面）：对<strong>每个平面</strong>只做一次 shading ，每个平面的颜色相同<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/83.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>Gouraud shading（逐顶点）：三角形<strong>每个顶点</strong>计算法线，对每个顶点做一次 shading；三角形内部进行<strong>颜色插值</strong>，求颜色<ul>
<li>引申问题：顶点的法线怎么求？</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/84.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>Phong shading（逐像素）：<strong>每个像素</strong>上应用 shading。在每个顶点计算法线、在顶点内部插值，得到每个像素自己的法线，分别做 shading<ul>
<li>引申问题：知道顶点法线，内部的法线怎么插值？</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/85.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="着色频率的选择"><a href="#着色频率的选择" class="headerlink" title="着色频率的选择"></a>着色频率的选择</h5><ul>
<li>对不同着色频率模型，不一定是逐像素着色就是最好的方法。如下图，每列是不同的着色模型，从上到下模型的面数变多。可以看到，如果模型本身比较精细，采用不太精确的着色频率模型，就能得到差不多的结果，不必采用太麻烦的着色频率。取决于具体物体。<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/82.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h5><ul>
<li><p>引申问题：Gouraud模型、Phong模型中，顶点的法线怎么算？</p>
<ul>
<li>任何一个顶点都跟若干三角形相连。因此，把关联平面的法线求平均（由于三角形大小可能不同，可以求加权平均，权是三角形的面积），得到顶点的法线</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/86.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>引申问题：Phong模型中，知道顶点法线，内部的法线怎么插值？</p>
<ul>
<li>用到<strong>重心坐标</strong> </li>
<li>注意：所有的法线都是方向，求出来要做归一化、变为单位向量</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/87.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="3-Graphics-Real-time-Rendering-Pipeline"><a href="#3-Graphics-Real-time-Rendering-Pipeline" class="headerlink" title="3 - Graphics (Real-time Rendering) Pipeline"></a>3 - Graphics (Real-time Rendering) Pipeline</h4><h5 id="实时渲染管线"><a href="#实时渲染管线" class="headerlink" title="实时渲染管线"></a>实时渲染管线</h5><p>从一个3D场景，如何得到最后的一张图，这个过程就叫做 Pipeline。</p>
<ul>
<li>输入：三维空间中的点</li>
<li>经过<strong>顶点处理</strong>（MVP变换，视口变换等步骤），把这些点投影到屏幕上</li>
<li>屏幕上的点会形成三角形</li>
<li>经过<strong>光栅化</strong>（采样，深度测试），把三角形变为若干 fragments</li>
<li>每个像素进行<strong>着色</strong> </li>
<li>输出：整个屏幕上所有像素的颜色</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/88.png" style="zoom: 80%;" / loading="lazy"></li>
</ul>
<p>整个操作被写在GPU里。</p>
<ul>
<li>MVP变换应用在顶点处理中，对每个顶点做变换<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/89.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>对屏幕的像素采样、看是否在三角形内，是光栅化的步骤<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/90.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>光栅化产生了一系列的 fragment（像素）后，使用 Z-Buffer 判断遮挡关系（也可以把这一步划分到光栅化的一部分）<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/91.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><u>如果是逐顶点 Shading，着色会发生在顶点处理中；如果做逐像素 Shading，要等像素都产生后、在 Fragment 里做</u>  <ul>
<li>现代 GPU 是可编程的，可以自定义顶点、像素如何着色。shader 代码控制了顶点和像素是如何着色的</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/92.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>三角形内部不同的点拥有不同的属性，显示为纹理，如何做纹理映射在后面说<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/93.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="Shader"><a href="#Shader" class="headerlink" title="Shader"></a>Shader</h5><p>现代GPU允许通过shader编程，实现顶点、像素的自定义着色。shader是能够在GPU上直接运行的语言。</p>
<ul>
<li>写的 shader 在意味上是通用的，即<u>每一个顶点或每一个 fragment 都会执行这个 shader</u>，而不需要针对某一个顶点、fragment 特殊定义 shader（不需要写for循环）。</li>
<li>如果写的是顶点操作，叫做 vertex shader （顶点着色器）；如果写的是像素的操作，叫做 fragment&#x2F;pixel shader（像素着色器）。</li>
<li>对于像素着色器，要计算并输出像素最后的颜色</li>
</ul>
<pre class="language-glsl" data-language="glsl"><code class="language-glsl"><span class="token keyword">uniform</span> <span class="token keyword">sampler2D</span> myTexture<span class="token punctuation">;</span>	<span class="token comment">// 全局变量，纹理</span>
<span class="token keyword">uniform</span> <span class="token keyword">vec3</span> lightDir<span class="token punctuation">;</span>			<span class="token comment">// 全局变量，光照方向</span>
<span class="token keyword">varying</span> <span class="token keyword">vec2</span> uv<span class="token punctuation">;</span>				<span class="token comment">// interp. by rasterizer</span>
<span class="token keyword">varying</span> <span class="token keyword">vec3</span> norm<span class="token punctuation">;</span>				<span class="token comment">// 顶点的法线，interp. by rasterizer</span>

<span class="token keyword">void</span> <span class="token function">diffuseShader</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>	
    <span class="token comment">// 通过纹理，得到该点的漫反射系数 kd</span>
	<span class="token keyword">vec3</span> kd<span class="token punctuation">;</span>	
    kd <span class="token operator">=</span> <span class="token function">texture2d</span><span class="token punctuation">(</span>myTexture<span class="token punctuation">,</span> uv<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token comment">// 漫反射系数 * （光照方向、法线方向的点乘，限定在[0,1]）。即漫反射项，或者是 Lambetrtian shading model</span>
    kd <span class="token operator">*=</span> <span class="token function">clamp</span><span class="token punctuation">(</span><span class="token function">dot</span><span class="token punctuation">(</span><span class="token operator">-</span>lightDir<span class="token punctuation">,</span> norm<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token comment">// 存储这个值，输出 fragment 的颜色</span>
    gl_FragColor <span class="token operator">=</span> <span class="token keyword">vec4</span><span class="token punctuation">(</span>kd<span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span></code></pre>

<p>一个网站：<a href="http://shadertoy.com/view/ld3Gz2">ShaderToy</a>，免去OpenGL、DirectX等，只需写着色器即可，可以在网页上执行并产生结果。</p>
<ul>
<li>这个蜗牛的所有几何形体都是通过数学方法定义的，没有用到任何的三角形</li>
<li>这个例子是几何形体的投影，跟三角形的投影不一样</li>
</ul>
<h5 id="现代图形学的发展"><a href="#现代图形学的发展" class="headerlink" title="现代图形学的发展"></a>现代图形学的发展</h5><ul>
<li>现代GPU高度并行，显卡可以同时处理大量几何、并且着色非常快。</li>
<li>现在的发展趋势：超级复杂的场景，游戏引擎集合各种模块，尤其是图形渲染模块</li>
<li>GPU是整个一套渲染管线的硬件实现，shader这部分是可编程的。除了vertex shader、fragment shader，现在还有geometry shader来做几何的计算，compute shader来做通用的计算。<ul>
<li>GPU理解为高度并行的处理器</li>
</ul>
</li>
</ul>
<h4 id="4-Texture-Mapping-纹理映射"><a href="#4-Texture-Mapping-纹理映射" class="headerlink" title="4 - Texture Mapping 纹理映射"></a>4 - Texture Mapping 纹理映射</h4><p>如何对三角形内部进行填充？引入纹理映射的思路：希望在物体的不同位置定义不同的属性。</p>
<p>纹理定义了一个物体上任意一个点的属性。有了这些属性，在着色时，各个点就可以被差异化着色。</p>
<h5 id="纹理图"><a href="#纹理图" class="headerlink" title="纹理图"></a>纹理图</h5><p>现象：任何一个3D物体的表面都是2D的。也就是说，3D物体的表面跟一张图有映射关系。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/94.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>纹理就是一张图。可以对纹理进行一系列变换，或取一部分，蒙在3D物体的表面。物体表面上的点跟纹理图上的点之间的关系就是纹理映射。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/95.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>一般是美工做这个步骤：</p>
<ul>
<li>通过先建模、再把模型展开，放到纹理图上的某个位置进行对应</li>
<li>自动化的过程：给任意模型，把它展开成一个平面，并且产生的三角形尽可能得少扭曲。这是图形学中几何的一个研究方向 parameterization（参数化）</li>
</ul>
<h5 id="纹理映射"><a href="#纹理映射" class="headerlink" title="纹理映射"></a>纹理映射</h5><p>不管怎么把空间中的三角形映射到纹理上，本课我们认为已经有了这个映射关系。对于三角形上的每个顶点，我们都知道它对应在纹理上的一个坐标。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/96.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>纹理图上的坐标系：$(u, v)$ 。通常，不管分辨率、长宽比，都约定 $u,v \in [0, 1]$ 。</p>
<p>纹理并不是只能用一次，可以让纹理重复多次、把场景贴满（tile，或许unity2d的tilemap也是这样？）。纹理如果设计得好，可以使纹理在复制时无缝衔接。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/97.png" style="zoom: 33%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/98.png" style="zoom: 33%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/99.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
<p>下一个问题：知道了三角形的三个顶点对应的纹理坐标 $(u, v)$，如何做三角形内部的插值？（三角形的顶点具有不同的属性，如何得到三角形内部点的属性？需要用到重心坐标。）</p>
<hr>
<h3 id="Lecture-09-Shading-3-Texture-Mapping-cont"><a href="#Lecture-09-Shading-3-Texture-Mapping-cont" class="headerlink" title="Lecture 09 - Shading 3 (Texture Mapping cont.)"></a>Lecture 09 - Shading 3 (Texture Mapping cont.)</h3><p>前两节shading课，讲了blinn-phong反射模型、着色模型和着色频率、图形渲染管线、纹理映射。不同的材质的平面、不同的光线作用，会产生不同的着色结果。</p>
<p>着色的基本已经讲完了。本课是关于纹理的一些应用。</p>
<ul>
<li>Barycentric coordinates</li>
<li>Texture queries</li>
<li>Applications of textures</li>
<li>(Shadow Mapping)</li>
</ul>
<h4 id="1-重心坐标（Barycentric-Coordinates）：在三角形内部进行任何属性的插值"><a href="#1-重心坐标（Barycentric-Coordinates）：在三角形内部进行任何属性的插值" class="headerlink" title="1 - 重心坐标（Barycentric Coordinates）：在三角形内部进行任何属性的插值"></a>1 - 重心坐标（Barycentric Coordinates）：在三角形内部进行任何属性的插值</h4><h5 id="关于插值"><a href="#关于插值" class="headerlink" title="关于插值"></a>关于插值</h5><ul>
<li><p>为什么会用到三角形插值：很多东西是在顶点上计算的，需要在三角形内部得到平滑的过渡</p>
</li>
<li><p>插值什么内容：纹理映射中的$(u,v)$坐标；顶点的颜色（逐顶点shading）；顶点的法向量（逐像素shading）</p>
</li>
<li><p>为了实现插值，需要引入<strong>重心坐标</strong></p>
</li>
</ul>
<h5 id="重心坐标"><a href="#重心坐标" class="headerlink" title="重心坐标"></a>重心坐标</h5><ul>
<li><u>每个三角形定义一套重心坐标</u>（换一个三角形，就换一套重心坐标）</li>
<li>定义：在 $\triangle ABC$ 的平面内任意一点，都可以表示为 $A,B,C$ 三点的线性组合，且线性组合的系数 $\alpha,\beta,\gamma$ 相加为 1<ul>
<li>$(x, y)&#x3D;\alpha A + \beta B + \gamma C\space,\space\alpha+\beta+\gamma&#x3D;1$ </li>
<li>如果某点的 $\alpha,\beta,\gamma$ 相加为 1，且都是非负数，则这个点在三角形内部</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/100.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>计算：可以通过每个顶点“对面”的三角形的面积来计算  $\alpha,\beta,\gamma$ <ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/101.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>由面积公式，可以简化计算成坐标形式：</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/103.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>举例：三角形的重心（centroid），把三角形分成面积相同的三块<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/102.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="图形学的插值和重心坐标"><a href="#图形学的插值和重心坐标" class="headerlink" title="图形学的插值和重心坐标"></a>图形学的插值和重心坐标</h5><p>在图形学中，需要插值的属性，同样应该使用重心坐标进行线性组合。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/104.png" style="zoom: 50%;" / loading="lazy"> 
</li>
<li><p>使用重心坐标，可以通过顶点坐标，计算出插线性插值的参数。从而实现任意一点的插值。</p>
</li>
<li><p>注意：投影变换中，重心坐标可能改变（空间中的三角形投影到平面上，如果三角形发生变化、重心坐标自然发生变化）</p>
<ul>
<li>因此，<u>如果插值3D空间中的属性，需要在3D中进行重心坐标的插值，而不是投影后在新的三角形内做插值</u> </li>
<li>主要应用在：光栅化计算深度。在光栅化过程中，要应用逆变换（从2D变换回投影前）、求得像素在3D空间的深度</li>
</ul>
</li>
</ul>
<h4 id="2-纹理放大（Texture-Magnification）"><a href="#2-纹理放大（Texture-Magnification）" class="headerlink" title="2 - 纹理放大（Texture Magnification）"></a>2 - 纹理放大（Texture Magnification）</h4><h5 id="2-1-在渲染中应用纹理"><a href="#2-1-在渲染中应用纹理" class="headerlink" title="2.1 在渲染中应用纹理"></a>2.1 在渲染中应用纹理</h5><p>屏幕中的任一个采样点（像素或MSAA等细分），都有一个位置，在这个位置上可以插值出 $(u,v)$ 纹理坐标。通过纹理坐标查询纹理图，纹理在该点的值可以当作漫反射系数 $K_d$。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/105.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>在此过程中，可能出现一些问题。</p>
<h5 id="2-2-纹理本身太小，怎么办？（双线性插值）"><a href="#2-2-纹理本身太小，怎么办？（双线性插值）" class="headerlink" title="2.2 纹理本身太小，怎么办？（双线性插值）"></a>2.2 纹理本身太小，怎么办？（双线性插值）</h5><p>纹理本身太小，画面分辨率太高（高清的模型，使用低清的纹理），<u>像素的中心映射到纹理非整数的位置上</u>。</p>
<ul>
<li><p>最邻近映射</p>
<ul>
<li>不管投影到何处，都当作离得最近的纹理坐标的值</li>
<li>导致很多地方映射到同样的纹理坐标位置上，出现格子</li>
</ul>
</li>
<li><p><strong>双线性插值</strong>（Bilinear interpolation）</p>
<ul>
<li>定义1D的插值操作</li>
<li>以图中情况为例，先在水平方向上，插值计算 $u_0,u_1$ 点的值</li>
<li>然后在竖直方向上，插值计算红点处的值</li>
<li>因此，红点处的值综合考虑了周围四个点的值，并且结果是根据距离的线性插值</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/106.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>更高次的插值，比如取临近 16 个点的 Bicubic interpolation，进行 3 次插值，能得到更好的结果。</p>
</li>
<li><p>好的质量都伴随更高的开销，双线性插值在能接受的计算量上得到了不错的结果。</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/107.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h5 id="2-3-纹理本身太大，怎么办？（Mipmap-和三线性插值）"><a href="#2-3-纹理本身太大，怎么办？（Mipmap-和三线性插值）" class="headerlink" title="2.3 纹理本身太大，怎么办？（Mipmap 和三线性插值）"></a>2.3 纹理本身太大，怎么办？（Mipmap 和三线性插值）</h5><h6 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h6><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/108.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>在透视投影中，如果还按照每个点在纹理图上寻找坐标的方法进行着色，远处有摩尔纹、近处有锯齿，产生了走样（aliasing）。</p>
<p>产生这种现象的原因：</p>
<ul>
<li>由于透视投影，近处的像素覆盖的纹理小，远处的像素覆盖的纹理大，不同像素覆盖的纹理大小各不相同</li>
<li>像素覆盖的纹理小，查询像素中心的纹理值即可</li>
<li>像素覆盖的纹理大（因为纹理图大），像素中心的纹理值，认为是覆盖的整块纹理区域的平均值。在这种情况下，不能简单地使用像素中心来采样<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/109.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>理论上如何分析这个问题？</p>
<ul>
<li>走样问题：信号变化过快，采样的频率跟不上信号变化的速度。<ul>
<li>当纹理特别大，像素内部会包含变化的纹理（单个像素覆盖一块纹理）。像素内频率高，但只用像素一个采样点，就发生了走样</li>
<li>因此，需要在像素内使用更高频的采样方法<ul>
<li>MSAA的思路：对于一个像素，用更多的采样点采样，再取每个像素的平均。这里也可以这样做，比如每个像素取512个采样点，但会让计算代价变大。</li>
</ul>
</li>
<li>另一个思路：采样会引起走样，是否能<strong>不采样</strong>？即：立刻可以知道纹理图一个区域的值的平均，不进行采样。<ul>
<li>算法问题：点查询问题（Point Query，给定一个点，求它的值。之前的双线性插值等）和范围查询问题（Range Query，给定一个区域，不做点采样立刻得到它的范围内平均值或最值）</li>
<li>引入概念：<strong>Mipmap</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="Mipmap：进行范围查询"><a href="#Mipmap：进行范围查询" class="headerlink" title="Mipmap：进行范围查询"></a>Mipmap：进行范围查询</h6><ul>
<li><p>特点：fast（查得快）、approx.（查到的是近似值）、square（仅能做正方形查询）</p>
</li>
<li><p>拿到一个纹理作为第0层，之后的每层把上层的分辨率缩小一半。共有log层。</p>
</li>
<li><p>提前计算一个纹理对应的mipmap</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/110.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>在CV中，叫 image pyramid，两者是相同的概念</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/111.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>一个小问题：总共引入了多少额外的存储？</p>
<ul>
<li>级数求和问题，$1+\frac{1}{4}+\frac{1}{16}+… &#x3D; \frac{4}{3}$ </li>
<li><u>只多了三分之一的存储量</u></li>
</ul>
</li>
</ul>
<p>mipmap的查询</p>
<ul>
<li>首先，可以通过近似，把像素覆盖的纹理范围近似成一个正方形<ul>
<li>可以根据相邻像素中心在纹理图上的坐标，求得像素两个方向上的变化趋势</li>
<li>可以把两个方向上比较大的变化趋势，作为正方形的长度</li>
<li>由这一步，可以得到任何一个像素，覆盖的纹理图的正方形范围</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/112.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>然后，由 mipmap 的计算方法，在 mipmap 的 $D&#x3D;\log_2L$ 层查找这个像素的纹理平均值<ul>
<li>如计算出像素对应正方形为 $4\times 4$，就可以在 mipmap 的第二层找到平均值</li>
<li><u>在某一层上的查询，依然是 bilinear interpolation</u>（覆盖的正方形不一定正好是 mipmap 的一个格子，而可能覆盖在四个格子上）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/113.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>如图，离得近的位置，每个像素的纹理在低层的 mipmap 查询；离得远就在高层查询。即由于近大远小，在远处，一个像素表达的信息更多，覆盖的纹理范围大，才会近似一个区域</li>
<li>另一个问题：在哪一层 mipmap 上查并不连续，而是跳跃的，图中不同颜色没有渐变，在 shading 时可能会在不同层之间看到缝隙。</li>
<li>引入 <strong>Trilinear Interpolation</strong>（三线性插值，mipmap 层与层之间的插值），来解决这个问题。这样，可以通过第一层、第二层，得到第 1.8 层的 mipmap</li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="Trilinear-Interpolation（三线性插值）"><a href="#Trilinear-Interpolation（三线性插值）" class="headerlink" title="Trilinear Interpolation（三线性插值）"></a>Trilinear Interpolation（三线性插值）</h6><ul>
<li>如：查询1.8层某个点的纹理值</li>
<li>分别在第一层、第二层内，做该点相同位置的 Bilinear Interpolation（水平、竖直方向的两次插值）</li>
<li>将这两层、两个点的查询结果再做一个插值（层与层之间的第三次插值）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/114.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>到此为止，在纹理的内部，可以双线性插值得到平滑的值；在任意 mipmap 层，也可以三线性插值得到平滑的值。从一张纹理图，可以得到完全连续的纹理表达</li>
<li>对于任意像素覆盖的区域，只需要<u>通过一次三线性插值查询</u>，就可以得到这块区域覆盖的纹理的平均值了</li>
</ul>
<p>三线性插值可以做到完全连续的表达，并且通过两次查询（两层mipmap）、一次插值（层之间），用很小的开销，得到任意一点的纹理查询，目前被广泛应用。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/115.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h6 id="其他思考"><a href="#其他思考" class="headerlink" title="其他思考"></a>其他思考</h6><p>再思考回来：mipmap能否完全解决问题？在远处，mipmap 会导致 overblue 现象。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/116.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>问题出在哪里？mipmap 的查询就是三线性插值计算，没有问题。因此，问题在于 mipmap 本身的限定条件：<strong>只能查询方块</strong>，<strong>得到近似结果</strong>。</p>
<p>正常的映射中，一个像素会映射到纹理上的各种形状，如图，有长条形的、斜着的区域。 用正方形近似计算会不准确。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/118.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>使用 <strong>Anisotropic Filtering（各向异性过滤）</strong>，可以解决三线性插值的一部分问题。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/117.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>各向异性：在不同方向上表现各不相同。各向同性：矩形在水平竖直方向上表现完全相同</li>
<li>图片的右上方，Mipmap 可以看作是对角线上的一系列变换，只能查询正方形的区域</li>
<li>各向异性过滤则会生成图中一系列水平竖直“压扁”的图片，称为 Ripmap，<u>额外开销是原来的 3倍</u> <ul>
<li>对比 Mipmap，额外开销仅三分之一</li>
</ul>
</li>
<li>各向异性过滤允许对<u>长条形</u>的区域做查询，但依然<u>不能查询斜着的区域</u> </li>
<li>游戏中有各向异性的选项，其中的参数 $n$x，说明计算到第 $n$ 层，即从左上角开始 $n*n$ 的区域，因此最终都收敛到三倍。各向异性的存储量跟开多少 x 关系不大，显卡的显存足够即可，跟计算力无关</li>
</ul>
<p>此时，可以使用 <strong>EWA过滤</strong> 等方法</p>
<ul>
<li>任意不规则形状，都可以拆成很多不同圆形，来覆盖这个不规则形状</li>
<li>每次查询一个圆形，多次查询就能覆盖不规则形状</li>
<li>代价：多次查询</li>
</ul>
<h4 id="3-纹理的应用"><a href="#3-纹理的应用" class="headerlink" title="3 - 纹理的应用"></a>3 - 纹理的应用</h4><p>在之前，把纹理想成颜色，就是漫反射的 $K_d$ 系数。实际上，纹理可以定义任何的属性。</p>
<p>在现代GPU中，可以把纹理理解为一块内存，可以对这块内存做快速的点查询、范围查询，或做滤波。简单来说，不必把纹理理解为图像，可以理解为<u>一块数据</u>，可以做不同类型的查询。</p>
<p>从这个角度，纹理可以表示很多东西：</p>
<h5 id="应用一：Environment-Map-x2F-Lighting"><a href="#应用一：Environment-Map-x2F-Lighting" class="headerlink" title="应用一：Environment Map&#x2F;Lighting"></a>应用一：Environment Map&#x2F;Lighting</h5><p>纹理作为环境贴图 &#x2F; 环境光照，<u>用纹理定义环境光</u> </p>
<ul>
<li>任何一个方向都可以看到光，不管是直接光照、光源还是反射来的光。<strong>人看到的物体都是光照反射到人眼。</strong> </li>
<li>把任何方向来的光都记录下来，就是环境贴图。用纹理描述整个环境光是什么样子，然后可以用这个纹理去渲染别的物体<ul>
<li>假设：环境光来自无限远处，只记录方向信息、不在意深度</li>
<li>左图：在屋子里四面八方看到的东西；右图：茶壶反射了这个环境光</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/119.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>可以把整个环境光记录在球上，然后展开：<strong>Spherical Environment Map</strong>。但展开后的图会发生扭曲<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/120.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>为了解决扭曲，在球外加一个立方体包围盒，从球心延申到立方体上，用立方体记录环境光：<strong>Cube Map</strong>。<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/122.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="应用二：Bump-Mapping"><a href="#应用二：Bump-Mapping" class="headerlink" title="应用二：Bump Mapping"></a>应用二：Bump Mapping</h5><p>纹理作为凹凸贴图 &#x2F; 法线贴图，<u>纹理存储点在法线方向的相对高度</u> </p>
<ul>
<li>在不增加几何模型复杂度的情况下（依然是一个球），可以通过应用复杂的纹理，定义一个点的相对高度，从而让法线发生变化、着色的明暗发生变化，实现图右的凹凸效果。<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/124.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>具体方法：</p>
<ul>
<li><p>flatland case：通过凹凸贴图，给光滑的表面加上高度的变化，让点的法线发生扰动。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/125.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>计算方法：可以根据差分方法，近似计算切线方向 $(1, dp)$，再根据垂直关系求得法线方向 $(-dp, 1)$ 。$c$ 是相关系数，越大说明凹凸贴图效果越强</li>
<li>$dp$ 使用相邻两点的高度差计算<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/126.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><p>3D case：也一样，在两个方向求导数</p>
<ul>
<li>为了方便，会假设原来的法线方向是 $(0,0,1)$，也就是在<u>局部坐标系</u>里进行计算，通过纹理映射修改法线方向、然后再计算回世界坐标。涉及到一个简单的坐标变换，在HW3里用到</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/127.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>更现代化的做法：<strong>Displacement mapping（位移贴图）</strong>，跟凹凸贴图类似，纹理都是<u>记录一个点的相对高度</u> </p>
<ul>
<li><p>位移贴图会真的做位置移动，真的移动了顶点的位置。改变了几何</p>
</li>
<li><p>凹凸贴图会根据位置移动，换算成法线的变化，然后做假的顶点运动。没有真的改变几何</p>
</li>
<li><p>下图中，凹凸贴图的边缘还是圆的，阴影也没有改变。位移贴图效果更好，但要求模型本身顶点较多</p>
<ul>
<li>本质上要求模型的变化要跟得上纹理的采样率</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/128.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>往往不想让模型太细致，因此 DirectX 使用 <strong>Dynamic Tessellation（动态曲面细分）</strong>：开始先用粗糙的模型，在应用位移贴图过程中进行检测，如果需要，再将三角形拆开成很多小三角形，再继续做位移贴图</p>
</li>
</ul>
<h5 id="应用三：3D-Procedual-Noise-Solid-Modeling"><a href="#应用三：3D-Procedual-Noise-Solid-Modeling" class="headerlink" title="应用三：3D Procedual Noise + Solid Modeling"></a>应用三：3D Procedual Noise + Solid Modeling</h5><p><u>3D的纹理定义空间中任何一个点的值</u> </p>
<ul>
<li>除了2D纹理，还可以定义3D的纹理，定义空间中任何一个点的值。也可以不直接给出值而是定义一个3D空间的噪声函数，计算出空间中任意一点的值</li>
<li>特别地，图中是 Perlin noise 噪声函数定义的裂缝图案，这个函数也可以定义山脉的起伏等，得到了广泛的应用。<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/129.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
</li>
<li>3D Textures and Volume Rendering：3D纹理，被广泛应用到体积渲染里。比如得出每个点的密度<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/131.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="应用四：Provide-Precomputed-Shading"><a href="#应用四：Provide-Precomputed-Shading" class="headerlink" title="应用四：Provide Precomputed Shading"></a>应用四：Provide Precomputed Shading</h5><p><u>纹理记录之前算好的信息</u> </p>
<ul>
<li>图中，右图眉毛遮挡一部分眼圈、投影阴影过来。在计算Shading时是不考虑这类信息的。</li>
<li>实时的做法叫 Ambient occlusion（环境光遮蔽），后面说</li>
<li>也可以<u>事先计算好</u>、写进一张纹理图（可见1、不可见0）、再把纹理贴上（着色结果跟纹理相乘）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/130.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h4 id="关于-Shading-的回顾"><a href="#关于-Shading-的回顾" class="headerlink" title="+ 关于 Shading 的回顾"></a>+ 关于 Shading 的回顾</h4><ul>
<li>Shading 1 &amp; 2<ul>
<li>Blinn-Phong reflectance model</li>
<li>Shading models &#x2F; frequencies</li>
<li>Graphics Pipeline</li>
<li>Texture mapping</li>
</ul>
</li>
<li>Shading 3<ul>
<li>Barycentric coordinates，三角形内的插值</li>
<li>Texture antialiasing (MipMap)，纹理太大做范围查询</li>
<li>Applications of textures </li>
<li>我的理解：重心坐标做三角形内的插值是在物体上，或者fragment（光栅化后）上的；而双线性、三线性插值是在纹理上的，主要解决纹理跟像素采样区域大小不匹配的问题。</li>
</ul>
</li>
</ul>
<p>到此，已经讲完了除了阴影技术外的光栅化过程，即硬件在做什么、实时渲染编程在做什么。</p>
<p>在实时渲染中，如何生成阴影、如何做近似的全局光照，等比较高级的内容，在之后的光线追踪中再说。</p>
<hr>
<h3 id="Lecture-10-Geometry-1-Introduction"><a href="#Lecture-10-Geometry-1-Introduction" class="headerlink" title="Lecture 10 - Geometry 1 (Introduction)"></a>Lecture 10 - Geometry 1 (Introduction)</h3><h4 id="1-几何的表示方法"><a href="#1-几何的表示方法" class="headerlink" title="1 - 几何的表示方法"></a>1 - 几何的表示方法</h4><h5 id="几何的显式、隐式表达"><a href="#几何的显式、隐式表达" class="headerlink" title="几何的显式、隐式表达"></a>几何的显式、隐式表达</h5><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/132.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>几何是非常复杂的。在图形学中进行如下分类，用不同的方式来表示不同的几何</p>
<h6 id="Implicit（隐式几何）"><a href="#Implicit（隐式几何）" class="headerlink" title="Implicit（隐式几何）"></a>Implicit（隐式几何）</h6><ul>
<li><u>不给出点具体的坐标</u>，只给出点满足的特定关系</li>
<li>广义来说，可以是 $f(x, y, z)&#x3D;0$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/133.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>对于隐式表示，<u>得知有哪些点在图形内（Sampling，或者得知式子描述的图形长什么样子）是困难的；判断一个点是否在内部是简单的</u> <ul>
<li><strong>Sampling</strong> can be hard, but <strong>Inside&#x2F;Outside Tests</strong> are easy.</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/134.png" style="zoom: 50%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/135.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h6 id="Explicit（显式几何）"><a href="#Explicit（显式几何）" class="headerlink" title="Explicit（显式几何）"></a>Explicit（显式几何）</h6><ul>
<li>直接给定所有的点，或通过参数映射的方法（如给定 $(u,v)$ 和对应到 $(x,y,z)$ 的方法）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/136.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>得知所有的点、显示几何形状是简单的；判断一个点是否在内、外是困难的<ul>
<li><strong>Sampling</strong> is easy, but <strong>Inside&#x2F;Outside Test</strong> is hard.</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/137.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>​    <img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/138.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>到目前，没有好的办法能完全解决几何的问题，需要根据需要去选择表示方法。</p>
<h4 id="2-几何的隐式表达"><a href="#2-几何的隐式表达" class="headerlink" title="2 - 几何的隐式表达"></a>2 - 几何的隐式表达</h4><h5 id="更多的隐式表达方法"><a href="#更多的隐式表达方法" class="headerlink" title="更多的隐式表达方法"></a>更多的隐式表达方法</h5><ul>
<li><p>Algebraic Surfaces：直接用式子来表示</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/139.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>Constructive Solid Geometry：通过简单几何的布尔关系，表示更复杂的几何。已被广泛应用在建模软件中</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/140.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>Distance Functions：不直接描述几何，而存储空间中任意一个点到想要描述的几何的最小距离。如果点在物体内部，距离就是负的</p>
<ul>
<li>Signed DF，可以对距离函数做blending，也就是对物体表面做blending</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/141.png" style="zoom:150%;" / loading="lazy"></li>
</ul>
</li>
<li><p>Level Set 水平集：也是距离函数的想法，只是直接把函数的值写在格子上</p>
<ul>
<li>在医学数据（CT，MRI等）、物理模拟等使用，See <a href="http://physbam.stanford.edu/">http://physbam.stanford.edu</a> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/142.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>Fractals 分型：跟递归的概念相似</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/143.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="隐式几何表达方法的特点"><a href="#隐式几何表达方法的特点" class="headerlink" title="隐式几何表达方法的特点"></a>隐式几何表达方法的特点</h5><ul>
<li>优点<ul>
<li>表述起来容易，存储方便。比如只用一个函数，或一个关系</li>
<li>内外查询、到表面距离查询方便</li>
<li>用隐式表示的表面，容易跟光线求交（直线跟平面公式求交）</li>
<li>严格描述物体</li>
<li>容易描述拓扑结构，如流体</li>
</ul>
</li>
<li>缺点<ul>
<li>难以用函数描述复杂的几何，如奶牛</li>
</ul>
</li>
</ul>
<h4 id="3-几何的显式表达"><a href="#3-几何的显式表达" class="headerlink" title="3 - 几何的显式表达"></a>3 - 几何的显式表达</h4><h5 id="更多的显式几何表达方法"><a href="#更多的显式几何表达方法" class="headerlink" title="更多的显式几何表达方法"></a>更多的显式几何表达方法</h5><ul>
<li>Point Cloud 点云：list of points $(x, y,  z)$ <ul>
<li>很简单的表示方法，但有一些局限性，用得不多</li>
<li>扫描出的一堆原始数据就是点云</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/145.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>Polygon Mesh 多边形面：用三角形（多边形）描述复杂物体<ul>
<li>是最广泛应用的方法</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/146.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>下图是真实工程中的一个立方体。分别是8个点（v），6种法线（vn），12个点的纹理坐标（vt），各个顶点形成的三角形（f，顶点v&#x2F;纹理坐标vt&#x2F;法线vn）<ul>
<li>数字对不上是因为自动生成，发生冗余</li>
<li>如，36行是一个三角形：使用5、1、4顶点，1、2、3纹理坐标，1、1、1法线</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/147.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li>以上是几个不同的例子。在图形学中，从曲线开始，定义各种曲线曲面，来定义显式几何。作为下节课的开始</li>
</ul>
<hr>
<h3 id="Lecture-11-Geometry-2-Curves-and-Surfaces"><a href="#Lecture-11-Geometry-2-Curves-and-Surfaces" class="headerlink" title="Lecture 11 - Geometry 2 (Curves and Surfaces)"></a>Lecture 11 - Geometry 2 (Curves and Surfaces)</h3><ul>
<li>Curves: Bezier curves, De Casteljau’s algorithm, B-splines etc</li>
<li>Surfaces: Bezier surfaces, Subdivision surfaces (triangles &amp; quads)</li>
</ul>
<p>贝塞尔曲线给出了每个点的计算方法，是显式的几何表示方法</p>
<h4 id="1-Bezier-curve-贝塞尔曲线"><a href="#1-Bezier-curve-贝塞尔曲线" class="headerlink" title="1 - Bézier curve 贝塞尔曲线"></a>1 - Bézier curve 贝塞尔曲线</h4><h5 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h5><p>用一系列控制点定义某个曲线。定义一个入方向、一个出方向，可以确定一条曲线</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/148.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>定义任意多的控制点，如何画出一条贝塞尔曲线？</p>
<h5 id="计算方法：de-Casteljau’s-Algorithm"><a href="#计算方法：de-Casteljau’s-Algorithm" class="headerlink" title="计算方法：de Casteljau’s Algorithm"></a>计算方法：de Casteljau’s Algorithm</h5><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/149.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>以四个控制点为例。定义一条曲线的起点在时间0，终点在时间1</li>
<li>任意一个时间 t<ul>
<li>根据它在线段 $b_0b_1$ 的相对位置，找到对应在 $b_1b_2,b_2b_3$ 的点，将它们相连</li>
<li>四个点、三条线段，变为了三个点、两条线段 $b_0^1b_1^1, b_1^1b_2^1$ 。每次会少一个点</li>
<li>递归在每个线段上找 t 对应的位置，直到只剩一个点</li>
<li>这个点就是时间 t 对应在贝塞尔曲线上的位置</li>
</ul>
</li>
<li>取一系列 t，最终得到完整的曲线</li>
</ul>
<p>显式几何要么是直接给出点，要么是给出参数化的点计算方法。贝塞尔曲线作为显式几何表示方法，就是给出了每个点的计算方法。</p>
<h5 id="贝塞尔曲线的代数表示"><a href="#贝塞尔曲线的代数表示" class="headerlink" title="贝塞尔曲线的代数表示"></a>贝塞尔曲线的代数表示</h5><ul>
<li><p>从计算过程推导</p>
<ul>
<li><p>下图金字塔里左右边乘的系数写反了</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/150.png" style="zoom: 67%;" / loading="lazy"> 
</li>
<li><p>以三个点的二次贝塞尔（quadratic bezier）为例：</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/151.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>代数定义</p>
<ul>
<li>给定 n 个控制点，就可以得到 n 阶贝塞尔曲线</li>
<li>任意 n 阶数的贝塞尔曲线，上面的时间 t，对应位置由伯恩斯坦多项式作为系数，对给定的控制点的加权</li>
<li>$\bold b^n(t)&#x3D;\bold b^n_0(t)&#x3D;\sum^{n}_{j&#x3D;0}\bold b_jB^n_j(t)$ </li>
<li><div>$B^n_i(t)=\begin{pmatrix}n\\i\end{pmatrix}t^i(1-t)^{n-i}$ </div>

<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/152.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>对于3D空间的控制点，一样可以进行伯恩斯坦多项式的计算<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/153.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><p>贝塞尔曲线的性质</p>
<ul>
<li>规定必须过起点和终点，两个点处切线的方向可以通过一阶展开式来算。如果4个控制点，系数就是3</li>
<li>仿射变换不变性<ul>
<li>可以先对控制点做仿射变换、再计算贝塞尔曲线，不必记录贝塞尔曲线的若干点、做仿射变换</li>
<li>对于投影变换就不行。投影变换不是仿射变换（平移不是仿射变换，透视投影中 $M_{persp\rightarrow ortho}$ 的最后一行也不是齐次坐标的 $(0,0,0,1)$ ）</li>
</ul>
</li>
<li>凸包性<ul>
<li>最终的贝塞尔曲线一定在所有控制点形成的凸包内</li>
<li>凸包：包围所有点的最小凸多边形，图中的蓝色区域<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/154.png" style="zoom: 33%;" / loading="lazy"></li>
</ul>
</li>
<li>由此，如果所有控制点排列在一条线上，贝塞尔曲线肯定就是直线</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Piecewise-Bezier-curve-逐段的贝塞尔曲线"><a href="#Piecewise-Bezier-curve-逐段的贝塞尔曲线" class="headerlink" title="Piecewise Bézier curve 逐段的贝塞尔曲线"></a>Piecewise Bézier curve 逐段的贝塞尔曲线</h5><ul>
<li>不使用很多控制点，定义一个贝塞尔曲线；而是每次用很少的控制点，定义贝塞尔曲线的一段，最后再连起来</li>
<li>通常，大家喜欢定义 Piecewise cubic Bézier（四个控制点、三个线段的贝塞尔曲线），分别是开头、结尾端点 + 两个控制点<ul>
<li>如果希望不同段之间平滑连接，由之前的第一个性质，开头、结尾处的切线方向就是两个控制点之间的方向。因此把对应的三个控制点放到一条直线上就行</li>
</ul>
</li>
<li><a href="http://math.hws.edu/eck/cs424/notes2013/canvas/bezier.html">可以在这里试一下</a> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/155.png" style="zoom: 50%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/156.png" style="zoom: 50%;" / loading="lazy">  </li>
<li>两段贝塞尔曲线的“连续”衔接<ul>
<li>$C^0$ 连续：在几何上都过同一点。两个函数在值上连续<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/157.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>$C^1$ 连续：在过同一点的基础上，共线、方向相反、距离相同。理解为一阶导连续<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/158.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>也有更高阶的连续，要求曲率连续等</li>
</ul>
</li>
</ul>
<h5 id="图形学中的贝塞尔曲线"><a href="#图形学中的贝塞尔曲线" class="headerlink" title="图形学中的贝塞尔曲线"></a>图形学中的贝塞尔曲线</h5><ul>
<li>除了贝塞尔曲线，图形学中也用其他方式定义曲线<ul>
<li>splines（样条）：定义曲线经过的一些点</li>
<li>B-splines：贝塞尔曲线的扩展<ul>
<li>贝塞尔曲线动一个点、整个线会改变，只能通过分段来避免这个缺点。而 B-splines 有更好的局部性</li>
<li>B-splines 是极其复杂的，还有 NURBS 等延申。本课只讲到贝塞尔曲线</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2-Bezier-Surfaces"><a href="#2-Bezier-Surfaces" class="headerlink" title="2 - Bézier Surfaces"></a>2 - Bézier Surfaces</h4><p>从贝塞尔曲线可以得到贝塞尔曲面。</p>
<p>使用双线性插值的思路：在两个方向上，分别应用贝塞尔曲线</p>
<ul>
<li>以 4×4 为例，在 4 行上应用贝塞尔曲线，每个时间都会得到对应的 4 个贝塞尔曲线的点</li>
<li>把这 4 个点作为另一个方向上的贝塞尔曲线的控制点，最终画出曲面</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/160.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>在此过程中，会有其他问题，比如怎样保证把每条贝塞尔曲线拼到一起后，还能形成连续的曲面。往往是通过两个时间尺度 $(u,v)$ 来统一进行参数映射</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/159.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<hr>
<h3 id="Lecture-12-Geometry-3"><a href="#Lecture-12-Geometry-3" class="headerlink" title="Lecture 12 - Geometry 3"></a>Lecture 12 - Geometry 3</h3><p>几何处理（对于多边形网格），shadow mapping</p>
<h4 id="1-Mesh-Operations-Geometry-Processing"><a href="#1-Mesh-Operations-Geometry-Processing" class="headerlink" title="1 - Mesh Operations: Geometry Processing"></a>1 - Mesh Operations: Geometry Processing</h4><h5 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h5><p>Lecture 10 提到，用 Polygon Mesh（多边形网格）表示几何是目前最常用的方式。用三角形或四边形网格，描述不同的表面。是一种显式的几何表示方法。</p>
<p>对于Mesh，涉及到几种几何处理：</p>
<ul>
<li>Mesh subdivision 网格细分：使用更细致的网格，表示更平滑的曲面。upsampling</li>
<li>Mesh simplification 网格简化：用更少的网格，节省存储（在保持基本形状的前提下）。downsampling</li>
<li>Mesh regularization ：避免出现特别尖 &#x2F; 长的三角形，而是都跟正三角形类似。</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/161.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="Mesh-Subdivision-网格细分（upsampling）"><a href="#Mesh-Subdivision-网格细分（upsampling）" class="headerlink" title="Mesh Subdivision 网格细分（upsampling）"></a>Mesh Subdivision 网格细分（upsampling）</h5><p>各种网格细分算法大致都分两步：首先往细节划分，然后改变位置。</p>
<h6 id="Loop-Subdivision"><a href="#Loop-Subdivision" class="headerlink" title="Loop Subdivision"></a>Loop Subdivision</h6><p>loop细分（发明人叫loop，跟循环没关系）分成两步：先细分、再调整。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/162.png" style="zoom: 50%;" / loading="lazy"> 
</li>
<li><p>细分，增加三角形数量</p>
<ul>
<li>loop细分把一个三角形拆成四个</li>
</ul>
</li>
<li><p>调整，改变三角形的位置</p>
<ul>
<li>loop细分把顶点分为 <strong>新、旧</strong> 两组。新顶点是各边的中点，旧顶点是三角形的顶点。使用不同的策略改变它们的位置</li>
<li>对于新顶点，考虑其相邻两个三角形，进行调整<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/163.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>对于旧顶点，一部分考虑相邻的6个旧顶点，另一部分保留自己的位置<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/164.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><p>Loop细分仅对三角形细分，不适用于一般场合</p>
</li>
</ul>
<h6 id="Catmull-Clark-Subdivision"><a href="#Catmull-Clark-Subdivision" class="headerlink" title="Catmull-Clark Subdivision"></a>Catmull-Clark Subdivision</h6><ul>
<li>引入概念<ul>
<li>Non-quad face 非四边形面：不是四边形的面</li>
<li>Extraordinary vertex 奇异点：度不为4的点</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/165.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>细分方法<ul>
<li>非四边形面每条边取中点、每个面取中点，把它们连起来</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/166.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>每一个非四边形面都引入一个新的奇异点；在引入奇异点后，非四边形面消失</li>
<li>宏观上的现象：<u>每个非四边形面，在一次细分后，都变成一个奇异点</u> 。<u>在第一次细分之后，就不可能再有非四边形面了</u>。</li>
</ul>
</li>
<li>调整方法<ul>
<li>依然是分为新点（再分边的中心点、面的中心点）、旧点（原来的顶点），分别做调整</li>
<li>无非就是定义规则、取平均，不再细说</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/167.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h6 id="两种细分方法的效果"><a href="#两种细分方法的效果" class="headerlink" title="两种细分方法的效果"></a>两种细分方法的效果</h6><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/168.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="Mesh-Simplification-网格简化（downsampling）"><a href="#Mesh-Simplification-网格简化（downsampling）" class="headerlink" title="Mesh Simplification 网格简化（downsampling）"></a>Mesh Simplification 网格简化（downsampling）</h5><h6 id="目的-1"><a href="#目的-1" class="headerlink" title="目的"></a>目的</h6><p>出于计算资源的考虑，在不同的情况下会采用不同细分程度的模型。如离得远、在移动端，都常常使用低模。</p>
<p>高模和低模的几何模型，跟纹理的 MipMap 的概念类似，即“层次结构的几何”和“层次结构的图像”。然而，目前实现层次结构的几何是困难的，在存储、过渡（无法三线性插值）上都有不同的难题。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/169.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h6 id="一种方法：Edge-Collapse-边坍缩"><a href="#一种方法：Edge-Collapse-边坍缩" class="headerlink" title="一种方法：Edge Collapse 边坍缩"></a>一种方法：Edge Collapse 边坍缩</h6><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/170.png" style="zoom: 50%;" / loading="lazy"> 
</li>
<li><p>两个问题：如何判断在哪里进行坍缩，具体如何坍缩？</p>
</li>
<li><p>引入一种<strong>二次度量误差</strong>，我们希望把点放在新位置上，可以最小化二次误差</p>
<ul>
<li>二次误差的概念跟 L2 距离相似，即让点到相关面的距离平方和最小</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/171.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>对于模型的所有边，都假设：如果坍缩这条边、并把点放在最佳位置上，会得到一个多大的二次度量误差</p>
<ul>
<li>因此，对于一个模型，会从它的二次度量误差最小的边开始坍缩</li>
<li>对每条边打分，打的分就是二次度量误差。从小的开始、一个个进行坍缩</li>
</ul>
</li>
<li><p>若干问题：</p>
<ul>
<li>坍缩一条边后，会引起其他边的变化，二次度量误差也改变。因此，在一次坍缩后，需要更新关联边的二次度量误差<ul>
<li>需要一个数据结构，能O(1)取最小值，并且以较小的代价更新受影响的元素——<strong>优先队列或堆</strong></li>
</ul>
</li>
<li>通过一步步执行局部最优解，试图达到全局最优解，属于<strong>贪心策略</strong></li>
</ul>
</li>
<li><p>使用边坍缩，简化的模型也能保留一部分特征</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/173.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="2-Shadow-Mapping"><a href="#2-Shadow-Mapping" class="headerlink" title="2 - Shadow Mapping"></a>2 - Shadow Mapping</h4><h5 id="Shadow-Mapping"><a href="#Shadow-Mapping" class="headerlink" title="Shadow Mapping"></a>Shadow Mapping</h5><h6 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h6><p>之前的着色（Blinn-Phong 反射模型），只考虑局部的现象：考虑 shading point 本身，考虑光源、摄像机。不考虑其他物体、或物体其他部分对着色点的影响。</p>
<p>实际上，其他物体挡住了 shading point，光线就到达不了，从而在 shading point 产生阴影。之前说的着色解决不了阴影，现在来解决这个问题——限制在光栅化内。</p>
<h6 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h6><ul>
<li><p>Shadow Mapping 是图像空间的做法（Image-space algorithm）</p>
<ul>
<li><u>在生成阴影的这一步，不需要知道场景的几何信息</u> </li>
<li>Shadow Mapping 本身会产生走样现象</li>
</ul>
</li>
<li><p>关键现象：如果一个点不在阴影里，则<u>（1）可以从摄像机看到这个点，（2）可以从光源看到这个点</u> </p>
</li>
<li><p>Shadow Mapping 只能处理点光源、方向光源的阴影，这种阴影通常都有很明显的边界，一个点要么被看到、要么不被看到，也就是非0即1的判断过程，称为“硬阴影”</p>
</li>
</ul>
<h6 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h6><ul>
<li>第一步：从光源渲染场景，记录不同方向看到的深度<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/174.png" style="zoom: 60%;" / loading="lazy"></li>
</ul>
</li>
<li>第二部：从摄像机渲染场景，看到的点投影回光源，检查光源图记录的该位置深度跟当前深度是否一致。如果不一致，说明被遮挡了<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/175.png" style="zoom: 60%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>从实际渲染场合再次理解这两步：</p>
<ul>
<li>从光源渲染场景，生成深度图<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/176.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>从摄像机渲染场景，每个点投影回光源，对比记录的深度跟当前深度是否相同<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/177.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>可以看到，图中有一些噪点出现，这是由于shadow mapping技术本身的一些问题</li>
</ul>
</li>
<li>得到带有阴影的渲染结果<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/178.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h6 id="Shadow-Mapping的若干问题"><a href="#Shadow-Mapping的若干问题" class="headerlink" title="Shadow Mapping的若干问题"></a>Shadow Mapping的若干问题</h6><ul>
<li>有浮点数判定相等的步骤，会带来数值精度问题</li>
<li>从光源看向场景的过程中，有不同的分辨率选择。如果 shadow map 的分辨率低、渲染场景的分辨率高，则记录的阴影信息是走样的。<ul>
<li>之前提到，光栅化产生锯齿，如果用低分辨率的 shadow map，使用高分辨率渲染场景，那么高分辨率范围内的多个点投影到同一个深度像素上，就会产生有锯齿的阴影。如果用高分辨率的shadow map又会产生开销。</li>
</ul>
</li>
<li>shading 只做一次渲染（MVP变换、视口变换、光栅化），而 shadow mapping 需要渲染场景两遍（先从光源再从相机看向场景）</li>
<li>只能做硬阴影</li>
</ul>
<p>然而，不妨碍 Shadow Mapping 技术成为主流的技术。也有不同的科研工作尝试解决上述三个问题。</p>
<h5 id="硬阴影和软阴影"><a href="#硬阴影和软阴影" class="headerlink" title="硬阴影和软阴影"></a>硬阴影和软阴影</h5><ul>
<li>对于点光源，一个点要么可见、要么不可见，因此会形成边缘锐利的阴影，就是硬阴影</li>
<li>而软阴影指的是阴影会慢慢过渡，不再是非0即1。此外，越靠近物体根部，阴影越硬<ul>
<li>软阴影是物理上的 Penumbra（半影）概念。物理上，一个区域完全看不到光源，就是全影，如果部分看到光源，就是半影</li>
<li>图中右侧是日食现象，有一部分位于本影区域，完全看不到太阳；有一部分位于半影区域，能看到一部分</li>
<li>因此，阴影的类型取决于能看到多少光源；点光源确实只能产生硬阴影；软阴影肯定不是由点光源形成的</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/179.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>本课程至此，已经讲完了图形学四大部分的前两个</p>
<ul>
<li>光栅化：图形学的其他部分也会用到光栅化，因此首先讲。光栅化的着色，以及两次光栅化用 Shadow Mapping 做阴影都讲了</li>
<li>几何：显式隐式的表现方法，三角形面（几何处理，显式方法），曲线曲面（贝塞尔曲线，显式方法）等</li>
<li>光线追踪：不再用光栅化的方法继续做了，因为光栅化有一些现象不好做</li>
<li>动画 &#x2F; 模拟</li>
</ul>
<hr>
<h3 id="Lecture-13-Ray-Tracing-1-Whitted-Style-Ray-Tracing"><a href="#Lecture-13-Ray-Tracing-1-Whitted-Style-Ray-Tracing" class="headerlink" title="Lecture 13 - Ray Tracing 1 (Whitted-Style Ray Tracing)"></a>Lecture 13 - Ray Tracing 1 (Whitted-Style Ray Tracing)</h3><p>为什么光线追踪，Whitted-style光线追踪，光线跟物体求交（隐式表面，三角形，AABB）</p>
<h4 id="1-Basic-Ray-Tracing-Algorithm"><a href="#1-Basic-Ray-Tracing-Algorithm" class="headerlink" title="1 - Basic Ray-Tracing Algorithm"></a>1 - Basic Ray-Tracing Algorithm</h4><h5 id="为什么光线追踪"><a href="#为什么光线追踪" class="headerlink" title="为什么光线追踪"></a>为什么光线追踪</h5><h6 id="光栅化的问题"><a href="#光栅化的问题" class="headerlink" title="光栅化的问题"></a>光栅化的问题</h6><p>光线追踪和光栅化是两个不同的成像方式。光栅化过程中，有一些问题没有解决好：</p>
<ul>
<li>光栅化不好表示全局的效果<ul>
<li>（软）阴影</li>
<li>Glossy反射、间接光照（光线在到达人眼之前弹射不止一次），等</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/180.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>光栅化的着色只考虑光线的一次弹射，从光源到 shading point、到人眼，如Blinn-Phong模型</li>
<li>光栅化的阴影使用 shadow mapping，不能处理软光照，生成软阴影</li>
</ul>
<h6 id="光栅化和光线追踪"><a href="#光栅化和光线追踪" class="headerlink" title="光栅化和光线追踪"></a>光栅化和光线追踪</h6><ul>
<li>把光栅化理解为：<u>一种很快、很近似的渲染方法，质量相对低</u>。可以用于<u>实时</u>场合。</li>
<li>光线追踪符合真实的物理规律，<u>更准确，但更慢</u>。更多作为<u>离线</u>的应用。（一帧要渲染10k CPU hour）</li>
</ul>
<h5 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h5><h6 id="图形学中的光线"><a href="#图形学中的光线" class="headerlink" title="图形学中的光线"></a>图形学中的光线</h6><p>在图形学计算中，通过简化现实生活中的光，给出光线的定义：</p>
<ul>
<li>不考虑波。光线沿着直线传播</li>
<li>光线之间不会发生碰撞，交叉时不互相影响</li>
<li>光线从光源发出，最终进入人眼（打到场景中，经过反射、折射等）。光线追踪就是试图模拟这一过程<ul>
<li>reciprocity：光线的可逆性。人眼发出一些感知的光线，最后打到光源，仍然是一条可行的光路</li>
</ul>
</li>
</ul>
<h6 id="图形学中的Ray-Casting"><a href="#图形学中的Ray-Casting" class="headerlink" title="图形学中的Ray Casting"></a>图形学中的Ray Casting</h6><ul>
<li>对于每个像素，从摄像机连一条光线、穿过像素格子、打到场景中的一个点</li>
<li>把这个点再跟光源连线，判断这个点对光源是否可见（是否在阴影里）</li>
<li>如果可见，就形成了一条有效的光路，可以计算能量并计算颜色（着色）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/181.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h6 id="一个光线追踪例子"><a href="#一个光线追踪例子" class="headerlink" title="一个光线追踪例子"></a>一个光线追踪例子</h6><ul>
<li>认为眼睛是一个针孔摄像机（一个点），光源是点光源，场景中的物体会发生完美的反射</li>
<li>光线从眼睛出发，经过每一个像素格子，投射到场景中，沿着一根光线记录最近的交点<ul>
<li>在投射光线的过程中，也完美解决了深度测试的问题</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/182.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>对于交点，检测会不会被光源照亮<ul>
<li>交点向光源连一条线（shadow ray），如果中间没有物体阻挡，则光源可以照亮这个点，否则会被阴影阻挡</li>
</ul>
</li>
<li>根据法线方向、入射方向、出射方向、光线，计算该点的着色情况，填入像素格子<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/183.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>到此，光线还是只弹射一次。如果想弹射很多次，则使用Whitted-Style光线追踪，它是一个递归的过程。</p>
<h4 id="2-Recursive-Whitted-Style-Ray-Tracing"><a href="#2-Recursive-Whitted-Style-Ray-Tracing" class="headerlink" title="2 - Recursive(Whitted-Style) Ray Tracing"></a>2 - Recursive(Whitted-Style) Ray Tracing</h4><h5 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h5><p>whitted style 就是在模拟光线不断弹射的过程</p>
<ul>
<li>光线会分成若干光路（镜面反射、折射），并且每条光路折射的次数多了</li>
<li>对每一个交点，都连接 shadow ray 判断光源可见性，分别做着色，计算颜色值<ul>
<li>每折射一次，都会发生能量的递减</li>
</ul>
</li>
<li><u>把每条光路所有点的着色加到像素的值里去</u> </li>
<li>对不同的光线进行归类<ul>
<li>从眼睛打出来的第一根光线，称为 primary ray</li>
<li>在一次弹射之后的光线，称为 secondary rays</li>
<li>从折射点往光源的连线，称为 shadow rays</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/184.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h4 id="3-Ray-Surface-Intersection（如何求光线和场景内物体的交点）"><a href="#3-Ray-Surface-Intersection（如何求光线和场景内物体的交点）" class="headerlink" title="3 - Ray-Surface Intersection（如何求光线和场景内物体的交点）"></a>3 - Ray-Surface Intersection（如何求光线和场景内物体的交点）</h4><h5 id="光线"><a href="#光线" class="headerlink" title="光线"></a>光线</h5><ul>
<li>光线：起点和方向<ul>
<li>是一条射线</li>
<li>起点 $\bold o$，方向 $\bold d$ </li>
<li>光线上 $t$ 时刻的点表示为：$\bold r(t)&#x3D;\bold o+t \bold d \space (0 \leq t \lt \infty )$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/185.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="光线跟隐式表面求交"><a href="#光线跟隐式表面求交" class="headerlink" title="光线跟隐式表面求交"></a>光线跟隐式表面求交</h5><h6 id="光线跟球求交：解二次函数"><a href="#光线跟球求交：解二次函数" class="headerlink" title="光线跟球求交：解二次函数"></a>光线跟球求交：解二次函数</h6><ul>
<li>球：$\bold p:(\bold p-\bold c)^2-R^2&#x3D;0$ </li>
<li>交点意味着同时满足在球上、在方向上，因此将两个方程联立即可</li>
<li>$(\bold o+t\bold d-\bold c)^2-R^2&#x3D;0$ </li>
<li>求解这个关于 $t$ 的二次方程即可，取非负数、非虚数的解</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/186.png" style="zoom: 50%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/187.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h6 id="推广：光线跟任意隐式表面求交"><a href="#推广：光线跟任意隐式表面求交" class="headerlink" title="推广：光线跟任意隐式表面求交"></a>推广：光线跟任意隐式表面求交</h6><ul>
<li>隐式几何表面：$\bold p:f(\bold p)&#x3D;0$ </li>
<li>交点处 $f(\bold o+t\bold d)&#x3D;0$ </li>
<li>求解关于 $t$ 的方程，取非负数、非虚数的解，交点是 $\bold o+t\bold d$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/188.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<h5 id="光线跟显式表面求交之1——三角形"><a href="#光线跟显式表面求交之1——三角形" class="headerlink" title="光线跟显式表面求交之1——三角形"></a>光线跟显式表面求交之1——三角形</h5><h6 id="光线跟三角形求交"><a href="#光线跟三角形求交" class="headerlink" title="光线跟三角形求交"></a>光线跟三角形求交</h6><ul>
<li>应用：<ul>
<li>Rendering: visibility, shadows, lighting …</li>
<li>Geometry: inside&#x2F;outside test<ul>
<li>使用拓扑学定理：<u>从一个封闭图形的内部发射射线，跟图形表面的交点数量一定是奇数</u></li>
</ul>
</li>
</ul>
</li>
<li>求光线跟显式几何的交点<ul>
<li>朴素的思路：挨个判断光线跟物体表面的每一个三角形面是否相交，离光源最近的交点就是要求的交点</li>
<li>结果可能是0个交点或1个交点</li>
<li>计算量太大：每根光线要做 #pixels × #objects；如果光线折射，要把新光线再逐像素、逐三角形求交</li>
<li>因此，寻求加速的方法</li>
</ul>
</li>
</ul>
<h6 id="普通方法"><a href="#普通方法" class="headerlink" title="普通方法"></a>普通方法</h6><ul>
<li>思路	<ul>
<li>三角形一定在平面内</li>
<li><strong>首先做三角形跟平面求交，找到交点再判断是否在三角形内</strong></li>
</ul>
</li>
<li>定义平面<ul>
<li>定义平面上任意一个点 $\bold p’$ + 一个法向量 $\bold N$ </li>
<li>$\bold p:(\bold p-\bold p’)\cdot \bold N &#x3D; 0$，展开得 $ax+by+cz+d&#x3D;0$ </li>
<li>变成了光线跟隐式平面求交问题</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/189.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>求光线跟平面的交点<ul>
<li>联立，求解 $(\bold p-\bold p’)\cdot\bold N&#x3D;(\bold o+t\bold d-\bold p’)\cdot \bold N&#x3D;0$ </li>
<li>得 $t&#x3D;\frac{(\bold p’-\bold o)\cdot \bold N}{\bold d\cdot\bold N}$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/190.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>检查点是否在三角形内部，用到之前讲的向量叉乘、查看是否同方向</li>
</ul>
<h6 id="Moller-Trumbore-Algorithm"><a href="#Moller-Trumbore-Algorithm" class="headerlink" title="Moller Trumbore Algorithm"></a>Moller Trumbore Algorithm</h6><ul>
<li>不用分两步先求光线跟平面交点、再判断交点在三角形内，可以直接求光线跟三角形的交点</li>
<li>用<u>重心坐标</u>描述三角形平面上交点的位置 $(1-b_1-b_2)\bold{\vec P_0} + b_1 \bold{\vec P_1} + b_2\bold{\vec P_2}$ </li>
<li>联立，求解 $\bold {\vec O} +t \bold {\vec D} &#x3D; (1-b_1-b_2)\bold{\vec P_0} + b_1 \bold{\vec P_1} + b_2\bold{\vec P_2}$ <ul>
<li>三个未知数 $t,b_1,b_2$；三个维度的三个方程</li>
</ul>
</li>
<li>如果得出 $1-b_1-b_2,b_1,b_2$ 都是非负，可以立刻判定点在三角形内</li>
<li>具体计算方法如图<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/191.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="4-Axis-Aligned-Bounding-Boxes-AABBs"><a href="#4-Axis-Aligned-Bounding-Boxes-AABBs" class="headerlink" title="4 - Axis-Aligned Bounding Boxes(AABBs)"></a>4 - Axis-Aligned Bounding Boxes(AABBs)</h4><h5 id="光线跟显式表面求交之二——Bounding-Volumes"><a href="#光线跟显式表面求交之二——Bounding-Volumes" class="headerlink" title="光线跟显式表面求交之二——Bounding Volumes"></a>光线跟显式表面求交之二——Bounding Volumes</h5><p>之前说过，逐像素、逐三角形求交计算量太大，无法接受，因此采用加速方法。前面讲了普通方法（分两步）和 Moller Trumbore 算法（直接求）。</p>
<p>还有别的加速方法：</p>
<ul>
<li><strong>Bounding Volumes</strong>：用简单的形状把物体包起来<ul>
<li>如果光线碰不到包围盒，那么一定碰不到里面的物体表面</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/192.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>把长方体理解为：<u>三个“对面”形成的交集</u> </li>
<li>通常，使用 Axis-Aligned Bounding Box（轴对齐包围盒，<strong>AABB</strong>），长方体的面沿着坐标轴<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/193.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>AABB的好处：计算方便<ul>
<li>如图，对于任意平面，光线跟平面求交点的计算稍复杂，需要跟法向量 $\bold N$ 做点乘</li>
<li>如果平面跟坐标轴平行，可以直接用距离在轴上的分量、除以方向（法向量也就是速度）在轴上的分量，就得到时间 $t$ 。如图，就是把 $x$ 方向的距离跟时间相除</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/197.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="光线跟AABB求交"><a href="#光线跟AABB求交" class="headerlink" title="光线跟AABB求交"></a>光线跟AABB求交</h5><ul>
<li><p>以2D来考虑，包围盒是两个“对面”形成的交集</p>
</li>
<li><p><u>对于每个“对面”，求出光线进、出的时间</u>（先假设光线是直线，$t$ 可以＜ 0）</p>
<ul>
<li>“对面”的两个平面分别计算 $t$ <ul>
<li>对于两个竖直平面，在 $t_{min}$ 光线跟左边平面相交，在 $t_{max}$ 光线跟右边平面相交</li>
<li>对于两个水平平面，在 $t_{min}$ 光线跟底部平面相交，在 $t_{max}$ 光线跟顶部平面相交</li>
</ul>
</li>
</ul>
</li>
<li><p><u>求线段的交集</u>，就得到光线进、出包围盒的时间</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/194.png" style="zoom: 60%;" / loading="lazy"> </li>
<li><p>为什么求线段的交集？从3D考虑</p>
<ul>
<li><strong>只有当光线进入所有的“对面”，光线才进入了包围盒</strong> </li>
<li><strong>只要光线离开任意一个“对面”，光线就离开了包围盒</strong></li>
</ul>
</li>
<li><p>3D场景的做法</p>
<ul>
<li>对3个“对面”，计算 $t_{min},t_{max}$ </li>
<li>取 $t_{enter}&#x3D;\max(t_{min}),t_{exit}&#x3D;\min(t_{max})$ </li>
<li>如果 $t_{enter}&lt;t_{exit}$，说明光线跟包围盒有交点（光线在这一段时间进入了包围盒）</li>
</ul>
</li>
<li><p>光线是射线而不是直线</p>
<ul>
<li>如果 $t_{exit}&lt;0$，说明包围盒在光线的背后，不可能有交点</li>
<li>如果 $t_{enter}&lt;0$、 $t_{exit}≥0$，说明光源在包围盒内部，光线一定跟包围盒有交点</li>
</ul>
</li>
<li><p>综上，<strong>当且仅当</strong>   $t_{enter}&lt;t_{exit}$ &amp;&amp; $t_{exit}≥0$，光线跟AABB有交点</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/195.png" style="zoom: 50%;" / loading="lazy"> 
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/196.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<hr>
<h3 id="Lecture-14-Ray-Tracing-2-Acceleration-Radiometry"><a href="#Lecture-14-Ray-Tracing-2-Acceleration-Radiometry" class="headerlink" title="Lecture 14 - Ray Tracing 2 (Acceleration, Radiometry)"></a>Lecture 14 - Ray Tracing 2 (Acceleration, Radiometry)</h3><p>在讲这节课的时候，GTC2020有一些新技术</p>
<ul>
<li>DLSS 2.0，使用深度学习的超分辨率算法</li>
<li>RTXGI，实时渲染中的全局光照。之后会讲离线渲染的全局光照<ul>
<li>随着RTX的发展，很多离线的算法都能搬到实时中来</li>
<li>就算实时光线追踪普及，老的实时渲染算法（光栅化）也依然有一定的价值</li>
</ul>
</li>
</ul>
<p>上节课说到了AABB，以及光线如何跟AABB求交。如何<strong>应用AABB，来加速光线跟物体求交</strong>？</p>
<ul>
<li>使用AABB来加速光线追踪<ul>
<li>Uniform grids 均匀格子</li>
<li>Spatial partitions 空间划分</li>
<li>注意，这都是在光线已经跟AABB相交的前提下，在AABB内、求光线跟物体相交的加速方法</li>
</ul>
</li>
<li>Basic radiometry 辐射度量学</li>
</ul>
<h4 id="1-Uniform-Spatial-Partitions-Grids-均匀格子"><a href="#1-Uniform-Spatial-Partitions-Grids-均匀格子" class="headerlink" title="1 - Uniform Spatial Partitions (Grids) 均匀格子"></a>1 - Uniform Spatial Partitions (Grids) 均匀格子</h4><p>思路</p>
<ul>
<li>认为：计算光线跟盒子求交是简单的，计算光线跟物体求交是困难的</li>
<li>整体思路：场景中有若干包围盒，先判断光线是否跟包围盒相交；如果相交，再判断光线是否跟包围盒内的物体相交<ul>
<li>在第二步的包围盒内使用均匀格子，加速判断光线跟物体相交</li>
</ul>
</li>
</ul>
<p>步骤</p>
<ul>
<li>场景预处理<ul>
<li><u>把包围盒划分成格子</u> </li>
<li>标记跟物体表面相交的格子</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/198.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>首先计算光线跟格子相交；如果格子内有物体，就做一次光线与物体求交<ul>
<li>对于计算光线跟格子相交，不需要计算所有的格子。使用把光线的光栅化思想（bresenham，HW1里有），可以得到光线往那个方向去、下个格子是谁</li>
<li>对于计算光线跟物体相交，可以按照之前讲的跟三角形求交的方法做</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/199.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>加速效果分析</p>
<ul>
<li><p>trade-off：格子不能太稀疏也不能太密集</p>
<ul>
<li><p>格子越稀疏，如整个空间只有一个格子，没有加速的意义</p>
</li>
<li><p>格子越密集，做光线与格子求交更频繁</p>
</li>
<li><p>通过启发式的算法，计算出比较合理的格子划分方法是：#cells &#x3D; C * #objs，3D空间C≈27</p>
<ul>
<li>也就是划分27倍物体个数的格子</li>
</ul>
</li>
</ul>
</li>
<li><p>不同的场景，想要划分的格子也不一样</p>
<ul>
<li>出现大规模集中或大规模空白的场景，不适合均匀划分格子</li>
</ul>
</li>
</ul>
<h4 id="2-Spatial-Partitions-空间划分"><a href="#2-Spatial-Partitions-空间划分" class="headerlink" title="2 - Spatial Partitions 空间划分"></a>2 - Spatial Partitions 空间划分</h4><h5 id="树形空间划分方法"><a href="#树形空间划分方法" class="headerlink" title="树形空间划分方法"></a>树形空间划分方法</h5><p>物体分布稀疏的地方可以少划分格子，物体密集的地方多划分格子。</p>
<p>当划分到足够少的物体，就停下来、不再划分。并且组织成树结构：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/200.png" style="zoom: 67%;" / loading="lazy">  
</li>
<li><p>Oct-Tree 八叉树</p>
<ul>
<li>把3D空间均匀划分成8份</li>
<li>缺点是如果维度继续上升，树的叉会指数级增长</li>
</ul>
</li>
<li><p>KD-Tree</p>
<ul>
<li>空间能得到划分，并且跟维度无关</li>
<li>首先水平划分，形成两个格子；每个再竖直划分，分别形成两个格子；继续下去</li>
<li>如果是3D，就是沿x、沿y、沿z循环划分</li>
<li>可以保证空间的均匀划分，并且保持了<strong>二叉树</strong>的性质</li>
</ul>
</li>
<li><p>BSP-Tree</p>
<ul>
<li>空间的二分划分</li>
<li>每次选一个方向把空间分开，不同的空间再分开，只是不是横平竖直地分开</li>
<li>由AABB类比，斜着的线计算上不如 KD-Tree（AABB的优越性就是AA简化了光线跟平面交点的计算）；在高维情况下，也会越来越不好计算（3D要用平面划分，再高维用超平面）</li>
</ul>
</li>
</ul>
<h5 id="KD-Tree"><a href="#KD-Tree" class="headerlink" title="KD-Tree"></a>KD-Tree</h5><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/201.png" style="zoom: 60%;" / loading="lazy"></li>
</ul>
<p>上图是以一边为例的KD-Tree，实际上每个区域都会继续划分，总体是一个二叉树</p>
<ul>
<li><u>在中间结点，只需记录被划分成什么样的格子</u> </li>
<li><u>在叶子节点实际存储跟格子相交的几何形体</u></li>
</ul>
<p>设计具体的数据结构，来存储KD-Tree</p>
<ul>
<li>对于任何结点，需要知道<ul>
<li>沿着哪个轴进行划分</li>
<li>划分的位置（不一定是中间）</li>
<li>两个子节点的指针</li>
</ul>
</li>
<li>叶子节点存储实际的几何物体</li>
</ul>
<p><u>在进行光线追踪之前，会把这个加速结构建立好</u>。这个结构如何帮助我们做光线追踪的加速？</p>
<ul>
<li>光线经过某个结点，就判断光线跟某个结点是否有交点<ul>
<li>如果没有，什么都不做（跟包围盒不相交，就不可能跟其中物体相交）</li>
<li>如果有，则可能跟两个子结点有交点<ul>
<li>判断是否跟两个子结点相交，如果相交则继续判定</li>
<li>如果是叶子结点，就跟结点内所有物体求交</li>
</ul>
</li>
</ul>
</li>
<li>以下图所示的结构为例，首先跟A相交，就判断跟1、B是否相交，然后判断2、C，3、D，到了3是叶子结点，就计算光线跟3中所有的物体求交</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/202.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>其他问题</p>
<ul>
<li>知道了划分格子的方法，以及搜索的方式，还有一个问题：如何判定格子跟物体有交集？也就是<u>如何判定物体存到哪个叶子结点</u> <ul>
<li>这是一个难点，需要判定一个3D立方体是否跟一个三角形有交集<ul>
<li>有各种相交的情况，不能单纯通过顶点判断</li>
</ul>
</li>
</ul>
</li>
<li>一个物体，可能跟很不同的格子有交点，那么在多个叶子结点中都要存这个物体<ul>
<li>一个物体可能出现在多个叶子结点中，这个性质不好</li>
</ul>
</li>
<li>概括来说，KD-Tree①物体可能存在于不同的格子里②建立并不简单，需要考虑三角形与格子的求交。由于这些问题的存在，KD-Tree在近10年用得少了</li>
</ul>
<p>因此，寻求其他的划分方法，不从空间做起、而从物体做起</p>
<h4 id="3-Object-Partitions-amp-Bounding-Volume-Hierarchy-BVH"><a href="#3-Object-Partitions-amp-Bounding-Volume-Hierarchy-BVH" class="headerlink" title="3 - Object Partitions &amp; Bounding Volume Hierarchy (BVH)"></a>3 - Object Partitions &amp; Bounding Volume Hierarchy (BVH)</h4><p>在目前的图形学，不管做实时光线追踪还是各种离线的结构，BVH是被普遍应用的，它解决了KD-Tree的两个问题。</p>
<h5 id="BVH结构"><a href="#BVH结构" class="headerlink" title="BVH结构"></a>BVH结构</h5><h6 id="BVH的划分步骤"><a href="#BVH的划分步骤" class="headerlink" title="BVH的划分步骤"></a>BVH的划分步骤</h6><ul>
<li><p>把一个盒子里的所有三角形组织成两部分，存在一些划分技巧</p>
<ul>
<li><p>沿哪个轴划分</p>
<ul>
<li>可以用KD-Tree相似的划分方法，即各个轴循环，得到尽量均匀的划分结果</li>
<li>也可以用不同技巧，比如<u>每次按照最长的轴划分成两部分</u></li>
</ul>
</li>
<li><p>在哪个位置划分</p>
<ul>
<li>取“中间”的物体，如果有 n 个三角形，取中位数第 n&#x2F;2 个三角形<ul>
<li>所有三角形重心沿某个轴排序，O(nlogn)</li>
<li>任意一列数找第 i 大的数，都可以在 O(n) 时间解决：快速选择算法（TopK）</li>
</ul>
</li>
<li><u>保证两部分的物体数量差不多，也就是保证树接近平衡</u></li>
</ul>
</li>
<li><p>我的理解：某种意义上，这样划分让BVH更加接近平衡二叉树，查找更快</p>
</li>
</ul>
</li>
<li><p>对于这两个部分，分别再求它们的包围盒</p>
</li>
<li><p>再递归划分包围盒</p>
</li>
<li><p>当叶子结点内有足够少的三角形就可以停下了，同样将实际物体记录在叶子结点中</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/203.png" style="zoom: 67%;" / loading="lazy"> </li>
<li><p>以上图为例</p>
<ul>
<li>首先在水平方向，把所有三角形分成红蓝、黄绿两部分，然后划分包围盒</li>
<li>对于红蓝，纵向坐标更长，因此纵向划分成红、蓝两部分，求各自的包围盒</li>
<li>同样，对于黄绿，横向坐标更长，取个数中间的三角形，划分成两部分，再分别求新的包围盒</li>
</ul>
</li>
</ul>
<h6 id="BVH的性质"><a href="#BVH的性质" class="headerlink" title="BVH的性质"></a>BVH的性质</h6><ul>
<li><p>避免了KD-Tree的老问题</p>
<ul>
<li><p>一个物体只可能出现在一个格子中</p>
</li>
<li><p>给一堆三角形求包围盒是容易的，取所有维度上顶点坐标的最小最大值即可</p>
</li>
<li><p>省去了三角形跟格子求交的事情</p>
</li>
</ul>
</li>
<li><p>带来一个新问题</p>
<ul>
<li>对空间的划分不严格，不同的包围盒可以相交</li>
<li>对于每个三角形划分到哪个区域，图中是比较好的划分。也有不同包围盒相交特别多的情况<ul>
<li>因此，关于怎样更好地划分，有很多不同的研究</li>
</ul>
</li>
</ul>
</li>
<li><p>当物体位置移动，或者加了新物体，就需要重新计算新的BVH</p>
</li>
</ul>
<h6 id="BVH的数据结构"><a href="#BVH的数据结构" class="headerlink" title="BVH的数据结构"></a>BVH的数据结构</h6><ul>
<li>中间结点<ul>
<li>包围盒</li>
<li>两个孩子指针</li>
</ul>
</li>
<li>叶子结点<ul>
<li>包围盒</li>
<li>实际的物体</li>
</ul>
</li>
</ul>
<h6 id="BVH伪代码"><a href="#BVH伪代码" class="headerlink" title="BVH伪代码"></a>BVH伪代码</h6><pre class="language-c++" data-language="c++"><code class="language-c++">Intersect(Ray ray, BVH node)&#123;
    &#x2F;&#x2F; 不相交
    if(ray missed node.bbox) return ;
    
    &#x2F;&#x2F; 相交，是叶子结点，就查找所有物体
    if(node is a leaf node)&#123;
        test intersection with all objs;
        return closest intersection;
    &#125;
    
    &#x2F;&#x2F; 相交，不是叶子结点，就查找子结点
    hit1 &#x3D; Intersect(ray, node.child1);
    hit2 &#x3D; Intersect(ray, node.child2);
    
    return the closer of hit1, hit2;
&#125;</code></pre>

<h5 id="物体划分和空间划分"><a href="#物体划分和空间划分" class="headerlink" title="物体划分和空间划分"></a>物体划分和空间划分</h5><p>我的理解：对于AABB内部的空间，前面讲了各种划分方法。实际上，一种是按照空间划分，在划分过程中忽略物体跟空间的关系，也带来了两个问题，分别是物体归属多个格子、树难建立；而BVH是按照物体进行划分，再建立各自的包围盒作为格子空间</p>
<ul>
<li>按照空间划分（KD-Tree）<ul>
<li>物体可能存在于多个格子</li>
<li>需要计算三角形跟包围盒相交问题</li>
</ul>
</li>
<li>按照物体划分（BVH）<ul>
<li>物体只存在于一个格子</li>
<li>不同的包围盒可能有相交</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/204.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>到此，光线跟场景的求交都讲完了（光线跟三角形、AABB、各种加速结构求交）。这些都是 Whitted-Style 光线追踪的内容。</p>
<p>以下就是非Whitted-Style的内容。</p>
<h4 id="4-Basic-Radiometry-辐射度量学"><a href="#4-Basic-Radiometry-辐射度量学" class="headerlink" title="4 - Basic Radiometry 辐射度量学"></a>4 - Basic Radiometry 辐射度量学</h4><p>为了更好地模拟真实世界中的光照，需要引入辐射度量学的基础内容。</p>
<h5 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h5><p>前面成像方法的问题</p>
<ul>
<li><p>HW3，实现了Blinn-Phong光照模型，其中光的强度 $I$ 的物理意义是什么？</p>
<ul>
<li>$\begin{array}{ll}L &#x3D; L_a + L_d + L_s &#x3D; k_aI_a+k_d (I&#x2F;r^2)\max(0, \bold {n\cdot l})+k_s(I&#x2F;r^2)\text{max}(0, \bold n\cdot\bold h)^p\end{array}$ </li>
<li><strong>光栅化</strong>的shading过程，有些数值只用作计算，没有实际的物理意义</li>
</ul>
</li>
<li><p>Whitted-Style<strong>光线追踪</strong>，得到的图像看上去真实吗？</p>
<ul>
<li>在很多地方的计算做了简化，比如反射、折射的能量衰减方式</li>
</ul>
</li>
<li><p>辐射度量学：在物理上准确定义光照的方法</p>
<ul>
<li>把光精确地定义出来，包括物体表面跟光如何精确地作用</li>
<li>定义3D空间中光的一些属性（无时间）<ul>
<li><strong>radiant flux</strong>，<strong>intensity</strong>，<strong>irradiance</strong>，<strong>radiance</strong></li>
<li>翻译过来可能是：辐射通量，辐射强度，辐射照度，辐射亮度</li>
</ul>
</li>
<li>依然基于几何光学来做（光是直线而不是波）</li>
</ul>
</li>
</ul>
<h5 id="属性一：Radiant-Energy-and-Flux-Power"><a href="#属性一：Radiant-Energy-and-Flux-Power" class="headerlink" title="属性一：Radiant Energy and Flux (Power)"></a>属性一：Radiant Energy and Flux (Power)</h5><p>Energy：能量 [J]；Flux (Power)：功率 [lm]</p>
<p>物理量定义</p>
<ul>
<li>Radient Energy：电磁辐射的<strong>能量</strong>，用焦耳表示<ul>
<li>Radient energy is the energy of electromagnetic radiation. It is measured in units of joules, and denoted by the symbol: </li>
<li>$Q\space [\text{J&#x3D;Joule}]$</li>
</ul>
</li>
<li>Radient flux &#x2F; power：<strong>单位时间的能量（功率）</strong><ul>
<li>Radiant flux (power) is the energy emitted, reflected, transmitted or received, per unit time.</li>
<li>$\Phi \equiv\frac{\text{d}Q}{\text{d}t} \space[\text{W&#x3D;Watt}] \space[\text{lm&#x3D;lumen}]^\bold *$ </li>
<li>为了分析能量变化，自然需要定义单位时间的能量。在整套辐射度量体系中，考虑的都是单位时间的性质</li>
<li>类比power，如灯泡的功率（亮度）是多少瓦特。在辐射度量学中使用lumen作为flux的单位</li>
<li>另外一个角度定义flux：给定单位时间，通过平面的光子数量就是flux<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/205.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><strong>用 flux 衡量一个点光源的总亮度</strong></li>
</ul>
</li>
<li>通过energy和power，定义其他的物理量，如：光源辐射的能量（Radiant Intensity），物体表面接收多少能量（Irradiance），光在传播中的能量（Radiance）<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/206.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="属性二：Radiant-Intensity"><a href="#属性二：Radiant-Intensity" class="headerlink" title="属性二：Radiant Intensity"></a>属性二：Radiant Intensity</h5><p>Intensity：单位立体角上的能量；立体角：空间中的一个角度</p>
<h6 id="角和立体角"><a href="#角和立体角" class="headerlink" title="角和立体角"></a>角和立体角</h6><ul>
<li><p>The radiant (luminous) intensity is <u>the power per unit</u> <u>solid angle</u> emitted by a  point light source. </p>
<ul>
<li>power per unit solid angle：<strong>单位立体角上的flux</strong> </li>
<li>$I(\omega)\equiv\frac{\mathrm d \Phi}{\mathrm d\omega}$ </li>
<li>单位：$[\frac{\text W}{\text {sr}}]\space[ \frac{\text{lm}}{\text{sr}}\text{&#x3D;cd&#x3D;candela} ]$</li>
</ul>
</li>
<li><p>角和立体角</p>
<ul>
<li>角（弧度）<ul>
<li>用弧度来定义一个角 $\theta &#x3D; \frac{l}{r}$，弧长和半径同步放大不影响角度的大小</li>
<li>圆对应的 radians 是 $2\pi$</li>
</ul>
</li>
<li>立体角是角在3D空间中的延申<ul>
<li>从球心出发，形成一个锥，打到球面上形成一个面积</li>
<li>$\Omega&#x3D;\frac{A}{r^2}$ </li>
<li>整个球对应的 steradians 是 $4\pi$</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/207.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>微分立体角</p>
<ul>
<li>通过 $\theta,\phi$ 可以定义球面上唯一的方向，分别表示向上方向的夹角、绕向上方向的偏转角度</li>
</ul>
</li>
<li><p>知道了一个方向 $\omega$，就可以求出这个方向上的单位（微分）立体角 $\mathrm d\omega$ </p>
<ul>
<li><p>在球上一个单位的面积为 $\mathrm dA&#x3D;(r\mathrm d\theta)(r\sin \theta \mathrm d\phi)&#x3D;r^2\sin\theta\mathrm d\theta\mathrm d\phi$ ，矩形面积</p>
<ul>
<li><p>微分立体角是单位面积除以 $r^2$，即 $\mathrm d\omega&#x3D;\frac{\mathrm dA}{r^2}&#x3D;\sin\theta\mathrm d\theta\mathrm d\phi$ </p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/208.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>把整个球上的单位立体角积分，肯定就是 $4\pi$ </p>
<ul>
<li><div>$\Omega=\int_{S^2}\mathrm d\omega=\int^{2\pi}_{0}\int^{\pi}_{0}\sin\theta \mathrm d\theta \mathrm d\phi=4\pi$ </div></li>
</ul>
</li>
<li><p>总结：在辐射度量中，如果表示三维空间的一个方向，通常使用 $\omega$ 来表示。可以通过 $\theta,\phi$ 定义它的位置，并且可以通过 $\sin\theta \mathrm d\theta\mathrm d\phi$ 算出它的微分（单位）立体角。</p>
</li>
<li><p>微分立体角的含义是：当 $\theta,\phi$ 各变化一点点，会引起多大的立体角的变化</p>
</li>
</ul>
</li>
</ul>
<h6 id="回到-Intensity-上来"><a href="#回到-Intensity-上来" class="headerlink" title="回到 Intensity 上来"></a>回到 Intensity 上来</h6><ul>
<li>定义一个点光源的 flux，作为它的总亮度</li>
<li><strong>intensity 是光源在任何一个方向上的亮度</strong> <ul>
<li>$\Phi&#x3D;\int_{S^2}I\mathrm d\omega&#x3D;4\pi I$，可以通过积分计算总的 flux</li>
<li>$I&#x3D;\frac{\Phi}{4\pi}$ ，可以计算任何方向上光的 intensity</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/209.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>以一个灯泡举例<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/210.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>两个属性总览：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/211.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="属性三：Irradiance"><a href="#属性三：Irradiance" class="headerlink" title="属性三：Irradiance"></a>属性三：Irradiance</h5><p>Irradiance：单位面积上的能量</p>
<h6 id="irradiance"><a href="#irradiance" class="headerlink" title="irradiance"></a>irradiance</h6><ul>
<li>The irradiance is the power per (perpendicular &#x2F; projected) unit area incident on a surface point.<ul>
<li>power per unit area：$E(\bold x)\equiv\frac{\mathrm d\Phi(\bold x)}{\mathrm dA}$ ，<strong>入射单位面积上的能量（flux）</strong> </li>
<li>单位：$[\rm \frac{W}{m^2}]\space[\frac{lm}{m^2}&#x3D;lux]$ </li>
<li>默认都是垂直入射。跟Blinn-Phong光照模型的漫反射的 Lambert’s Cosine Law 一样，如果光照不是直射，需要乘以 $\cos\theta$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/212.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>用 irradiance 分析光照传播的递减：<ul>
<li>本质上，立体角没变，intensity 也没变，irradiance 以 $r^2$ 在衰减</li>
<li>在辐射度量学里，Intensity 是不变的，因为是固定的立体角，随着传播，面积会变大。真正缩减的是 irradiance。在之前，Blinn-Phong 模型认为衰减的是 Intensity</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/213.png" style="zoom: 60%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="属性四：radiance"><a href="#属性四：radiance" class="headerlink" title="属性四：radiance"></a>属性四：radiance</h5><h6 id="动机-2"><a href="#动机-2" class="headerlink" title="动机"></a>动机</h6><ul>
<li>Radiance is the fundamental field quantity that describes the distribution of light in an environment<ul>
<li>Radiance is the quantity associated with a ray，定义 radiance 是为了用来描述光线的一些属性</li>
<li>Rendering is all about computing radiance，光线追踪需要计算</li>
</ul>
</li>
</ul>
<h6 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h6><ul>
<li>The radiance (luminance) is the power emitted, reflected, transmitted or received by a surface, <u>per unit solid angle, per projected unit area</u>.<ul>
<li>在单位立体角、并且在单位投影面积上的能量</li>
<li>有一个单位大小的面，从这个面、向某一个方向上辐射出的能量。微分两次的概念<ul>
<li>面的大小可能不同，考虑单位大小</li>
<li>面会向各个方向辐射能量，考虑某一个方向</li>
</ul>
</li>
<li>$L(\mathrm p, \omega)\equiv \frac{\mathrm d^2\Phi(\mathrm p,\omega)}{\mathrm d\omega\mathrm dA\cos\theta}$ </li>
<li>单位：$[\rm \frac{W}{sr\space m^2}]\space [\rm\frac{cd}{m^2}&#x3D;\frac{lm}{sr \space m^2}&#x3D;nit]$</li>
</ul>
</li>
<li>把 radiance 跟 irradiance、intensity 联系起来<ul>
<li>定义<ul>
<li>radiance：power per unit solid angle per projected unit area</li>
<li>irradiance：power per projected unit area，光源发出的能量入射到单位面积</li>
<li>intensity：power per solid angle，光源在任意方向的flux</li>
</ul>
</li>
<li>因此<ul>
<li><u>radiance：irradiance per solid angle</u> </li>
<li><u>radiance：intensity per projected unit area</u></li>
</ul>
</li>
</ul>
</li>
<li>以两种方式理解 Radiance<ul>
<li>Incident Radiance，从<strong>接收能量</strong>理解（ir-词根的意思是向内）<ul>
<li>radiance 是单位立体角的 irradiance</li>
<li>irradiance 跟 radiance 的区别：<strong>是否有方向性</strong> <ul>
<li>radiance 表示<u>从某一个方向</u>照射到<u>单位面积</u>上，并且被接收的能量</li>
<li>irradiance 表示这个<u>单位面积</u>在<u>所有方向</u>上接收的能量，把所有方向 radiance 积分起来</li>
</ul>
</li>
<li>$L(\mathrm p, \omega)&#x3D;\frac{\mathrm dE(\mathrm p)}{\mathrm d\omega\cos\theta}$  </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/214.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>Exiting Radiance，从<strong>发出能量</strong>理解<ul>
<li>intensity：单位立体角上的能量，radiance ：单位（垂直）面积上的 intensity</li>
<li>一个小面积向各个方向发出能量，intensity 就是 $\mathrm dA$ 往<strong>某个方向</strong>辐射出去的能量</li>
<li>radiance 是<strong>单位面积</strong>上发出的 intensity</li>
<li>$L(\mathrm p, \omega)&#x3D;\frac{\mathrm dI(\mathrm p,\omega)}{\mathrm dA\cos\theta}$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/215.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="理解-irradiance-和-radiance-的关系"><a href="#理解-irradiance-和-radiance-的关系" class="headerlink" title="理解 irradiance 和 radiance 的关系"></a>理解 irradiance 和 radiance 的关系</h6><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/216.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>irradiance：一个小范围收到的所有能量是多少</li>
<li>radiance：一个小范围收到能量，从某个方向进来收到多少能量</li>
<li>图中的微分、积分式把 irradiance 和 radiance 联系起来</li>
</ul>
<p>有了 Radiance 的概念，就可以定义辐射度量学中的光照了。</p>
<hr>
<h3 id="Lecture-15-Ray-Tracing-3-Light-Transport-amp-Global-Illumination"><a href="#Lecture-15-Ray-Tracing-3-Light-Transport-amp-Global-Illumination" class="headerlink" title="Lecture 15 - Ray Tracing 3 (Light Transport &amp; Global Illumination)"></a>Lecture 15 - Ray Tracing 3 (Light Transport &amp; Global Illumination)</h3><p>通过 irradiance、radiance 的概念，定义所有的物体表面上的光线传播过程</p>
<ul>
<li>Light transport <ul>
<li>the reflection equation</li>
<li>the rendering equation</li>
</ul>
</li>
<li>Global illumination</li>
</ul>
<h4 id="1-Bidirectional-Reflectance-Distribution-Function-BRDF"><a href="#1-Bidirectional-Reflectance-Distribution-Function-BRDF" class="headerlink" title="1 - Bidirectional Reflectance Distribution Function (BRDF)"></a>1 - Bidirectional Reflectance Distribution Function (BRDF)</h4><h5 id="BRDF-双向反射分布函数"><a href="#BRDF-双向反射分布函数" class="headerlink" title="BRDF 双向反射分布函数"></a>BRDF 双向反射分布函数</h5><p>由 irradiance、radiance 的概念，已知入射光的能量，射到物体表面会向各个方向辐射。BRDF用于描述：<u>光线从某个方向进来，并反射到某个方向去，它的能量是多少</u>。</p>
<div>$$f_r(\omega_i\rightarrow\omega_r)=\frac{\mathrm dL_r(\omega_r)}{\mathrm dE(\omega_i)}=\frac{\mathrm dL_r(\omega_r)}{L_i(\omega_i)\cos\theta_i\mathrm d\omega_i}$$</div>

<ul>
<li>反射的一种理解：光线打到某个物体表面，被吸收了，物体表面再把这部分能量发出去（经过一个中间过程）<ul>
<li>一个小区域 $\text dA$，接收到<strong>一个方向</strong>的光线的能量作为 irradiance，然后反射到<strong>各个方向</strong>、转化成各个方向的 radiance</li>
<li>对于 $\text dA$ <ul>
<li>入射能量：接收到某一个方向立体角的 radiance 的能量，即 $L(\omega_i)\cos \theta_i \text d\omega_i$，这也是这块区域接收的 irradiance $dE(\omega_i)$</li>
<li>辐射能量：能量发射到四面八方，每个立体角上的 radiance $dL_r(\omega_r)$</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/217.png" style="zoom: 67%;" / loading="lazy"></li>
<li>即，对于一个单位面积，我们知道它从一个立体角上接收 irradiance，也知道它将会把这些能量发射到各个方向去</li>
</ul>
</li>
<li>BRDF 定义了：发射能量的方式，<strong>能量如何被分配到各个立体角上去</strong>。计算各个出射方向的能量的占比，也就是<strong>所有出射立体角的 radiance，除以这块区域接收到的总的 irradiance</strong> <ul>
<li>$f_r(\omega_i\rightarrow\omega_r)&#x3D;\frac{\mathrm dL_r(\omega_r)}{\mathrm dE(\omega_i)}$</li>
</ul>
</li>
</ul>
<p>再次理解 BRDF 的定义：</p>
<ul>
<li>定义 BRDF 函数来描述这个 radiance：考虑微小面积 $dA$，从某一个微小立体角 $d\omega_i$ 接收到的 irradiance，会如何<u>被分配到各个不同的立体角上去</u> <ul>
<li>对于任何出射方向，算出 radiance $dL_r(x, \omega_r)$，去除以微小面积接收到的 irradiance $dE(\omega_i)$ </li>
<li>这就是 BRDF，它告诉我们如何把一个方向上收集到的能量反射到其他方向上去</li>
<li>$f_r(\omega_i\rightarrow\omega_r)&#x3D;\frac{\mathrm dL_r(\omega_r)}{\mathrm dE(\omega_i)}&#x3D;\frac{\mathrm dL_r(\omega_r)}{L_i(\omega_i)\cos\theta_i\mathrm d\omega_i}$ </li>
<li>单位：$[\frac{1}{\rm sr}]$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/218.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>以上就是 BRDF 的数学或物理上的定义。如果只从理解的角度来看<ul>
<li><p>总的来说，BRDF 是光照打到物体、进行反射，它的<strong>能量分布</strong> </p>
<ul>
<li>如果是镜面反射，在镜面反射方向分布了所有能量，非镜面反射方向不会有能量</li>
<li>如果是漫反射，接收的能量会被均等地分布到所有方向</li>
</ul>
</li>
<li><p>概念上，<u>BRDF 描述了光线和物体是如何作用的</u>。BRDF 定义了物体不同的材质是怎么回事</p>
</li>
</ul>
</li>
</ul>
<h4 id="2-Reflection-Equation-反射方程"><a href="#2-Reflection-Equation-反射方程" class="headerlink" title="2 - Reflection Equation 反射方程"></a>2 - Reflection Equation 反射方程</h4><div>$$L_r(\mathrm p,\omega_r) =\int _{H^2}f_r(\omega_i\rightarrow\omega_r)L_i(\mathrm p,\omega_i)\cos\theta_i\mathrm d\omega_i$$</div>

<p>一个点可以接收四面八方的光照，而 BRDF 定义了每一个方向上接收光照、进行的能量反射情况。</p>
<p>对于每个入射方向，都对应一个BRDF。考虑每一个入射方向，对于出射方向的贡献</p>
<ul>
<li>一个入射方向对于一个出射方向的能量贡献：irradiance &#x3D; $L_i(\mathrm p,\omega_i)\cos\theta_i \text d\omega_i$ </li>
<li><u>irradiance 乘以 BRDF（到某个方向的比例），就是反射到出射方向的能量</u>  </li>
<li><u>所有半球面方向上的贡献积分起来，就是反射方程</u>，得到在所有入射光下、最后反射到该方向上的能量</li>
<li>$L_r(\mathrm p,\omega_r) &#x3D;\int _{H^2}f_r(\omega_i\rightarrow\omega_r)L_i(\mathrm p,\omega_i)\cos\theta_i\mathrm d\omega_i$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/219.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>这是一个<strong>递归</strong>的过程：入射光可能来自光源，也可能是其他物体反射而来的 radiance。之前说的“光线不止弹射一次”也体现在这里。</p>
<h4 id="3-Rendering-Equation-渲染方程"><a href="#3-Rendering-Equation-渲染方程" class="headerlink" title="3 - Rendering Equation 渲染方程"></a>3 - Rendering Equation 渲染方程</h4><h5 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h5><div>$$L_o(p,\omega_o)=L_e(p,\omega_o) + \int_{\Omega^+}L_i(p,\omega_i)f_r(p,\omega_i,\omega_o)(n\cdot\omega_i)\mathrm d\omega_i$$</div>

<p>通过反射方程，可以推出更加通用的渲染方程。</p>
<p>反射方程定义了物体反射光线的方法，但没有考虑物体自身会发光的情况。因此把物体发的光加上就可以了。</p>
<p>我们看到一个物体，它对某一个方向出射的光由两部分组成：它自身发的光 + 反射其他光。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/220.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>通过渲染方程，就用一个公式描述了所有光线的传播。所有限制在物体表面上的光线传播都满足这个方程。</p>
<h5 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h5><h6 id="渲染方程再理解"><a href="#渲染方程再理解" class="headerlink" title="渲染方程再理解"></a>渲染方程再理解</h6><ul>
<li><p>对于一个点光源，最终射出的光 &#x3D; 自发光 + 反射</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/221.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>对于多个点光源，在反射项是多个反射光线相加</p>
<ul>
<li>灯开得多就越亮，不难理解</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/222.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>对于面光源，反射项对立体角 $\mathrm d\omega$ 进行积分</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/223.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>如果不是从光源处直接接收光照，而是接收其它反射光</p>
<ul>
<li>这一点 $X$ 往某个方向辐射的 radiance，依赖于其他点 $X’$ 辐射出来的radiance（递归的过程）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/224.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>通过递归关系，把方程写成一个简单的形式来理解 </p>
<ul>
<li>unknown 不知道的项<ul>
<li>从某个方向看向一个点，不知道看到的能量是多少 $L_r(X,\omega_r)$ </li>
<li>从其他物体反射到该点的radiance $L_r(X’,-\omega_i)$</li>
</ul>
</li>
<li>known 知道的项<ul>
<li>物体的自发光方式 $L_e(X,\omega_r)$ </li>
<li>物体不同的材质 $f(X,\omega_i,\omega_r)\cos\theta_i\mathrm d\omega_i$</li>
</ul>
</li>
<li>在数学上，简单表达为 $l(u)&#x3D;e(u)+\int l(v)K(u,v)dv$ <ul>
<li>两个不同的位置分别用 $u,v$ 表示</li>
<li>$K(u,v)$ 是从其他点的 $l(v)$ 反射到 $u$ 点上的能量系数</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/225.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h6 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h6><ul>
<li>进一步<u>简写成算子形式</u> <ul>
<li>$L&#x3D;E+KL$ <ul>
<li>物体辐射的能量 &#x3D; 光源辐射的能量 + 辐射出来的能量被反射之后的能量</li>
<li>$K$ 是反射操作符，可以把辐射出来的能量反射掉</li>
<li>$E,L$ 是向量，$K$ 是矩阵</li>
<li>关于算子的严格定义，查看数学内容</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/226.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>把算子方程进行数学展开<ul>
<li>我们需要解 $L$（解渲染方程），得到任意一个方向能观测的能量，而通过简写，$L$ 是递归定义的</li>
<li>通过算子，可以求得 $L$ 的表达式（使用单位矩阵、分配律），并且算子有类似泰勒展开的性质<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/227.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>最终，写为 $L&#x3D;E+KE+K^2E+…$，并且 $K$ 是反射操作符</li>
</ul>
</li>
<li>理解为：可以把最后看到的一张图，分解为：<ul>
<li><strong>光线对弹射次数的一种分解</strong> <ul>
<li>直接看到光源会看到 $E$ </li>
<li>＋ 光源辐射出来的能量经过一次反射后会看到 $KE$ </li>
<li>＋ 光源辐射出的能量经过两次反射，会看到 $K^2E$ </li>
<li>＋ …</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/228.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="全局光照"><a href="#全局光照" class="headerlink" title="全局光照"></a>全局光照</h5><h6 id="全局光照的概念"><a href="#全局光照的概念" class="headerlink" title="全局光照的概念"></a>全局光照的概念</h6><ul>
<li>这样，引入了全局光照的概念<ul>
<li>光线不弹射，就是光源</li>
<li>光线弹射一次，直接光照 direct illumination</li>
<li>光线弹射两次，间接光照 indirect illumination</li>
<li>所有不同的光线弹射次数的结果全部加起来，就是<strong>全局光照 global illumination</strong> <ul>
<li>全局光照不仅是间接光照，而是直接光照、间接光照的集合</li>
</ul>
</li>
</ul>
</li>
<li>通过这个式子，再次理解<strong>光栅化</strong> （图中橙色部分）<ul>
<li>光栅化把物体投影到屏幕上，通过屏幕上任意一个着色点、物体位置、光源位置，可以做shading<ul>
<li><strong>光栅化 shading 做的就是直接光照</strong> </li>
<li>如 Blinn-Phong 模型计算环境光 + 漫反射 + 镜面反射，光线到物体到观测点，只有0次和1次的弹射</li>
</ul>
</li>
<li>从间接光照之后，光栅化就难以计算了。这也是使用光线追踪的原因：更方便地计算多次反射后的光照</li>
</ul>
</li>
</ul>
<h6 id="全局光照的效果"><a href="#全局光照的效果" class="headerlink" title="全局光照的效果"></a>全局光照的效果</h6><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/229.png" style="zoom: 80%;" / loading="lazy"> </li>
<li>观察图像<ul>
<li>直接光照看不到的地方就是黑色。从一次间接光照开始（一次间接光照是光弹射两次），阴影部分变亮了</li>
<li>图片上方的玻璃灯，光线弹射次数少的时候，光线出不来。比如双层玻璃，光线需要两次才能弹进去</li>
<li>随着光线弹射次数再增多，画面之间的变化变小了。<u>如果弹射无数次，图片会收敛到一个亮度，而不是一直变亮（过曝）</u> <ul>
<li>从观察上，图片变化越来越小了</li>
<li>从事实上，自然界中所有东西都是全局光照，没有越来越亮</li>
<li>能量守恒，能量是不会一直增加的</li>
<li>联想到摄像时，一直按快门就会过曝，这是因为对于能量，图形学一直考虑的是 flux，也就是单位时间。摄像在时间上积累了能量</li>
</ul>
</li>
</ul>
</li>
<li>也就是第一节课说的：画面越亮，全局光照做得越好</li>
</ul>
<p>还剩一个问题：我们知道并理解了渲染方程，如何解全局光照的渲染方程？——路径追踪就是解渲染方程的一种方式。</p>
<hr>
<h3 id="Lecture-16-Ray-Tracing-4-Monto-Carlo-Path-Tracing"><a href="#Lecture-16-Ray-Tracing-4-Monto-Carlo-Path-Tracing" class="headerlink" title="Lecture 16 - Ray Tracing 4 (Monto Carlo Path Tracing)"></a>Lecture 16 - Ray Tracing 4 (Monto Carlo Path Tracing)</h3><h4 id="1-Probability-Review"><a href="#1-Probability-Review" class="headerlink" title="1 - Probability Review"></a>1 - Probability Review</h4><p>随机变量和概率（Random Variables &amp; Probabilities）</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/233.png" style="zoom: 67%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/234.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>期望（Expected Value）</p>
<ul>
<li>不断取随机变量，求它们的平均</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/235.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>连续情况下的描述</p>
<ul>
<li>概率密度函数（Probability Density&#x2F;Distribution Function，PDF）</li>
<li>PDF的概念在蒙特卡洛积分中会用到</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/236.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>除了随机变量的分布、期望，考虑随机变量的函数的分布、期望</p>
<ul>
<li><p>依然是函数值×概率密度，积分起来</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/237.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h4 id="2-Monte-Carlo-Integration"><a href="#2-Monte-Carlo-Integration" class="headerlink" title="2 - Monte Carlo Integration"></a>2 - Monte Carlo Integration</h4><p>之前讲了 Radiometry；通过 radiance、irradiance 去解释反射问题，并得出了 reflection equation、rendering equation；通过算子把渲染方程拆成弹射形式，并定义 global illumination。</p>
<p>关于概率论相关知识，需要知道对于连续的概率分布函数，积分得到总体概率 1： $\int p(x)\text d x &#x3D; 1$。</p>
<h5 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h5><p>Why</p>
<ul>
<li>给定任何一个函数，需要求它的定积分</li>
<li>这个函数可能很复杂、写不出解析式；或者不关心解析式，因为定积分只要一个数作为结果，不定积分才关心解析式</li>
<li>使用数值方法，来求解这个定积分</li>
</ul>
<p>How</p>
<ul>
<li>回想黎曼积分：把定积分分为100份，每份取中点的函数值、计算矩形面积，最后相加</li>
<li>蒙特卡洛积分：直观上解释，考虑<strong>随机的采样方法</strong> <ul>
<li>求 $\int_{b}^{a}f(x)\text dx$ </li>
<li>取 $(a,b)$ 之间随意的值 $x$，计算它的函数值</li>
<li>$f(x)$ 作为高、$b-a$ 作为宽，将矩形的面积作为定积分的近似值</li>
<li>重复采样多次，把所有的近似值平均起来，得到相对准确的结果</li>
</ul>
</li>
</ul>
<h5 id="蒙特卡罗积分"><a href="#蒙特卡罗积分" class="headerlink" title="蒙特卡罗积分"></a>蒙特卡罗积分</h5><h6 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h6><ul>
<li>定积分：$\int_a^bf(x)\text dx$，求它的值是多少</li>
<li>随机变量：$X_i\sim p(x)$ </li>
<li>蒙特卡洛近似：$F_N&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^{N}\frac{f(X_i)}{p(X_i)}$</li>
</ul>
<h6 id="理解-1"><a href="#理解-1" class="headerlink" title="理解"></a>理解</h6><div>$$\int_a^bf(x)dx=\frac{1}{N}\sum_{i=1}^{N}\frac{f(X_i)}{p(X_i)}$$</div>

<ul>
<li><p>一个例子——均匀分布</p>
<ul>
<li>PDF：$p(X)&#x3D;\frac{1}{b-a}$ </li>
<li>$F_N&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^{N}\frac{f(X_i)}{p(X_i)}&#x3D;\frac{b-a}{N}\sum_{i&#x3D;1}^{N}f(X_i)$  </li>
<li>用矩形的面积理解上式<ul>
<li>$b-a$ 是宽</li>
<li>高：该区间内取 $N$ 次函数值求平均</li>
</ul>
</li>
<li>因此，当随机变量均匀采样，蒙特卡洛积分是合理的</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/238.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li><p>一般地</p>
<ul>
<li><p>对于任何积分 $\int_a^bf(x)dx&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^{N}\frac{f(X_i)}{p(X_i)}$ </p>
</li>
<li><p>结论：在积分域内，以一个 PDF 采样。采样出来的值 $f(X)$ 跟概率 $p(X)$ 相除，多次求平均，就能得到积分的值</p>
</li>
<li><p>只需知道采样对应的 PDF，就能近似计算定积分的值</p>
</li>
<li><p>notes</p>
<ul>
<li>采样数量越多，近似结果越准</li>
<li>蒙特卡洛积分的要求：在 $X$ 上求积分，需要在 $X$ 上采样</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="3-Monte-Carlo-Path-Tracing"><a href="#3-Monte-Carlo-Path-Tracing" class="headerlink" title="3 - Monte Carlo Path Tracing"></a>3 - Monte Carlo Path Tracing</h4><h5 id="回想-Whitted-Style-Ray-Tracing"><a href="#回想-Whitted-Style-Ray-Tracing" class="headerlink" title="回想 Whitted-Style Ray Tracing"></a>回想 Whitted-Style Ray Tracing</h5><ul>
<li>Whitted-Style 光线追踪：<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/184.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>从摄像机发出光线，穿过每一个像素格子，射入场景中</li>
<li>不断弹射光线，在任何一个弹射位置，跟光源连一条 shadow ray 判断光源可见性，分别做着色，计算颜色值<ul>
<li>当光线打到光滑物体，发生镜面反射 &#x2F; 折射</li>
<li>当光线打到漫反射物体，光线停止</li>
</ul>
</li>
<li><u>把每条光路所有点的着色加到像素的值里去</u></li>
</ul>
</li>
<li>以上两个步骤，有不基于物理的部分：<ul>
<li>Whitted-Style 光线追踪无法处理 Glossy 物体：Glossy 表面不是镜面反射，而是把光线集中打到周围一块区域<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/239.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>打到漫反射物体，光线不应该停止，而应该向四面八方散射<ul>
<li>前面讲过，光栅化其实是全局光照中的直接光照。左图是直接光照，右图是加上间接光照、成为全局光照</li>
<li>使用全局光照后，观察天花板的全局光照，方块也展示出了 colour bleeding 现象（被墙“染色”）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/240.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li>Whitted-Style Ray Tracing 是错的，渲染方程是对的：是完全按照物理量推导出来的<ul>
<li><p>$L_o(p,\omega_o)&#x3D;L_e(p,\omega_o) + \int_{\Omega^+}L_i(p,\omega_i)f_r(p,\omega_i,\omega_o)(n\cdot\omega_i)\mathrm d\omega_i$ </p>
</li>
<li><p>需要正确地解出渲染方程，有两个问题</p>
<ul>
<li>考虑来自各方向的光照，需要计算半球的定积分</li>
<li>对于光线是直接还是反射而来，不做区分（递归）</li>
</ul>
</li>
<li><p>它是定积分，采用蒙特卡洛方法来做</p>
</li>
</ul>
</li>
</ul>
<h5 id="直接光照情况下的蒙特卡洛方法"><a href="#直接光照情况下的蒙特卡洛方法" class="headerlink" title="直接光照情况下的蒙特卡洛方法"></a>直接光照情况下的蒙特卡洛方法</h5><h6 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h6><p>以一个简单情况为例，考虑一个着色点的直接光照是什么</p>
<ul>
<li>场景有其他物体会挡住光，有一个大的面光源</li>
<li>考虑着色点接收各个方向 $\omega_i$ 的光线，观测方向 $\omega_o$，方向都向外</li>
<li>该点不发光</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/241.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>求解该点的渲染方程</p>
<div>$$L_o(p,\omega_o)=\int_{\Omega^+}L_i(p,\omega_i)f_r(p,\omega_i,\omega_o)(n\cdot\omega_i)\mathrm d\omega_i$$</div>

<ul>
<li>该点的光照，就是各个方向的入射光 $L_i$ 经过 BRDF 反射到出射方向 $\omega_o$ 的光照之和</li>
<li>由于只考虑直接光照、不考虑多次反射，如果 $\omega_i$ 方向不是光源，$L_i &#x3D;0$ </li>
<li>使用蒙特卡洛方法：<ul>
<li>求方向的积分，就在不同方向上采样</li>
<li>随机选一个方向，作为随机变量。需要知道采样的 PDF</li>
</ul>
</li>
</ul>
<h6 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h6><div>$$\int_a^bf(x)dx=\frac{1}{N}\sum_{i=1}^{N}\frac{f(X_i)}{p(X_i)}$$</div>

<ul>
<li>$x$：方向，随机变量</li>
<li>$f(x)$：$L_i(p,\omega_i)f_r(p,\omega_i,\omega_o)(n\cdot\omega_i)$ </li>
<li>PDF（对半球的采样方法）：<strong>均匀采样</strong> $p(\omega_i)&#x3D;1&#x2F;2\pi$ <ul>
<li>立体角 $\Omega&#x3D;\frac{A}{r^2}$ </li>
<li>半球对应立体角 $2\pi$</li>
</ul>
</li>
<li>近似渲染方程：</li>
</ul>
<div>$$L_o(p,\omega_o)=\int_{\Omega^+}L_i(p,\omega_i)f_r(p,\omega_i,\omega_o)(n\cdot\omega_i)\mathrm d\omega_i \approx\frac{1}{N}\sum_{i=1}^{N}\frac{L_i(p,\omega_i)f_r(p,\omega_i,\omega_o)(n\cdot\omega_i)}{p(\omega_i)}$$</div>

<ul>
<li>由此，写出<strong>只考虑直接光照</strong>情况下，任何点 $p$ 向 $\omega_o$ 方向的着色算法</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">shade<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wo<span class="token punctuation">)</span>
    Randomly choose N directions wi <span class="token operator">~</span> pdf		<span class="token comment"># 对于p点，在N个入射方向采样</span>
    Lo <span class="token operator">=</span> <span class="token number">0.0</span>
    <span class="token keyword">for</span> each wi		<span class="token comment"># 对每一个方向取平均</span>
    	Trace a ray r<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wi<span class="token punctuation">)</span>	<span class="token comment">#建立入射方向wi到p点的光线</span>
    	<span class="token keyword">if</span> r hit the light		<span class="token comment"># 如果wi方向有光源，则计算该点求和式</span>
    		Lo <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> N<span class="token punctuation">)</span> <span class="token operator">*</span> L_i <span class="token operator">*</span> f_r <span class="token operator">*</span> cosine <span class="token operator">/</span> pdf<span class="token punctuation">(</span>wi<span class="token punctuation">)</span>
   	<span class="token keyword">return</span> Lo</code></pre>



<h5 id="引入间接光照的光线追踪"><a href="#引入间接光照的光线追踪" class="headerlink" title="引入间接光照的光线追踪"></a>引入间接光照的光线追踪</h5><ul>
<li>要算出在 Q 点反射多少 radiance 到 P 点，就是在 P 点观察 Q 点、计算 Q 点的直接光照<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/242.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>支持全局光照的路径追踪算法</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">shade<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wo<span class="token punctuation">)</span>
    Randomly choose N directions wi <span class="token operator">~</span> pdf
    Lo <span class="token operator">=</span> <span class="token number">0.0</span>
    <span class="token keyword">for</span> each wi
    	Trace a ray r<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wi<span class="token punctuation">)</span>
    	<span class="token keyword">if</span> r hit the light
    		Lo <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> N<span class="token punctuation">)</span> <span class="token operator">*</span> L_i <span class="token operator">*</span> f_r <span class="token operator">*</span> cosine <span class="token operator">/</span> pdf<span class="token punctuation">(</span>wi<span class="token punctuation">)</span>
    	<span class="token keyword">else</span> <span class="token keyword">if</span> r hit an <span class="token builtin">object</span> at q
    		Lo <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> N<span class="token punctuation">)</span> <span class="token operator">*</span> shade<span class="token punctuation">(</span>q<span class="token punctuation">,</span> <span class="token operator">-</span>wi<span class="token punctuation">)</span> <span class="token operator">*</span> f_r <span class="token operator">*</span> cosine <span class="token operator">/</span> pdf<span class="token punctuation">(</span>wi<span class="token punctuation">)</span>	<span class="token comment"># 递归计算q点出射光</span>
   	<span class="token keyword">return</span> Lo</code></pre>



<h5 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h5><h6 id="问题一：光线爆炸"><a href="#问题一：光线爆炸" class="headerlink" title="问题一：光线爆炸"></a>问题一：光线爆炸</h6><ul>
<li>此过程考虑在着色点上按 PDF 发出 N 个光线，如果打到物体再发出 N 个光线。光线弹射指数级递增</li>
<li>只有 <strong>N&#x3D;1</strong> 时，光线数量才不会爆炸<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/243.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">shade<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wo<span class="token punctuation">)</span>
    Randomly choose ONE directions wi <span class="token operator">~</span> pdf	<span class="token comment"># 在每个点只采样一次</span>
    Trace a ray r<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wi<span class="token punctuation">)</span>	<span class="token comment"># 这条光线hit到什么就返回什么</span>
    <span class="token keyword">if</span> r hit the light
        <span class="token keyword">return</span> L_i <span class="token operator">*</span> f_r <span class="token operator">*</span> cosine <span class="token operator">/</span> pdf<span class="token punctuation">(</span>wi<span class="token punctuation">)</span>
    <span class="token keyword">else</span> <span class="token keyword">if</span> r hit an <span class="token builtin">object</span> at q
        <span class="token keyword">return</span> shade<span class="token punctuation">(</span>q<span class="token punctuation">,</span> <span class="token operator">-</span>wi<span class="token punctuation">)</span> <span class="token operator">*</span> f_r <span class="token operator">*</span> cosine <span class="token operator">/</span> pdf<span class="token punctuation">(</span>wi<span class="token punctuation">)</span></code></pre>

<ul>
<li><strong>用 N&#x3D;1 来做蒙特卡洛积分，叫做 Path Tracing（路径追踪）</strong> <ul>
<li>如果 N ≠ 1，叫做 Distributed Ray Tracing（分布式光线追踪），会产生光线爆炸</li>
<li>而 N &#x3D; 1 就只剩一条光路，称为 Path</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/244.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>对于蒙特卡洛积分，N&#x3D;1 的误差很高。然而，在成像问题上，<u>ray tracing 是从摄像机穿过一个像素打出多条 path，分别计算其着色，再在多个 path 求平均</u>。因此，<strong>对于每个像素发出的 path 数量够多，着色的误差会减小</strong> <ul>
<li>每个 path 只向一个方向反射，形成了一条连接光源和摄像机的路径（path）</li>
<li>在一个像素内多次采样，path tracing 不仅解决了锯齿问题，而且解决了 texture mapping 中的走样问题</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">shade<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wo<span class="token punctuation">)</span>
    Randomly choose ONE directions wi <span class="token operator">~</span> pdf	<span class="token comment"># 在每个反射点只采样一次</span>
    Trace a ray r<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wi<span class="token punctuation">)</span>	<span class="token comment"># 这条光线hit到什么就返回什么</span>
    <span class="token keyword">if</span> r hit the light
        <span class="token keyword">return</span> L_i <span class="token operator">*</span> f_r <span class="token operator">*</span> cosine <span class="token operator">/</span> pdf<span class="token punctuation">(</span>wi<span class="token punctuation">)</span>
    <span class="token keyword">else</span> <span class="token keyword">if</span> r hit an <span class="token builtin">object</span> at q
        <span class="token keyword">return</span> shade<span class="token punctuation">(</span>q<span class="token punctuation">,</span> <span class="token operator">-</span>wi<span class="token punctuation">)</span> <span class="token operator">*</span> f_r <span class="token operator">*</span> cosine <span class="token operator">/</span> pdf<span class="token punctuation">(</span>wi<span class="token punctuation">)</span>

ray_generation<span class="token punctuation">(</span>camPos<span class="token punctuation">,</span> pixel<span class="token punctuation">)</span>
    Uniformly choose N sample positions within the pixel	<span class="token comment"># 每个像素均匀取采样点</span>
    pixel_radiance <span class="token operator">=</span> <span class="token number">0.0</span>
    <span class="token keyword">for</span> each sample <span class="token keyword">in</span> the pixel
        Shoot a ray r<span class="token punctuation">(</span>camPos<span class="token punctuation">,</span> cam_to_sample<span class="token punctuation">)</span>	<span class="token comment"># 对像素的采样点发出path</span>
        <span class="token keyword">if</span> r hit the scene at p
            pixel_radiance <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> N<span class="token punctuation">)</span> <span class="token operator">*</span> shade<span class="token punctuation">(</span>p<span class="token punctuation">,</span> sample_to_cam<span class="token punctuation">)</span>	<span class="token comment"># 对这条path计算着色。像素所有采样点的结果取平均</span>
    <span class="token keyword">return</span> pixel_radiance</code></pre>

<ul>
<li>代码中可以看到，path 由于只是一条光路，不需取平均；而对于像素的每个采样点计算出的 shading 结果取平均</li>
</ul>
<h6 id="问题二：递归结束"><a href="#问题二：递归结束" class="headerlink" title="问题二：递归结束"></a>问题二：递归结束</h6><p>递归需要停止；然而在真实世界中，光线弹射次数也不会停，让递归停止意味着能量损失。</p>
<p>引入方法：<strong>Russian Roulette</strong>（RR，俄罗斯轮盘赌）</p>
<ul>
<li><p>在之前的 path tracing 过程中，每个 shading point 稳定射出一条光线，并得到着色结果 $L_o$ </p>
</li>
<li><p>引入一个概率 $P$ </p>
<ul>
<li>当生效时，着色结果变为 $L_o&#x2F;P$ </li>
<li>当概率 $1-P$ 时，得到着色结果为 $0$</li>
</ul>
</li>
<li><p>这样，总体的期望没有改变：$E&#x3D;P*(L_o&#x2F;P)+(1-P)*0 &#x3D; L_o$</p>
</li>
</ul>
<p>在 shading point 上，不是稳定、而是以概率 $P$ 向外发出一条光线。最后返回的结果除以概率 $P$。这样，算法最终一定会停下来。</p>
<ul>
<li>发出第 n 条光线的概率：$P+P^2+…+P^n&#x3D;\frac{P(1-P^n)}{1-P}$</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">shade<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wo<span class="token punctuation">)</span>
    Manually specify a probability P_RR
    Randomly select ksi <span class="token keyword">in</span> a uniform dist<span class="token punctuation">.</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
    <span class="token keyword">if</span><span class="token punctuation">(</span>ksi <span class="token operator">></span> P_RR<span class="token punctuation">)</span> <span class="token keyword">return</span> <span class="token number">0.0</span><span class="token punctuation">;</span>
    
    Randomly choose ONE directions wi <span class="token operator">~</span> pdf
    Trace a ray r<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wi<span class="token punctuation">)</span>	<span class="token comment"># 这条光线hit到什么就返回什么</span>
    <span class="token keyword">if</span> r hit the light
        <span class="token keyword">return</span> L_i <span class="token operator">*</span> f_r <span class="token operator">*</span> cosine <span class="token operator">/</span> pdf<span class="token punctuation">(</span>wi<span class="token punctuation">)</span> <span class="token operator">/</span> P_RR
    <span class="token keyword">else</span> <span class="token keyword">if</span> r hit an <span class="token builtin">object</span> at q
        <span class="token keyword">return</span> shade<span class="token punctuation">(</span>q<span class="token punctuation">,</span> <span class="token operator">-</span>wi<span class="token punctuation">)</span> <span class="token operator">*</span> f_r <span class="token operator">*</span> cosine <span class="token operator">/</span> pdf<span class="token punctuation">(</span>wi<span class="token punctuation">)</span> <span class="token operator">/</span> P_RR</code></pre>

<p>到此，就得到了正确的 path tracing 方法了。</p>
<h4 id="4-对光源积分"><a href="#4-对光源积分" class="headerlink" title="4 - 对光源积分"></a>4 - 对光源积分</h4><h5 id="之前方法的问题"><a href="#之前方法的问题" class="headerlink" title="之前方法的问题"></a>之前方法的问题</h5><p>目前的方法并不高效：改为 Path Tracing 后，平均化依赖于每个像素取的采样点个数 SPP。当 SPP 小，噪声高。如何在小 SPP 的情况下提高表现？</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/245.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>当前的 Monte Carlo Path Tracing，对于每个 shading point，使用均匀的 PDF $1&#x2F;2\pi$，向四面八方采样（为了解决爆炸问题让 N&#x3D;1，但采样的方向是随机的），也就是打出光线寻找光源。</p>
<p>光源大，需要打出的 path 就少；光源少，可能每 50000 根 path 才能打到光源一次，其他的 path 就浪费了。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/246.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
<p>思路：对于着色点，<strong>不再向四面八方采样，而是向光源采样</strong>。（蒙特卡洛积分是可以选择 PDF 的。当然此处换到光源上也是用均匀的 PDF）</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/247.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>对于图中场景，如果对面光源均匀积分，PDF是 $1&#x2F;A$。然而之前提过，计算哪个区间的积分、就在哪里采样，此处<u>求半球面的积分、在面光源上采样，不能直接用蒙特卡洛方法</u>。</p>
<p>因此，需要<strong>把蒙特卡洛方程写成在光源表面上的积分</strong>。需要得知 $\text d\omega$ 和 $\text dA$ 之间的关系。</p>
<ul>
<li>$\text d\omega$ 是立体角，几何意义是该立体角在半径为 1 的球面上覆盖的面积</li>
<li>立体角的概念：面积 除以 距离平方</li>
<li>$\text d\omega &#x3D; \frac{dA\cos\theta’}{||x’-x||^2}$ <ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/248.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>重写蒙特卡洛渲染方程到光源的积分域：</p>
<div>$$L_o(x,\omega_o)=\int_{\Omega^+}L_i(x,\omega_i)f_r(x,\omega_i,\omega_o)\cos\theta\mathrm d\omega_i=\int_{A}L_i(x,\omega_i)f_r(x,\omega_i,\omega_o) \frac{\cos\theta\cos\theta'}{||x'-x||^2}dA$$</div>

<p>到此，把问题转化为：在光源上进行采样，计算光源上的定积分。又可以使用蒙特卡洛方法了</p>
<ul>
<li>PDF：$1&#x2F;A$ </li>
<li>$f(x)$：积分里面的式子</li>
</ul>
<h5 id="Sampling-the-Light"><a href="#Sampling-the-Light" class="headerlink" title="Sampling the Light"></a>Sampling the Light</h5><p>考虑最终看到的着色结果（radiance）的来源：</p>
<ul>
<li>来自光源：对光源平均采样，进行求解。是直接光照，不涉及RR（直射，不需要让光停下）</li>
<li>来自其他非光源：用之前的方法做。是间接光照，进行RR（作为递归终点）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/249.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">shade<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wo<span class="token punctuation">)</span>
    <span class="token comment"># 计算直接光照</span>
    <span class="token comment"># Contribution from the light source. 对光源采样</span>
    Uniformly sample the light at x' <span class="token punctuation">(</span>pdf_light <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">/</span>A<span class="token punctuation">)</span>
    L_dir <span class="token operator">=</span> L_i <span class="token operator">*</span> f_r <span class="token operator">*</span> cosθ <span class="token operator">*</span>  cosθ<span class="token string">' / |x'</span><span class="token operator">-</span>p<span class="token operator">|</span><span class="token operator">^</span><span class="token number">2</span> <span class="token operator">/</span> pdf_light
    
    <span class="token comment"># 计算间接光照</span>
    <span class="token comment"># Contribution from other reflectors. 对半球采样，应用RR</span>
    L_indir <span class="token operator">=</span> <span class="token number">0.0</span>
    Test Russian Roulette <span class="token keyword">with</span> probability P_RR
    Uniformly sample the hemisphere toward wi <span class="token punctuation">(</span>pdf_hemi <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> 2pi<span class="token punctuation">)</span>
    Trace a ray r<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wi<span class="token punctuation">)</span>
    <span class="token keyword">if</span> r hit a non<span class="token operator">-</span>emitting <span class="token builtin">object</span> at q		<span class="token comment"># 点q不再是光源，才计算。光源已经在L_dir算出来了</span>
    	L_indir <span class="token operator">=</span> shape<span class="token punctuation">(</span>q<span class="token punctuation">,</span> <span class="token operator">-</span>wi<span class="token punctuation">)</span> <span class="token operator">*</span> f_r <span class="token operator">*</span> cosθ <span class="token operator">/</span> pdf_hemi <span class="token operator">/</span> P_RR
        
    <span class="token keyword">return</span> L_dir <span class="token operator">+</span> L_indir</code></pre>



<p>一个小问题：</p>
<ul>
<li>对于光源采样，考虑它对 shading point 的贡献，这里假设了光源没有被遮挡</li>
<li>在shading point跟光源采样点连一条线，查看是否遮挡即可<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/250.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h5><p>到此，路径追踪已经讲完了。</p>
<ul>
<li><u>对于点光源，路径追踪是很难处理的</u>（难以对点光源采样，也难以做出直接光照），在此处不谈。可以把它做成一个<u>很小的面积光源</u> </li>
<li>Path Tracing is indeed difficult<ul>
<li>涉及到物理（辐射度量学）、概率、微积分（蒙特卡洛方法）、编码</li>
</ul>
</li>
<li>对于本课程，Path Tracing 的“入门”的意义较轻，“现代”的意义较重<ul>
<li>Path Tracing 是几乎 100% 正确的算法，Path Tracing 可以做到照片级真实感</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/251.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>Ray Tracing 和 Path Tracing</p>
<ul>
<li>以前说 Ray tracing，就是 Whitted-style Ray Tracing（光栅化的 shading 步骤）</li>
<li>现在说 Ray tracing，可以理解为：<strong>所有光线传播方法的大集合</strong> <ul>
<li>(Unidirectional &amp; bidirectional) path tracing</li>
<li>Photon mapping</li>
<li>Metropolis light transport</li>
<li>VCM &#x2F; UPBP …</li>
</ul>
</li>
<li>怎么生成一张图？现在通常回答：光栅化 or 光线追踪</li>
</ul>
<p>过程中略过的问题——相关研究领域</p>
<ul>
<li>怎样对半球均匀采样？generally，给定一个函数、一个 PDF，怎样进行采样？</li>
<li>蒙特卡洛积分可以用于任意 PDF，目前使用了均匀分布。<ul>
<li>选择什么样的PDF是最好的？怎样针对性地对某个函数选择最好的采样方法。（importance sampling 重要性采样理论）</li>
</ul>
</li>
<li>不同的随机数实现方法，能实现不扎堆的分布采样，性能更好<ul>
<li>low discrepancy sequences 低差异序列</li>
</ul>
</li>
<li>能否把采样半球、采样光源两种方法结合起来，得到更好的效果？<ul>
<li>multiple importance sampling，MIS</li>
</ul>
</li>
<li>从像素射出若干 path，为什么把它们平均起来就是像素的 radiance（是否要加权平均）？像素代表着什么？<ul>
<li>pixel reconstruction filter</li>
</ul>
</li>
<li>每个像素的 radiance 算出来了，并不是一个颜色。并且 radiance 跟颜色不是线性对应的。如何进行映射？<ul>
<li>gamma correction 伽马矫正，HDR 图，curves，color space</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Lecture-17-Materials-and-Appearances"><a href="#Lecture-17-Materials-and-Appearances" class="headerlink" title="Lecture 17 - Materials and Appearances"></a>Lecture 17 - Materials and Appearances</h3><p>接下来的课程安排：17、18 是材质的内容，18 是前沿的做法；然后是 19 光场、20 颜色的专题知识；最后两节课是物理模拟 &#x2F; 动画的入门内容。</p>
<p>有不同的材质，在不同的光照下，表现为不同的外观。光线追踪的渲染主要研究的是：不同的光照与不同的材质之间的作用方式。</p>
<p>在光线追踪中，材质 &#x3D;  BRDF，本节课就是讲各种不同材质的 BRDF。</p>
<h4 id="1-Materials"><a href="#1-Materials" class="headerlink" title="1 - Materials"></a>1 - Materials</h4><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/252.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p><strong>Material &#x3D; BRDF</strong></p>
<ul>
<li><p>在 rendering equation 中，BRDF 定义了光从一个方向反射到另一个方向上的能量。BRDF 项决定光的反射方式，也就是物体表面的材质。 </p>
</li>
<li><p>$L_o(p,\omega_o)&#x3D;L_e(p,\omega_o) + \int_{\Omega^+}L_i(p,\omega_i)f_r(p,\omega_i,\omega_o)(n\cdot\omega_i)\mathrm d\omega_i$ </p>
</li>
<li><p>$f_r(\omega_i\rightarrow\omega_r)&#x3D;\frac{\mathrm dL_r(\omega_r)}{\mathrm dE(\omega_i)}&#x3D;\frac{\mathrm dL_r(\omega_r)}{L_i(\omega_i)\cos\theta_i\mathrm d\omega_i}$ </p>
</li>
<li><p>本节课就是讲各种不同材质的 BRDF</p>
</li>
</ul>
<h5 id="不同的物体表面材质"><a href="#不同的物体表面材质" class="headerlink" title="不同的物体表面材质"></a>不同的物体表面材质</h5><h6 id="漫反射材质"><a href="#漫反射材质" class="headerlink" title="漫反射材质"></a>漫反射材质</h6><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/253.png" style="zoom: 67%;" / loading="lazy"> 
</li>
<li><p>漫反射材质均匀向外发出光线</p>
</li>
<li><p>如果假设入射光也是 uniform 的，并且 shading point 不自发光，就可以从能量守恒的角度理解漫反射：接收各方向的均匀光线即 irradiance，把它们全部散射出去。从每个立体角考虑，<strong>入射 radiance&#x3D;出射 radiance</strong> （因为入射和出射都是 uniform 的，并且能量守恒）</p>
</li>
</ul>
<p>计算：</p>
<ul>
<li>由于都是均匀的，假设入射光 $L_i(\omega_i)$ 是常数，BRDF $f_r$ 也是常数</li>
<li>半球上的立体角积分是 $2\pi$，cosine积分是 $\pi$</li>
</ul>
<div>$$L_o(\omega_o)=\int_{H^2}f_rL_i(\omega_i)\cos\theta_i\mathrm d\omega_i=f_rL_i\int_{H^2}\cos\theta_i\mathrm d\omega_i=\pi f_rL_i$$</div>

<ul>
<li><p>由于入射 radiance &#x3D; 出射 radiance，$L_o&#x3D;L_i$ </p>
</li>
<li><p>得 $f_r&#x3D;\frac{1}{\pi}$ </p>
</li>
<li><p>定义一个漫反射的反射率 albedo，在0~1，可以引入不同颜色的 albedo。入射、出射方式依然是均匀的，但出射更少</p>
</li>
</ul>
<div>$$f_r=\frac{\rho}{\pi},f_r\in[0,1/\pi]$$</div>



<h6 id="Glossy-材质（不是很光滑的金属）"><a href="#Glossy-材质（不是很光滑的金属）" class="headerlink" title="Glossy 材质（不是很光滑的金属）"></a>Glossy 材质（不是很光滑的金属）</h6><p>光线向一个方向集中反射</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/254.png" style="zoom: 60%;" / loading="lazy"></li>
</ul>
<h6 id="玻璃-x2F-水的材质"><a href="#玻璃-x2F-水的材质" class="headerlink" title="玻璃 &#x2F; 水的材质"></a>玻璃 &#x2F; 水的材质</h6><p>有反射也有折射</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/255.png" style="zoom: 60%;" / loading="lazy"></li>
</ul>
<h4 id="2-反射和折射，菲涅耳项"><a href="#2-反射和折射，菲涅耳项" class="headerlink" title="2 - 反射和折射，菲涅耳项"></a>2 - 反射和折射，菲涅耳项</h4><h5 id="完美的镜面反射-Reflection"><a href="#完美的镜面反射-Reflection" class="headerlink" title="完美的镜面反射 Reflection"></a>完美的镜面反射 Reflection</h5><p>入射、出射方向沿着法线方向对称。可以通过这个关系，计算出射方向 $\omega_o$ </p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/256.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>回想 Blinn-Phong 模型，考虑出、入射方向的 half vector 跟法线方向是否接近，不考虑出射光和观测方向是否一致。这是因为如图所示，计算出射方向确实不简单，涉及到点乘操作。而计算half vector只需两个向量相加、normalize 即可。</p>
<p>所有的反射都能用 BRDF 来描述。镜面反射的 BRDF 是什么？这个值很难求，涉及到 delta 函数，此处不提。</p>
<h5 id="折射-Refraction"><a href="#折射-Refraction" class="headerlink" title="折射 Refraction"></a>折射 Refraction</h5><p>右下角是 caustics 现象：海水反射阳光，海底的区域有几率同时被很多光线打到，表现为很亮。（这个现象 path tracing 不好做）</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/257.png" style="zoom: 60%;" / loading="lazy"></li>
</ul>
<h6 id="Snell’s-Law-折射定律"><a href="#Snell’s-Law-折射定律" class="headerlink" title="Snell’s Law 折射定律"></a>Snell’s Law 折射定律</h6><p>相关概念</p>
<ul>
<li>定义入射角 $\theta_i$，折射角 $\theta_t$ </li>
<li>不同材质有不同的折射率 $\eta$，材质间的折射满足 $\eta_i\sin\theta_i&#x3D;\eta_t\sin\theta_t$ </li>
<li>入射、出射的方位角关系跟反射一样，都是正相反</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/258.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>由折射定律，就能计算折射角 $\theta_t$ </p>
<div>$$\cos\theta_t=\sqrt{1-\sin^2\theta_t}=\sqrt{1-(\frac{\eta_i}{\eta_t})^2\sin^2\theta_i}=\sqrt{1-(\frac{\eta_i}{\eta_t})^2(1-\cos^2\theta_i)}$$</div>

<ul>
<li>观察此式，当 $1-(\frac{\eta_i}{\eta_t})^2(1-\cos^2\theta_i)&lt;0$ 时，公式是没有意义的，此式可以推出 $\frac{\eta_i}{\eta_t} &gt; 1$、$\eta_i &gt; \eta_t$ <ul>
<li>也就是说：光从某个介质来、到某个介质去，<u>入射介质的折射率大于折射介质的折射率时，可能会出现没有折射的现象，称为<strong>全反射</strong>现象</u> （从稀疏的地方打到密的地方才折射，比如从空气入水）</li>
<li>Snell’s Window &#x2F; Circle：人在水底只能看到 97.2° 的锥形区域<ul>
<li>考虑光线从水下射向大气，超过角度的光线都在水下发生全反射、不会进入空气中</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/259.png" style="zoom: 50%;" / loading="lazy">  </li>
<li>为了把光线追踪做对，很多物理现象都需要考虑。对于之前作业里的球，球的性质是光线入射和反射有对称性，而对于复杂的物体，可能会在内部发生全反射，需要更细致的判断</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>关于折射的 BRDF，应该叫 BTDF。（BRDF 是描述物体表面的反射的。）通常，认为折射、反射都是散射 scatter，会把两者统称为 BSDF</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/260.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="菲涅耳项"><a href="#菲涅耳项" class="headerlink" title="菲涅耳项"></a>菲涅耳项</h5><p>反射和折射的性质——菲涅耳项（Fresnel Reflection &#x2F; Term）</p>
<ul>
<li><p>放在墙边的书，从高处看不到反射，接近平视时可以看到明显的反射</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/261.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p><u>入射光（指观测方向）跟法线的角度，决定了有多少能量会被反射</u> </p>
</li>
<li><p>光打到物体表面，有一部分反射、有一部分折射。通过菲涅耳项，可以解释有多少能量进行反射、有多少能量进行折射。</p>
<ul>
<li>图中是折射率1.5的绝缘体<ul>
<li>如图红线，<u>入射光跟法向量垂直（跟物体表面平行），很多能量被反射；入射光跟物体表面垂直，很多能量被折射</u>。</li>
<li>联想日常生活中，垂直看玻璃会看到窗外，平行看玻璃（坐车看前排的玻璃）会看到反射。</li>
<li>另外两条线反映极化性质，S、P是两个不同方向的偏振方向，渲染中不考虑</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/262.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>对于导体（金属），菲涅耳项跟绝缘体不一样<ul>
<li>即使是垂直看，也是光线中的大部分被反射</li>
<li>因此使用铜、银制镜子，在所有情况下反射率都很高，而不用玻璃</li>
<li>理解：导体的折射率是一个复数</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/263.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><p>菲涅耳项的计算</p>
<ul>
<li><strong>可以精确计算反射出多少能量</strong>。可以看到，跟入射光跟法线的夹角有关，也跟介质有关</li>
<li>由于不关心极化，把S、P两个极求平均即可</li>
<li>也有简化方法：Schlick’s approximation，认为曲线在0°~90°的反射率一直是上升的过程<ul>
<li>0°时为 $R_0$，90°为 $1$ </li>
<li>只要不对材质的要求非常高，这就是图形学中被广泛应用的近似方法</li>
</ul>
</li>
<li>如图为具体计算方法</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/264.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="3-Microfacet-Materials-微表面模型"><a href="#3-Microfacet-Materials-微表面模型" class="headerlink" title="3 - Microfacet Materials 微表面模型"></a>3 - Microfacet Materials 微表面模型</h4><p>定义一个真正的基于物理的材质</p>
<h5 id="动机-3"><a href="#动机-3" class="headerlink" title="动机"></a>动机</h5><ul>
<li>基于一个假设：当离得足够远，在看向一个物体时，看不到微小的具体，只看到了最终对光的总体效应</li>
<li>图中，在空间站上看到地球陆地发生了完美的镜面反射，没有被细节的地形起伏所影响<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/265.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="微表面模型"><a href="#微表面模型" class="headerlink" title="微表面模型"></a>微表面模型</h5><p>假设物体表面是粗糙的，从近处可以看到 microsurface，但从远处看就是平的 macrosurface</p>
<ul>
<li><p>Macroscale：flat &amp; rough</p>
<ul>
<li>从远处，看到平坦、粗糙的平面</li>
<li><strong>从远处看到的是材质</strong></li>
</ul>
</li>
<li><p>Microscale：bumpy &amp; specular</p>
<ul>
<li>从近处看到凹凸不平的表面，并且<u>每个表面的微元都认为是完全的镜面反射</u> </li>
<li>每个微表面都有自己的法线</li>
<li>在物理上也是这样的：大量的微小镜子分布不规则，把光线均匀地反射到各个方向，形成漫反射</li>
<li><strong>从近处看到的是几何</strong></li>
</ul>
</li>
</ul>
<p><u>镜头拉远后，几何就变成材质</u>。</p>
<h5 id="微表面模型的-BRDF"><a href="#微表面模型的-BRDF" class="headerlink" title="微表面模型的 BRDF"></a>微表面模型的 BRDF</h5><p>由于每个微表面有自己的法线，可以研究每个微表面的法线分布</p>
<ul>
<li><p>如果表面平滑，所有法线都跟整体平面法向量差不多，这就是 <u>glossy 材质</u> </p>
<ul>
<li>如果全在一个方向，就是 <u>镜面反射材质</u></li>
</ul>
</li>
<li><p>如果表面粗糙，所有微表面朝向各不相同，形成 <u>漫反射材质</u> </p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/266.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>发现：<strong>可以通过微表面的法线分布，表示不同的材质</strong>  </p>
<div>$$f(\bold i,\bold o) = \frac{\bold F(\bold i,\bold h)\bold G(\bold i,\bold o,\bold h)\bold D(\bold h)}{4(\bold n,\bold i)(\bold n,\bold o)}$$</div>

<ul>
<li>$h$：half vector</li>
<li>$F(i, h)$：菲涅耳项<ul>
<li>总共反射的能量</li>
</ul>
</li>
<li>$G(i,o,h)$：shadowing-masking<ul>
<li>微表面之间可能会发生遮挡，有一些微表面会失去作用</li>
<li>当光线平着打到微平面，或者观测方向平行于微表面（grazing angle），容易发生这种现象</li>
<li>这一项用来修正这个现象</li>
</ul>
</li>
<li>$D(h)$：法线的分布<ul>
<li>只有微表面的法线方向跟 half vector 完全一致（因为微表面是镜面反射），才能把入射方向反射到出射方向</li>
<li>把所有的微表面在该方向的法线方向做查询</li>
<li>这一项决定了的反射方法，决定了集中还是发散</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/267.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h5><p>微表面模型非常强大，可以渲染出很高质量的图片。是最近 state-of-art 的模型。</p>
<p>目前提到 PBR（Physics-based Rendering），就一定会使用微表面模型。</p>
<p>微表面模型是一个统称，有很多不同的模型，都遵循划分微表面这一套。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/268.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h4 id="4-各向同性材质-Isotropic、各向异性材质-Anisotropic"><a href="#4-各向同性材质-Isotropic、各向异性材质-Anisotropic" class="headerlink" title="4 - 各向同性材质 Isotropic、各向异性材质 Anisotropic"></a>4 - 各向同性材质 Isotropic、各向异性材质 Anisotropic</h4><p>拉丝金属反射的光：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/269.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>区别：微表面的方向性</p>
<ul>
<li>各向同性 Isotropic：微表面并不存在方向性，或者方向性很弱</li>
<li>各向异性 Anisotropic：水平和竖直完全不一样，微表面的法线分布有明确的方向性</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/270.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="各向异性-BRDF"><a href="#各向异性-BRDF" class="headerlink" title="各向异性 BRDF"></a>各向异性 BRDF</h5><ul>
<li>输入方向 $\theta_i,\phi_i$，输出方向 $\theta_r,\phi_r$，BRDF $f_r(\theta_i,\phi_i;\theta_r,\phi_r)$ </li>
<li>如果不满足在相对方位角上旋转、得到相同的BRDF，就是各向异性的材质：$f_r(\theta_i,\phi_i;\theta_r,\phi_r) \ne f_r(\theta_i,\theta_r,\phi_r-\phi_i)$ <ul>
<li>入射、出射方向的相对方位角不变、绝对方位角改变，得出的BRDF不一样</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/271.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>生活中的各向异性：<ul>
<li>锅的底部，一圈圈刷的</li>
<li>尼龙，上下叠起来</li>
<li>天鹅绒，把毛都刷到一边</li>
</ul>
</li>
</ul>
<h4 id="5-BRDF的若干性质"><a href="#5-BRDF的若干性质" class="headerlink" title="5 - BRDF的若干性质"></a>5 - BRDF的若干性质</h4><ul>
<li>Non-negativity 非负性<ul>
<li>BRDF 表示能量的分布，永远是非负的</li>
<li>$f_r(\omega_i\rightarrow\omega_r)\ge0$</li>
</ul>
</li>
<li>Linearity 线性性质<ul>
<li>对于 Blinn-Phong 模型，分成 difused、specular、ambiant，分别计算后把结果相加</li>
<li>BRDF 本身也可以拆成很多块，可以分别计算并相加，得到的结果等同于整个做BRDF</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/272.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>Reciprocity principle 可逆性<ul>
<li>光路是可逆的：入射、出射方向互换，一定得到相同的BRDF值</li>
<li>$f_r(\omega_r\rightarrow\omega_i)&#x3D;f_r(\omega_i\rightarrow\omega_r)$</li>
</ul>
</li>
<li>Energy conservation 能量守恒<ul>
<li>BRDF 不会让能量变多</li>
<li>在 path tracing 时，经过无限次的光线弹射后，能量最终收敛，本质上就是能量守恒性质</li>
<li>$\forall\omega_r\int_{H^2}f_r(\omega_i\rightarrow\omega_r)\cos\theta_i\mathrm d\omega_i\le 1$</li>
</ul>
</li>
<li>Isotropic vs. anisotropic 各向同性各向异性<ul>
<li>如果各向同性，$f_r(\theta_i,\phi_i;\theta_r,\phi_r) &#x3D; f_r(\theta_i,\theta_r,\phi_r-\phi_i)$ <ul>
<li>入射、出射方向的相对方位角不变，BRDF 就不变</li>
<li>原本四维的 BRDF，如果是各向同性材质，就是三维的</li>
<li>并且，由可逆性质，$f_r(\theta_i,\theta_r,\phi_r-\phi_i)&#x3D;f_r(\theta_i,\theta_r,\phi_i-\phi_r)&#x3D;f_r(\theta_i,\theta_r,|\phi_r-\phi_i|)$ </li>
<li>相对方位角的正负也不必考虑了</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/273.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="6-BRDF的测量与储存"><a href="#6-BRDF的测量与储存" class="headerlink" title="6 - BRDF的测量与储存"></a>6 - BRDF的测量与储存</h4><p>BRDF 可以用各种基于物理的模型来描述（近似），但只有测出来的才是真正准确的 BRDF</p>
<ul>
<li>推出的 BRDF 跟实际值有一定的差距</li>
<li>如果可以测量，就可以不去推模型了</li>
</ul>
<p>怎么测？</p>
<ul>
<li>BRDF 就是两个方向的函数<ul>
<li>盯着某一点看，改变入射方向（用灯在四面八方照），相机从四面八方拍，这样就覆盖了 BRDF 所有可能的输入、输出方向对</li>
<li>枚举所有入射、出射方向，测量四维的 BRDF（对于每个入射方向，让出射方向在球面上全走一遍，二维乘二维）</li>
</ul>
</li>
<li>改进方法：<ul>
<li>如果测各向同性的 BRDF，就把四维降成三维</li>
<li>如果考虑可逆性，减少一半的测量</li>
<li>设计更好的思路，不采样所有的方向对，测量某些样本、猜剩下的</li>
</ul>
</li>
</ul>
<p>测量完 BRDF 的存储要求：表示简洁、测量数据的精确表示、任意方向对的有效评估、重要采样的良好分布</p>
<p>一个 BRDF 数据库：MERL</p>
<ul>
<li><p>测量了很多不同的材质（刚开始只有各向同性的，测量三维数据，即两个 $\theta$、相对方位角的绝对值）</p>
</li>
<li><p>最终结果存到三维数组里</p>
</li>
<li><p>每个材质做 90*90*180 次，需要进行压缩</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/274.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<hr>
<h3 id="Lecture-18-Advanced-Topics-in-Rendering"><a href="#Lecture-18-Advanced-Topics-in-Rendering" class="headerlink" title="Lecture 18 - Advanced Topics in Rendering"></a>Lecture 18 - Advanced Topics in Rendering</h3><h4 id="1-Advanced-Light-Transport"><a href="#1-Advanced-Light-Transport" class="headerlink" title="1 - Advanced Light Transport"></a>1 - Advanced Light Transport</h4><h5 id="总览-1"><a href="#总览-1" class="headerlink" title="总览"></a>总览</h5><ul>
<li>Unbiased light transport methods 无偏光线传播方法<ul>
<li>Bidirectional path tracing (BDPT) 双向路径追踪</li>
<li>Metropolis light transport (MLT)</li>
</ul>
</li>
<li>Biased light transport methods 有偏光线传播<ul>
<li>Photon mapping 光子映射</li>
<li>Vertex connection and merging (VCM)</li>
</ul>
</li>
<li>Instant radiosity (VPL &#x2F; many light methods) 实时辐射度</li>
</ul>
<h5 id="Biased-vs-Unbiased-Monte-Carlo-Estimators"><a href="#Biased-vs-Unbiased-Monte-Carlo-Estimators" class="headerlink" title="Biased vs. Unbiased Monte Carlo Estimators"></a>Biased vs. Unbiased Monte Carlo Estimators</h5><p>蒙特卡洛方法的有偏和无偏</p>
<ul>
<li>An unbiased Monte Carlo technique does not have any  systematic error<ul>
<li>The expected value of an unbiased estimator will always be the correct value, no matter how many samples are used</li>
<li>不管使用多少采样个数，蒙特卡洛方法估计的结果，期望是真实值，就是无偏的</li>
</ul>
</li>
<li>Otherwise, biased<ul>
<li>估计出的值的期望跟最后要的值不一样，就是有偏的</li>
<li>One special case, the expected value converges to the  correct value as infinite #samples are used — consistent</li>
<li>特殊情况：<ul>
<li>有偏：取多少样本，结果都是有偏的</li>
<li>在极限的定义下，取无限多的样本，得到的期望值会收敛到正确值</li>
<li>这种情况叫 consistent，也是有偏的</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>对于进一步有偏和无偏的理解（biased &#x3D;&#x3D; blurry），在下面的光子映射方法中引出</p>
<h5 id="方法一-Bidirectional-path-tracing-BDPT"><a href="#方法一-Bidirectional-path-tracing-BDPT" class="headerlink" title="方法一 Bidirectional path tracing (BDPT)"></a>方法一 Bidirectional path tracing (BDPT)</h5><p><em>无偏方法之双向路径追踪</em></p>
<ul>
<li>原来的光线追踪，利用光路的可逆性，从相机发射光线、判断跟物体相交，连接光源判断可见性</li>
<li>BDPT：<ul>
<li>从光源、摄像机生成两个子路径</li>
<li>把半路径的端点连起来，形成整条路径</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/275.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>如图，光线打到墙壁全部进行漫反射。在这种情况下，使用仅从摄像机发出光线的单项路径追踪就难以计算<ul>
<li>BDPT 实现很难，运行慢</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/276.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="方法二-Metropolis-light-transport-MLT"><a href="#方法二-Metropolis-light-transport-MLT" class="headerlink" title="方法二 Metropolis light transport (MLT)"></a>方法二 Metropolis light transport (MLT)</h5><p><em>无偏方法之 MLT</em></p>
<ul>
<li>A Markov Chain Monte Carlo (MCMC) application<ul>
<li>蒙特卡洛积分：在区间上均等概率采样，两次采样之间没有关系<ul>
<li>可以用任意 PDF 采样一个函数，不知道用什么样的 PDF 是最合适的</li>
<li>实际上，<u>用跟被积函数形状一致的采样函数 PDF，近似效果更好</u></li>
</ul>
</li>
<li>用马尔科夫链（统计学上的采样工具）：当前有一个样本，马尔科夫链可以生成跟这个样本靠近的下一个样本<ul>
<li>蒙特卡洛积分中，可以用任意函数的形状的 PDF 采样</li>
<li>因此可以通过马尔科夫链采样 PDF，让 PDF 跟被积函数形状一致</li>
</ul>
</li>
</ul>
</li>
<li>是一个局部的方法，给定任何路径，可以生成相似的路径<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/277.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>优点：特别适合做复杂光路的传播<ul>
<li>找到一条光路作为 seed，可以在它周围不断找到更多</li>
<li>图中右侧是游泳池类似的现象，SDS-pass（specular - diffuse - specular）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/278.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>缺点：<ul>
<li>很难在理论上分析收敛速度，不知道何时停止（path tracing 是可以的，蒙特卡洛方法分析 variance）</li>
<li>所有操作都是局部的，相邻像素之间没有关系、收敛速度不同，导致图片看上去“脏”</li>
<li>不会作为渲染动画的方法，只渲染场景或单张图片（两帧之间像素收敛可能不统一）</li>
</ul>
</li>
</ul>
<h5 id="方法三-Photon-Mapping"><a href="#方法三-Photon-Mapping" class="headerlink" title="方法三 Photon Mapping"></a>方法三 Photon Mapping</h5><p><em>有偏方法之光子映射</em></p>
<ul>
<li><p>优点</p>
<ul>
<li>特别适合渲染 caustics（光线聚焦产生的图案）</li>
<li>适合困难的光线如 SDS</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/280.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>做法：两步</p>
<ul>
<li>第一步——photon tracing<ul>
<li>从光源出发，向外辐射光子（光线），碰到物体就反射、折射，直到光子打到 Diffuse 的物体上，光子就停在那里</li>
<li>所有光子打完后，记录停留的位置</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/281.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>第二步——photon collection (final gathering)<ul>
<li>从摄像机出发，打出若干 sub path，反射、直射，直到打到 Diffuse 的物体上</li>
<li>这样，光源光线（光子）和摄像机的光线都停留在物体表面上</li>
<li>计算 local density estimation<ul>
<li>对任何着色点，取它最近的 n 个光子（nearest neighbor 问题，把光子组织成加速结构来解）</li>
<li>计算这些光子占据的着色点周围的面积、计算光子的密度</li>
<li>光子越集中的地方越亮</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/282.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>在计算过程中，如果找周围很少的光子数量，噪声会更大；如果找的很多，结果会模糊。用这个现象，分析为什么光子映射是有偏的方法，“biased” 意味着什么</p>
<ul>
<li>从这个角度，光子映射可以理解为 variance（噪声）和 bias（模糊）之间的妥协</li>
<li>photon collection 中，找的是数量（附近 n 个）而不是范围。因此对于密度的估计是不对的：<ul>
<li>正确应该是微分来算 $\text dN&#x2F;\text dA$，需要取着色点周围无限小的区域</li>
<li>但光子映射过程中是用实际数量来算 $\Delta N&#x2F;\Delta A$</li>
</ul>
</li>
<li>从极限的角度，如果光源打出特别多的光子，那么相同数量的光子覆盖的范围更小。在极限情况下 $\Delta A\rightarrow \text dA$ </li>
<li>光子打出的数量无限多，就能得到正确的结果；光子打出的数量不够，结果会“糊”，也就是有 bias</li>
<li>因此，光子映射是有偏的方法，但是是一致（consistent）的方法</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/283.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>再看有偏和无偏</p>
<ul>
<li>Biased &#x3D;&#x3D; blurry，<strong>得到的结果只要有模糊，就是有偏的</strong> </li>
<li>Consistent 一致性意味着虽然有模糊，但<u>只要样本足够多、最终会收敛到不模糊的结果</u> </li>
<li>如果在 shading point 按固定小面积计算密度，得到的结果一定是有偏的、不一致的<ul>
<li>在这种情况下，如果光子数量增多，范围内密度会增大，$\Delta A$ 不会趋近于 $\text dA$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="方法四-Vertex-connection-and-merging-VCM"><a href="#方法四-Vertex-connection-and-merging-VCM" class="headerlink" title="方法四 Vertex connection and merging (VCM)"></a>方法四 Vertex connection and merging (VCM)</h5><p><em>有偏方法之 VCM</em></p>
<ul>
<li><p>双向路径追踪和光子映射的结合体，在电影中得到了广泛应用</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/284.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="方法五-Instant-radiosity-IR-VPL-x2F-many-light-methods"><a href="#方法五-Instant-radiosity-IR-VPL-x2F-many-light-methods" class="headerlink" title="方法五 Instant radiosity (IR, VPL &#x2F; many light methods)"></a>方法五 Instant radiosity (IR, VPL &#x2F; many light methods)</h5><p><em>其他光线传播方法之实时辐射度</em></p>
<ul>
<li><p>在之前的光线传播中，并不区分光线是反射而来、还是自己发出来。把它们都作为 radiance。</p>
</li>
<li><p>实时辐射度利用了这种思想：<u>已经被照亮的地方，都认为它们是光源，再用它们照亮其他地方</u>。</p>
<ul>
<li><p>从光源打出 light sub-paths ，停在哪里，这个地方就变成新的光源（VPL）</p>
</li>
<li><p>当看到某个点时，就用所有新的光源去照亮这个着色点</p>
</li>
<li><p>图中实际上是考虑了光线弹射两次，用直接光照的方法（都从光源发光）得到了间接光照的结果</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/285.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>除了 IR，类似的 many-light rendering 也是一个研究领域</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/286.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>对于 Light Transport，至今没有一个完美的方法。很多公司还在使用 Path Tracing。</p>
<h4 id="2-Advanced-Appearance-Modeling"><a href="#2-Advanced-Appearance-Modeling" class="headerlink" title="2 - Advanced Appearance Modeling"></a>2 - Advanced Appearance Modeling</h4><h5 id="总览-2"><a href="#总览-2" class="headerlink" title="总览"></a>总览</h5><p>Appearance &#x3D; Material &#x3D; BRDF</p>
<ul>
<li>Non-surface models<ul>
<li>Participating media</li>
<li>Hair &#x2F; fur &#x2F;  fiber (BCSDF)</li>
<li>Granular material</li>
</ul>
</li>
<li>Surface models<ul>
<li>Translucent material (BSSRDF)</li>
<li>Cloth</li>
<li>Detailed material (non-statistical BRDF)</li>
</ul>
</li>
<li>Procedural appearance</li>
</ul>
<p>是否是表面模型，取决于是否定义在物体表面上</p>
<h5 id="2-1-Non-surface-models"><a href="#2-1-Non-surface-models" class="headerlink" title="2.1 - Non-surface models"></a>2.1 - Non-surface models</h5><h6 id="模型一-Participating-media"><a href="#模型一-Participating-media" class="headerlink" title="模型一 Participating media"></a>模型一 Participating media</h6><p>参与介质 &#x2F; 散射介质定义在空间中（不只是一个表面），如雾、云。光线打进去，会被吸收、被散射。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/287.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>就像 Diffuse 表面会进行均匀散射，使用 Phase Function 来定义散射介质发生的散射</li>
<li><u>类比 BRDF 规定光线如何反射，Phase Function 规定了光线如何散射</u>。具体的函数不再提</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/288.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>对于散射介质的渲染</p>
<ul>
<li><p>假设光线身处其中，往某个方向走，能走多远决定了介质的吸收能力有多强，停下来后考虑把光线往哪个方向再次发射</p>
</li>
<li><p>由此，任何一个点都可能发生方向的改变</p>
</li>
<li><p>把散射点相连，找到一个 path，计算此路径对 shading 的贡献</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/289.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>被用在电影（离线）、游戏（实时）中渲染烟和雾：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/290.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>也不只是烟和雾。很多物体看上去是一个表面，实际上光线会进入其中并发生散射。比如展示的巧克力，或者手挡住闪光灯还能看到光线。光线会进入一个物体，可能发生比较少的散射。</p>
<h6 id="模型二-Hair-Appearance"><a href="#模型二-Hair-Appearance" class="headerlink" title="模型二 Hair Appearance"></a>模型二 Hair Appearance</h6><p>头发会梳成一个面，也会单根表现出一些性质。对于头发的高光，分有色高光和无色高光（白色的）。</p>
<ul>
<li>如图，头发整体有一个高光，同时每根头发也各不同</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/292.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>从研究头发开始，人们广泛采用一个简单的模型：<strong>Kajiya-Kay Model</strong> </p>
<ul>
<li>一根光线打到一个圆柱上，考虑每根头发会把光线<u>散射成一个圆锥</u>。约等于 Diffuse + Specular</li>
<li>使用这种模型渲染的头发看上去不太合理</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/293.png" style="zoom: 60%;" / loading="lazy"></li>
</ul>
<p>后面，使用一个复杂的模型：<strong>Marschner Model</strong> </p>
<ul>
<li>考虑光线打到圆柱，有一部分会被直接反射；也有一部分会穿进头发，发生折射，再穿出去</li>
<li>记反射是 R，穿进穿出一个表面是 T。光线穿过头发就是 TT，还有 TRT（穿透、内部反射、穿透回去）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/295.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>具体来说，头发表示为一个玻璃柱。分 cuticle（表皮）、cortex（内部），cortex 代表头发的颜色，模拟了光线进入头发、会被部分吸收的现象，黑发吸收得多，金发吸收得少</li>
<li>Marschner 模型模拟了三种圆柱的作用：R，TT，TRT</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/296.png" style="zoom: 67%;" / loading="lazy">   </li>
<li>头发有很多，光线会发生多次散射。头发的渲染是困难的</li>
</ul>
<p>此外，对于动物，使用人的头发的渲染方法，得不出真实的效果。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/298.png" style="zoom: 67%;" / loading="lazy">  
</li>
<li><p>在生物上，人和动物的毛发除了 cuticle、cortex，还有中间的 Medulla（髓质），髓质内部非常复杂，光线打到髓质会像进入散射介质一样、被打到四面八方。动物的髓质比人的髓质大很多，光线更容易发生散射</p>
</li>
<li><p>因此，Marschner Model 的玻璃柱模型忽略髓质是不好的。对于人类头发，加入髓质的模型也会提高渲染质量</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/297.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>加入髓质，定义 <strong>Double Cylinder Model</strong> </p>
<ul>
<li><p>在圆柱模型中间加一个圆柱，来模拟Medulla。光线打到Medulla会进行散射，记为s</p>
</li>
<li><p>同样，定义五种光线模型，具体如图</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/299.png" style="zoom: 67%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/300.png" style="zoom: 67%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/301.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h6 id="模型三-Granular-Material"><a href="#模型三-Granular-Material" class="headerlink" title="模型三 Granular Material"></a>模型三 Granular Material</h6><p>似颗粒状的，一堆小东西形成的模型，如沙子、香料等</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/302.png" style="zoom: 60%;" / loading="lazy"> </li>
<li>如果渲染每个颗粒，计算量巨大。可以简化整个模型，认为每一个单元上由很多石子构成，各部分成分有多少</li>
<li>Granular 模型的计算量依然巨大，至今没有比较好地应用</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/303.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="2-2-Surface-Models-表面模型"><a href="#2-2-Surface-Models-表面模型" class="headerlink" title="2.2 - Surface Models 表面模型"></a>2.2 - Surface Models 表面模型</h5><h6 id="模型一-Translucent-Material"><a href="#模型一-Translucent-Material" class="headerlink" title="模型一 Translucent Material"></a>模型一 Translucent Material</h6><p>以玉石、水母为代表的透光性材质（Semi-transparent 半透明，Translucent 透光性），光线在物体内部发生一些散射，然后再射出物体。</p>
<p>把这种现象称为：<strong>Subsurface Scattering 次表面散射</strong></p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/304.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>次表面散射可以看作 BRDF 的延申，定义 BSSRDF</p>
<ul>
<li>BRDF 所有的作用都发生在一个点上，定义光线打到一个点上、从这个点反射出去的能量；BSSRDF 光线进入一个点，可以从很多个地方出去</li>
<li><u>BSSRDF 不仅考虑任何点、接收任何方向的光照，而且要考虑不同点对于某个方向射出能量的贡献</u>。因此，不仅对半球积分，也对面积积分</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/305.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>这样计算很复杂，使用近似方法：<strong>Dipole Approximation</strong> </p>
<ul>
<li>用物体内部、外部两个光源照亮物体着色点区域，就很像次表面反射的结果</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/306.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>BSSRDF模拟了真实世界中，光线进入物体、在局部进行散射的现象，尤其是在渲染人的皮肤时效果很好</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/307.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h6 id="模型二-Cloth"><a href="#模型二-Cloth" class="headerlink" title="模型二 Cloth"></a>模型二 Cloth</h6><p>布料：由一系列缠绕的纤维构成，缠绕有不同的层级</p>
<ul>
<li>最基础的纤维（一根羊毛、化纤）经过缠绕，可以得到不同的股 Ply</li>
<li>不同的股再缠绕，会形成线 Yarn</li>
<li>有了线，可以做成布（编织和纺织）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/308.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>对于这类布的渲染，光线肯定跟纤维的缠绕方式有关。</p>
<p>第一种方法，如果认为布料就是<strong>物体表面</strong>，对于布料的渲染，通过给定任意的编制图案，计算出最终的光照结果（一个 BRDF 的关系） 。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/309.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>然而，（前面的各向异性提过）对于天鹅绒等所有毛发向外的布料，正常情况下不是一个平面，不能用 BRDF 来表示。</p>
<p>因此，另一种方法是：<strong>认为织物是空间中分布的体积，当作反射介质来做</strong> </p>
<ul>
<li>把它划分成细小的格子，每个格子内知道纤维的分布，能计算光线的吸收和散射</li>
<li>这样<u>把渲染布料（面），转化成渲染云、烟、雾等反射介质（体积）的问题</u> </li>
<li>计算量非常大，但结果更好</li>
</ul>
<p>还有一种方法：布料是由一根根的<strong>实际纤维</strong>组成的，直接暴力渲染每一根纤维（当作头发）</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/311.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>直到今天，散射介质、实际纤维、物体表面，这三种布料渲染的方法都有人使用。</p>
<h6 id="模型三-Detailed-Appearance"><a href="#模型三-Detailed-Appearance" class="headerlink" title="模型三 Detailed Appearance"></a>模型三 Detailed Appearance</h6><p>动机：在渲染时，往往得到过于完美的结果，带来不真实感。在生活中有各种各样的细节，比如划痕等。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/313.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>回顾：微表面模型</p>
<ul>
<li><p>微表面模型最重要的是微表面的法线分布，在描述分布时用比较简单的模型（如正态分布），导致看上去结果缺少细节</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/314.png" style="zoom: 67%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/315.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>可以通过不同方法（高度贴图，加亮片）定义细节，但渲染它们非常困难，因为微表面是镜面，从摄像机 &#x2F; 光源射出的光线，经过镜面反射，很难打到光源&#x2F;摄像机上去，渲染高光需要很长时间</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/316.png" style="zoom: 67%;" / loading="lazy"> 
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/317.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>在这里，考虑一个像素会覆盖很多的微表面，如果能算出一个小范围内的微表面法线分布，就能替代光滑的分布，并用在微表面模型里</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/318.png" style="zoom: 67%;" / loading="lazy"> 
</li>
<li><p>考虑覆盖的范围，会得到各种不同的法线分布（NDF）。如果像素覆盖的微表面多，就会显示出一定的统计规律；如果覆盖的少，可能会有更独特的性质</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/319.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><p>不同类型的法线贴图，也会引起不同的法线分布。刷过的平面会得到各向异性的法线分布；离散的亮片会得到很多离散的点</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/320.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><p>当物体小到一定程度（跟光波大小相似），就不能再使用几何光学进行分析了，而必须考虑对光波的衍射、干涉等现象进行模拟</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/321.png" style="zoom: 67%;" / loading="lazy"> 
</li>
<li><p>波动光学的 BRDF 跟几何光学的 BRDF 相似，但多了不连续的特点。因为光会发生干涉，引起一些地方加强、一些地方减弱，形成不连续的效果</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/322.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="2-3-Procedural-appearance-程序化生成"><a href="#2-3-Procedural-appearance-程序化生成" class="headerlink" title="2.3 - Procedural appearance 程序化生成"></a>2.3 - Procedural appearance 程序化生成</h5><p>程序化生成：用一定的方式指导模型 &#x2F; 表面 &#x2F; 材质的生成，并且不需要真正生成，可以动态进行查询。定义一个三维的纹理（可以看到物体内部长什么样子），但不需要进行存储，什么时候用到、什么时候查询。有一个空间中的函数$f(x,y,z)$，给定坐标、返回该点的纹理值。</p>
<p>这种函数叫做 noise function。可以对函数进行一系列处理，以得到任意不同的效果。</p>
<ul>
<li>比如：设置一个阈值进行二值化，生成车子上的锈迹</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/323.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>噪声函数非常有用，比如生成复杂的地形、生成水面等等、生成木头的纹理等等。</p>
<p>再次提醒：并不是真正地生成，而是一个支持三维查询的函数。</p>
<p>一个专门做程序化材质的软件：houdini，在工业界被广泛地使用，它的程序化材质是真的生成。</p>
<hr>
<h3 id="Lecture-19-Cameras-Lenses-and-Light-Fields"><a href="#Lecture-19-Cameras-Lenses-and-Light-Fields" class="headerlink" title="Lecture 19 - Cameras, Lenses and Light Fields"></a>Lecture 19 - Cameras, Lenses and Light Fields</h3><p>19、20两节课是相对独立的话题，了解图形学背后的一些简单知识：相机，透镜，光场；颜色，感知</p>
<h4 id="1-相机的曝光度（快门和光圈）"><a href="#1-相机的曝光度（快门和光圈）" class="headerlink" title="1 - 相机的曝光度（快门和光圈）"></a>1 - 相机的曝光度（快门和光圈）</h4><h5 id="成像"><a href="#成像" class="headerlink" title="成像"></a>成像</h5><p>成像方法：Imaging &#x3D; Synthesis + Capture</p>
<ul>
<li>图形学的两种成像方法：光栅化和光线追踪。这两种其实都是合成（Synthesis）的方法：物体在世界中并不存在，成像过程是合成一些不存在的东西</li>
<li>另一种方法是通过捕捉（Capture）方法来成像：真实世界中已经存在一些东西，把它们变成照片，这就是成像。一种简单的方法是用相机拍照</li>
<li>此外，也有针对其他 Imaging 方法的研究（计算摄影学），考虑其他的成像方法</li>
</ul>
<p>相机的成像</p>
<ul>
<li>小孔相机和透镜相机<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/325.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>针孔相机没有“景深”效果<ul>
<li>针孔相机没有深度，每个地方拍出来都是锐利的</li>
<li>做光线追踪时也是用的针孔相机的模型</li>
</ul>
</li>
<li>透镜相机带有深度，模拟其成像过程可以做出带有景深的渲染</li>
</ul>
</li>
<li>Shutter 快门：控制光能否进入机身，控制光在 1&#x2F;n 秒内进入相机</li>
<li>Sensor 传感器：把进入相机的光捕捉下来，上面的任何一个点记录 <strong>irradiance</strong> <ul>
<li>不能不通过快门直接用传感器接收光，因为如果没有快门，传感器区分不出方向，传感器上的每个点都会接收来自不同方向的光（只能收集 irradiance，不能收集 radiance）</li>
<li>也有人在研究能区分方向的传感器</li>
</ul>
</li>
</ul>
<h5 id="概念1：Field-of-view（FOV）视场"><a href="#概念1：Field-of-view（FOV）视场" class="headerlink" title="概念1：Field of view（FOV）视场"></a>概念1：Field of view（FOV）视场</h5><p>FOV 视场</p>
<ul>
<li>视野角度大小，“广角”相机就是 FOV 大</li>
<li>Focal Length（焦距）可以决定 FOV 的大小<ul>
<li>定义：sensor 的高度 $h$，焦距 $f$，$\text {FOV}&#x3D;2\arctan(\frac{h}{2f})$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/324.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>定义 FOV 和焦距<ul>
<li>由于 FOV 跟传感器大小、焦距都有关，当定义 FOV 时，通常认为以 35mm 的胶片为传感器大小，再给出焦距的长度，以此定义 FOV 。比如单反镜头的 xx mm 说的就是焦距</li>
<li>而对于手机，给出的是等效焦距长度。手机本身很薄、胶片也更小。也就是同时缩小了 $h,f$，可以保持大的 FOV</li>
</ul>
</li>
<li>同样的胶片，焦距越长，视场越小。焦距越长，看到的地方更远，视野更小<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/326.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>目前可以混淆传感器和胶片的概念。在后面的渲染中，sensor 用来记录每个像素的 irradiance，film 决定最后存储成什么样的格式<ul>
<li>传感器也有不同的大小，同样能决定分辨率、视场大小</li>
</ul>
</li>
</ul>
<h5 id="概念2：Exposure-曝光度，光圈和快门"><a href="#概念2：Exposure-曝光度，光圈和快门" class="headerlink" title="概念2：Exposure 曝光度，光圈和快门"></a>概念2：Exposure 曝光度，光圈和快门</h5><ul>
<li>$H &#x3D; T\times E$，Exposure &#x3D; time × irradiance<ul>
<li>理解：<ul>
<li>在辐射度量学中，考虑的都是单位时间：单位时间光打到一个表面、单位时间光又辐射出去。irradiance 是单位时间进来多少光</li>
<li>而对于现实照相，考虑累计的时间，时间段内的所有光都进入相机、被传感器捕捉<ul>
<li>对着一个明亮的场景，拍照得出的结果就亮</li>
<li>对着一个暗的场景，快门按下的时间长（曝光时间长），也可以得到亮的照片</li>
</ul>
</li>
</ul>
</li>
<li>对应到相机中<ul>
<li>T：按下快门的时间</li>
<li>E（irradiance）：光打到 sensor 上的一个单元的多少<ul>
<li>由光圈大小 aperture、焦距 focal length 决定</li>
<li>光圈是一个仿生学的设计，仿照人的瞳孔，控制单位时间内接收光的多少。在暗处瞳孔放大，让单位时间看到更多光</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>曝光度可以度量为照片亮的程度。相机中影响成片亮不亮的因素：<ul>
<li>Aperture size：光圈的大小，f-stop 参数可以控制光圈的大小</li>
<li>Shutter speed：快门开放的时间</li>
<li>ISO gain（ISO增益，感光度）：一个后处理，给感光的结果乘上一个数</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/327.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>观察图中的参数变化对应照片变化<ul>
<li>f-stop 越大，光圈越小，图像越锐利。光圈越大，在一定范围内图像更虚</li>
<li>shutter speed 开放时间越长，捕捉到的运动信息越多</li>
<li>ISO 乘上的数越大，在放大信号的同时也放大了噪声</li>
</ul>
</li>
</ul>
</li>
<li>再看快门和光圈<ul>
<li>光圈的 f-number（f-stop）就是 Exposure Levels<ul>
<li>调节光圈就调节了<u>固定时间内进入相机的能量</u>，也就是直接调节了曝光度</li>
<li>非正式的理解：f 数是光圈的直径的倒数</li>
</ul>
</li>
<li>机械快门有一个快门打开的过程<ul>
<li>如果过程中物体产生了运动，就发生<strong>运动模糊</strong>。时间段内所有的信息都被传感器记录下来并取平均。这也是手抖拍照模糊的原因<ul>
<li>减小快门时间，运动模糊的程度就降低了，但为了维持亮度，要调大光圈或用大 ISO</li>
<li>运动模糊也不一定是坏事！<ul>
<li>在人的感知方面，运动模糊可以体现物体的动态感</li>
<li>以采样的角度考虑：对于光栅化的采样，如果采样率不够、即两个像素之间距离远，就会产生走样（锯齿），需要进行反走样，即先模糊、再采样。而运动模糊相当于在对<strong>时间</strong>的采样中做了反走样</li>
</ul>
</li>
</ul>
</li>
<li>快门打开的过程同样造成 <strong>Rolling shutter</strong> 问题，光进入相机的时间不一样，导致照片中不同位置记录的是不同时间进入的光。视觉上，极高速运动中的物体会产生扭曲的效果</li>
</ul>
</li>
<li>需要进行光圈和快门的权衡<ul>
<li>如 f-stop 从4到8，对应光圈面积变为1&#x2F;4，那么需要快门时间变成4倍</li>
<li>对于高速摄影，要求快门时间小，那么光圈面积就要调大（f-stop 调小）</li>
<li>对于延时摄影，用小面积的光圈长时间曝光</li>
<li>在摄像时，不能兼顾景深和运动模糊两个效果</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/328.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2-相机的镜头（透镜）"><a href="#2-相机的镜头（透镜）" class="headerlink" title="2 - 相机的镜头（透镜）"></a>2 - 相机的镜头（透镜）</h4><h5 id="Thin-Lens-Approximation"><a href="#Thin-Lens-Approximation" class="headerlink" title="Thin Lens Approximation"></a>Thin Lens Approximation</h5><p>真正相机的镜头是很多复杂透镜的组合，我们研究理想的薄透镜作为近似。</p>
<ul>
<li><u>平行的光线经过透镜后汇聚到一点</u>，称为 Focal Point 焦点，透镜到焦点的水平距离称为焦距。</li>
<li>光线是可逆的，<u>过焦点的光通过透镜后变成平行光</u> </li>
<li><u>任意一条光线，如果过透镜的中心，它的方向不会发生改变</u> </li>
<li>假设薄透镜的焦距可以动态任意更改（模拟现实相机的透镜组）<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/329.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h6 id="薄透镜方程"><a href="#薄透镜方程" class="headerlink" title="薄透镜方程"></a>薄透镜方程</h6><div>$$\frac{1}{f} = \frac{1}{z_i} + \frac{1}{z_0}$$</div>

<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/330.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>物距 $z_0$：物体到透镜的垂直距离，相距 $z_i$ ：成像到透镜的垂直距离，焦距 $f$ </li>
<li>$\frac{1}{f} &#x3D; \frac{1}{z_i} + \frac{1}{z_0}$，展示了焦距、物距、相距的关系</li>
<li>计算方法：根据两条性质①过焦点的光线经过透镜变平行，②平行光线经过透镜过焦点，构造两组相似三角形<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/331.png" style="zoom: 50%;" / loading="lazy"></li>
</ul>
</li>
<li>由这个方程，物体的物距改变了，相距也要随之改变</li>
</ul>
<p>通过薄透镜，就可以解释很多的问题。</p>
<h5 id="Defocus-Blur"><a href="#Defocus-Blur" class="headerlink" title="Defocus Blur"></a>Defocus Blur</h5><h6 id="Circle-of-Confusion（CoC）"><a href="#Circle-of-Confusion（CoC）" class="headerlink" title="Circle of Confusion（CoC）"></a>Circle of Confusion（CoC）</h6><ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/332.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>如图，在 Focal Plane 平面上的点，经过透镜聚焦在 Sensor Plane 上（看 $z_s’$ 和 $z_s$）</li>
<li>而如果物体不在 Focal Plane 平面上，而在 Object 处的一个点，经过透镜聚焦在 Image 上（$z_0$ 和 $z_i$） ，光线继续传播到 sensor 就会变成一个圆</li>
<li>在这个圆内，分不出来是从哪里传来的光线，称为 Circle of Confusion</li>
<li>构造相似三角形也能求得 CoC 的公式，如上图。发现 CoC 的长度 $C$ 跟透镜的长度 $A$ 呈正比<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/333.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>看到的东西是否模糊取决于光圈的大小，<strong>光圈越大越模糊</strong> </li>
<li>光圈为 f&#x2F;1.4 （大光圈），看到的效果更模糊；f&#x2F;22 的小光圈的图片更清晰</li>
<li>这也是在第一节中展示的光圈大带来景深效果的原理</li>
</ul>
</li>
<li><u>CoC是景深、近视等现象的成因</u></li>
</ul>
<p>在此，重新更加准确地定义 f-stop：</p>
<ul>
<li>f 数是焦距除以光圈的直径，$N&#x3D;f&#x2F;A$ </li>
<li>f 数通常写作 f&#x2F;2，反应绝对光圈直径 (A) 可以通过将焦距 (f) 除以相对光圈 (N，f数) 来计算</li>
</ul>
<p>因此：</p>
<div>$$C=A\frac{|z_s-z_i|}{z_i}=\frac{f}{N}\frac{|z_s-z_i|}{z_i}$$</div>

<ul>
<li>COC除了跟透镜大小（光圈大小） $A$ 呈正比，也理解为跟 f 数 $N$ 呈反比，总之要拍摄清晰的图片就选择小光圈</li>
</ul>
<h5 id="Ray-Tracing-Ideal-Thin-Lenses"><a href="#Ray-Tracing-Ideal-Thin-Lenses" class="headerlink" title="Ray Tracing Ideal Thin Lenses"></a>Ray Tracing Ideal Thin Lenses</h5><p>之前的 path tracing 产生光线的方法是：从相机往每个像素的采样点发出光线，这是小孔成像的模型。</p>
<p>现在，已知了光线穿过薄透镜的行为，因此可以用薄透镜来计算光线追踪的过程、渲染成像。</p>
<ul>
<li>定义 sensor，即成像平面的大小；定义透镜的属性，即焦距和大小</li>
<li>对于某个想要成像的物体（平面），定义透镜摆放的位置（物距 $z_0$）<ul>
<li>由透镜公式，已知物距、焦距，计算相距 $z_i$</li>
</ul>
</li>
<li>Ray Tracing 的过程：<ul>
<li>希望找一些光线能够穿过透镜打到场景中</li>
<li>由于对于 sensor 的每个点 $x’$，穿过透镜中心点光线一定打到 $x’’’$ 处</li>
<li>在透镜选一个点 $x’’$，就知道从 $x’$ 穿过 $x’’$ 打到 $x’’’$ 的方向</li>
<li>只需考虑 $x’’\rightarrow x’’’$ 这条光线上的 radiance，计算出来后记录到 $x’$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/334.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="Depth-of-Field"><a href="#Depth-of-Field" class="headerlink" title="Depth of Field"></a>Depth of Field</h5><p>用 Defocus blur 正式定义景深的概念：</p>
<ul>
<li>用大光圈，CoC 的面积就大，图片更加模糊；但在刚好对焦的平面附近不会出现 CoC 现象，不会模糊。大、小光圈会影响产生模糊的（深度）范围</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/335.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>在对焦附近的区域，CoC 足够小，看上去是清晰的，就是景深<ul>
<li>光经过透镜，打到成像平面上。在成像平面附近的一段区域内，都认为 CoC 是足够小的</li>
<li>景深：在实际场景中有一段深度，经过投影后会在成像平面附近形成一段区域，这段区域内 CoC 足够小。计算景深就是在成像平面附近范围内，对应到场景中的深度范围</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/336.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>可以在网站上尝试：<a href="http://graphics.stanford.edu/courses/cs178/applets/dof.html">Depth of field (stanford.edu)</a></li>
</ul>
<h4 id="3-Light-Field-x2F-Lumigraph-光场"><a href="#3-Light-Field-x2F-Lumigraph-光场" class="headerlink" title="3 - Light Field &#x2F; Lumigraph 光场"></a>3 - Light Field &#x2F; Lumigraph 光场</h4><h5 id="全光函数"><a href="#全光函数" class="headerlink" title="全光函数"></a>全光函数</h5><p>定义人如何看到世界：全光函数</p>
<ul>
<li>对于人来说，看到的都是来自各方向的光，而不在意深度信息。因此，可以用一个 2D 的屏幕记录来自各方向的光，如果人看向这个屏幕，会感到自己在看向 3D 的场景。这就是虚拟现实的原理。</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/337.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>可以用一个函数来描述<u>人眼所能看到的所有东西</u>：<strong>全光函数（The Plenoptic Function）</strong> </p>
<p>假设一个人站在一个场景里，往任何方向去看。</p>
<ul>
<li>用极坐标定义一个方向 $\theta,\phi$ ，定义在这个方向看到的值为 $P(\theta, \phi)$ </li>
<li>引入波长（各种颜色）的概念 $\lambda$，可以看到彩色的世界 $P(\theta, \phi,\lambda)$ </li>
<li>扩展到时间，成为电影 $P(\theta, \phi,\lambda,t)$ </li>
<li>如果人的位置可以在三维空间中移动，看到的是全息电影 $P(\theta, \phi,\lambda,t,V_X,V_Y,V_Z)$ </li>
<li>最后，不把函数当作电影，而是理解成：在任何位置、任何时间，往任何方向看，看到不同的颜色，这就是我们能看到的所有东西——整个世界就衡量为一个 7D 的函数，称为全光函数</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/338.png"  / loading="lazy"></li>
</ul>
<h5 id="从全光函数定义光场"><a href="#从全光函数定义光场" class="headerlink" title="从全光函数定义光场"></a>从全光函数定义光场</h5><h6 id="光场是全光函数的一部分"><a href="#光场是全光函数的一部分" class="headerlink" title="光场是全光函数的一部分"></a>光场是全光函数的一部分</h6><ul>
<li>光线 Ray<ul>
<li>5D 的定义<ul>
<li>3D 表示空间中一点，2D 表示极坐标一个方向</li>
<li>$P(\theta, \phi, V_X,V_Y,V_Z)$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/339.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>4D 的定义<ul>
<li>两个点定义一条光线，2D 位置 + 2D 方向</li>
<li>为什么是 2D 位置在接下来的光场定义中给出</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/340.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="光场是什么"><a href="#光场是什么" class="headerlink" title="光场是什么"></a>光场是什么</h6><ul>
<li>当看向一个物体时<ul>
<li>对于一个物体，会从任意位置、任意方向看向它</li>
<li>由于光线是可逆的。也就可以反过来，<u>描述物体能被看到的所有情况</u>：描述物体在包围盒上的任何位置，往任何方向的发光情况</li>
<li>由此，当看向物体时，可以从对应包围盒上点的对应方向进行查询。</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/341.png" style="zoom: 80%;" / loading="lazy"></li>
</ul>
</li>
<li>这个记录发光情况的结构就是<strong>光场</strong> </li>
<li>光场：在任意位置，往任意方向发出光的强度。<u>2D 的位置，2D 的方向</u>。<ul>
<li>位置：3D 物体的表面是在 2D 空间中的（如 UV 纹理映射，这里的包围盒也是一个贴图）</li>
<li>方向：空间中的方向用 $\theta,\phi$ 表示</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/342.png" style="zoom: 80%;" / loading="lazy">  </li>
<li>如图紫色连线，在<u>任何一个位置</u>放置摄像机，通过查询光场，都能得出：看向物体的任意一个点，看到的光的情况</li>
<li>光场的好处就是：可以得出任意一个方向的观测结果</li>
</ul>
</li>
</ul>
<p>光场概念的一些理解</p>
<ul>
<li>正如人眼不关心看到的是 3D 空间还是 2D 屏幕，光场可以定义在物体表面，也可以定义在物体外的一个黑盒上，只要定义黑盒表面任意一个位置的光线情况，就可以记录它的光场<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/343.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>推广这个理解，取一个平面，右边是一些发光物体、发出光穿过平面。对于光场只需要知道平面左边的情况，而对右边的物体不关心<ul>
<li>知道平面上的任意一点 $S$、知道一个方向 $\theta,\phi$ 即可定义任何一条光线<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/344.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li>进而，对于任何一个光场，都可以用两个平面来定义<ul>
<li><u>分别取两个平面上的一个点，就能定义任何一条光线</u> <ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/345.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>对光场进行参数化：<ul>
<li>$u,v$、$s,t$ 表示两个点，找到所有两个点的组合、记录光线的情况，就是光场的参数化方法</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/346.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="对于“两个平面”光场参数化方式的不同理解"><a href="#对于“两个平面”光场参数化方式的不同理解" class="headerlink" title="对于“两个平面”光场参数化方式的不同理解"></a>对于“两个平面”光场参数化方式的不同理解</h6><p>两种理解方式</p>
<ul>
<li><p>按照如图方式放置两个平面，整个世界在 $s,t$ 平面的右边</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/347.png" style="zoom: 67%;" / loading="lazy"> 
</li>
<li><p>理解一</p>
<ul>
<li>固定在 $u,v$ 上取一个点，看向 $s,t$ 平面上所有的点，<u>会看到一个完整的物体图像</u> </li>
<li>整个 $s,t$ 平面就是完整的物体</li>
<li>因为 $s,t$ 平面右边就是世界（不关心它具体是什么），相当于小孔成像</li>
<li>“摄像机阵列”：有很多摄像机，分别从某个角度看向世界、拍一张图，组合起来成为光场<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/348.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><p>理解二</p>
<ul>
<li>固定在 $s,t$ 上取一个点，看向 $u,v$ 平面上所有的点</li>
<li>反过来考虑，从 $u,v$ 平面上任何一个点都看向 $s,t$ 上的同一个点，可以看到<u>一个物体不同方向的一个像素</u> </li>
<li>光场中， $s,t$ 上的一个点就是物体的一个像素。从不同的方向看向这个像素</li>
<li>可以理解为：<u>物体一个点上的 irradiance，通过这种方式拆成了 radiance</u>。可以看到物体的任意一个像素，打到不同方向上的光到底是什么</li>
<li>自然界中一些生物的复眼成像原理就是在呈现一个光场<ul>
<li>平时拍出的照片，每个像素记录的是 irradiance：不区分光来自的方向，而进行平均</li>
<li>如果用小的透镜，把来自各个方向的光分到不同位置上，就可以把原本打到一个像素上的光分到不同的 sensor 上记录，如图把红、绿、蓝光分别进行记录；看向一个像素时，其实是看向穿过像素的、不同方向的光</li>
<li>本质上，这就是记录了 radiance 而非 irradiance，这就是<strong>光场摄像机</strong>的基本原理：<u>使用一个透镜，把来自不同方向的光分开，分别记录下来</u> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/349.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="4-Light-Field-Camera-光场摄像机"><a href="#4-Light-Field-Camera-光场摄像机" class="headerlink" title="4 - Light Field Camera 光场摄像机"></a>4 - Light Field Camera 光场摄像机</h4><p>光场摄像机：</p>
<ul>
<li>一个功能：可以先拍照，然后后期动态地调整焦距、光圈大小等<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/350.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>原理：把像素换成一个微透镜，把来自不同方向的光分开，再在后方的sensor上分别记录下来（把感光元件往后挪）</li>
<li>原本的一个像素，变成在光场照相机内记录一块像素，也就是 irradiance 被拆分成了 radiance<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/351.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>如果还原成一个普通的照片，就在每一个透镜都选择一条光线、记录在对应每个像素的结果上，就得到一张原始的照片。这就相当于把摄像机放到某个方向来拍摄<ul>
<li>有了光场，就可以虚拟地移动拍摄的位置了。光场摄像机可以移动摄像机的位置</li>
<li>重新聚焦也是相似的道理：由于拥有整个光场，可以按需查找需要改变的光线，计算应该取哪些方向</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/352.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>总结起来，光场摄像机的种种功能，来自于其记录了光线的所有信息，包括位置、方向</li>
<li>此外，光场摄像机的一些不足：<ul>
<li>对胶片分辨率要求高：原本一个像素的信息，现在需要用更多的像素来记录（原来的位置分辨率×方向分辨率）</li>
<li>高成本：分辨率问题，微透镜的实现问题</li>
<li>trade-off：更精密的方向信息 or 位置信息</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/353.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<hr>
<h3 id="Lecture-20-Color-and-Perception"><a href="#Lecture-20-Color-and-Perception" class="headerlink" title="Lecture 20 - Color and Perception"></a>Lecture 20 - Color and Perception</h3><ul>
<li>what is color</li>
<li>color perception</li>
<li>color reproduction &#x2F; matching </li>
<li>color space</li>
</ul>
<h4 id="1-What-is-color"><a href="#1-What-is-color" class="headerlink" title="1 - What is color"></a>1 - What is color</h4><h5 id="Physical-Basis-of-Color"><a href="#Physical-Basis-of-Color" class="headerlink" title="Physical Basis of Color"></a>Physical Basis of Color</h5><p>Spectrum 光谱</p>
<ul>
<li>颜色是一个混合出来的结果：用棱镜可以把光线折射、分解成不同的颜色，将它们混合起来又形成原来的光线</li>
<li>原因是不同的波长对应不同的折射率，而任何一种光都对应一种光谱，光谱就是光线的能量在不同波长上的分布</li>
<li>图形学关注可见光的光谱范围：400-700nm。</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/354.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<p>Spectral Power Distribution (SPD) 谱功率密度</p>
<ul>
<li>SPD 衡量了光线在不同的波长的强度是多少</li>
<li>通过 SPD，可以描述各种光在各个波长上的分布，不同的光对应不同的SPD<ul>
<li>对于蓝天，集中在小波长、高频区域，看上去是蓝色；对于阳光看上去又是另外的样子</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/355.png" style="zoom: 67%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/356.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>SPD 具有线性性质：用不同的光照射同一个物体，分开的 SPD 加起来就是共同照射时的 SPD。理解为两个灯一起开会更亮<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/357.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>从光谱到颜色：<strong>颜色是人的感知</strong>，而不是物理上的概念</p>
<h4 id="2-Color-Perception"><a href="#2-Color-Perception" class="headerlink" title="2 - Color Perception"></a>2 - Color Perception</h4><h5 id="Biological-Basis-of-Color"><a href="#Biological-Basis-of-Color" class="headerlink" title="Biological Basis of Color"></a>Biological Basis of Color</h5><p>人的眼睛就是一个摄像机</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/358.png" style="zoom: 50%;" / loading="lazy"> </li>
<li>对应各个部位<ul>
<li>瞳孔：光圈；晶状体：透镜；视网膜：sensor</li>
<li>在视网膜上，有感光细胞<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/359.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>棒状细胞感知光的强度，用它得到一个灰度图</li>
<li>锥形细胞比较少，可以感知颜色<ul>
<li>锥形细胞内部又分为三种不同的锥形细胞：S、M、L 型细胞，分别更擅长感知高、中、低波长对应的光</li>
<li>每个人体内三种锥形细胞的分布也大不相同，每个人对颜色的感知不一样</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Tristimulus-Theory-of-Color"><a href="#Tristimulus-Theory-of-Color" class="headerlink" title="Tristimulus Theory of Color"></a>Tristimulus Theory of Color</h5><p>目前，知道了不同的光在波长上的分布 SPD，又知道了每一种细胞对于不同波长的响应方式，就可以计算感知的结果了。用响应曲线跟 SPD 做波长上的积分即可。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/360.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>对于不同的SPD，人们看到了三个数：S、M、L</li>
</ul>
<p>人们看不到光线、光谱，只看到积分出来的结果也就是三个数</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/361.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h5 id="Metamerism-同色异谱"><a href="#Metamerism-同色异谱" class="headerlink" title="Metamerism 同色异谱"></a>Metamerism 同色异谱</h5><p>由于人只看到结果，会不会出现：两个光线、光谱不相同，但积分的结果相同，看上去是一样的？这种现象就叫做同色异谱现象。</p>
<p>正是应用了这种现象，才能给人们呈现各种各样的颜色。</p>
<ul>
<li>假如拍了一张照片，并希望能在电视上显示出来，就进行光谱的调节，使得在电视上和自然界中，人看上去是一样的颜色</li>
<li>通过调和光谱，得到某一种颜色，使得这种颜色跟另外一种颜色看上去一样，这叫做 color matching 颜色匹配过程。不需要光谱一样，只需要最后看到的颜色一样</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/362.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>接下来就是如何进行 color matching</li>
</ul>
<h4 id="3-Color-Reproduction-x2F-Matching"><a href="#3-Color-Reproduction-x2F-Matching" class="headerlink" title="3 - Color Reproduction &#x2F; Matching"></a>3 - Color Reproduction &#x2F; Matching</h4><h5 id="Additive-Color-加色系统"><a href="#Additive-Color-加色系统" class="headerlink" title="Additive Color 加色系统"></a>Additive Color 加色系统</h5><h6 id="颜色的混合"><a href="#颜色的混合" class="headerlink" title="颜色的混合"></a>颜色的混合</h6><ul>
<li>有几种不同的颜色，以 RGB 为例，各自有不同的 SPD</li>
<li>把 RBG 各自乘上某个强度，然后混合（相加）</li>
<li>最终得到一种颜色，用各自混合的强度系数来表示这个颜色</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/363.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h6 id="计算机中的加色系统"><a href="#计算机中的加色系统" class="headerlink" title="计算机中的加色系统"></a>计算机中的加色系统</h6><ul>
<li>在计算机中，把 R、G、B 的强度都调成255，会得到白色。真实世界中也是如此</li>
<li>画画中，调和不同的颜料会越调越黑，是减色系统</li>
<li>加色系统可以实现：<u>通过基本颜色的线性组合，匹配各种不同的颜色</u> <ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/364.png" style="zoom: 67%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/365.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>也有的颜色不能匹配出来（因为任何一个基本颜色的强度都不能减到0以下），这时由线性性质，给待匹配的颜色加上某个值，相当于在匹配中减去某个值，依然是一个线性的组合<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/366.png" style="zoom: 80%;" / loading="lazy"> </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/367.png" style="zoom: 80%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h6 id="CIE-RGB-系统"><a href="#CIE-RGB-系统" class="headerlink" title="CIE RGB 系统"></a>CIE RGB 系统</h6><ul>
<li>给定3种 SPD 是单波长的颜色，作为基本颜色，给定单波长需要混合的颜色。考虑如何用这三种单波长颜色，混合出某一个波长上对应的颜色<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/368.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>对每一个给定波长进行试验，得出每个波长上对应的三种单波长基本颜色的线性组合系数，绘制匹配函数图像</li>
<li>这样，知道了每个<u>单波长</u>的颜色混合方式</li>
<li>对于真实情况的<u>多波长</u>分布 SPD：<ul>
<li>给定任何一个实际的 SPD，只需分别对于每个单波长，考虑要用多少的 RGB 即可</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/369.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>也就是，先用单波长混合出任意的单波长颜色，然后在波长上积分，求得任意多波长 SPD 的总的 RGB 混合方式</li>
</ul>
</li>
</ul>
<h4 id="4-Color-Space"><a href="#4-Color-Space" class="headerlink" title="4 - Color Space"></a>4 - Color Space</h4><h5 id="XYZ-颜色空间，色域"><a href="#XYZ-颜色空间，色域" class="headerlink" title="XYZ 颜色空间，色域"></a>XYZ 颜色空间，色域</h5><p>已经有了广泛使用的 sRGB（Standardized RGB） 系统</p>
<ul>
<li>先找一台机器做好它的RGB，后面的机器就按照这个方法来制造</li>
<li>sRGB 广泛应用于各种成像设备</li>
<li>RGB 系统所能形成的 gamut（色域）是有限的</li>
</ul>
<h6 id="另外一个系统：CIE-XYZ"><a href="#另外一个系统：CIE-XYZ" class="headerlink" title="另外一个系统：CIE XYZ"></a>另外一个系统：CIE XYZ</h6><ul>
<li>不是像 RGB 系统一样，用实验测出来颜色的匹配，而是人造的颜色匹配系统，事先定义颜色匹配的曲线</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/370.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>具体设计<ul>
<li>X 有两个峰值、没有负数，整个波长空间上都有分布</li>
<li>Y 分布比较正常，表示亮度 luminance</li>
</ul>
</li>
<li>可视化<ul>
<li>先做归一化到 $x,y,z$ ，只需显示两个维度就可以在 2D 平面上可视化了</li>
<li>Y 表示的是亮度，可以把亮度固定，然后让 X、Z 变化，画图画 $x,y$  </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/371.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>看到的是<u>一个颜色空间能显示的所有颜色</u>，称为 <strong>gamut 色域</strong></li>
</ul>
</li>
</ul>
<h6 id="色域"><a href="#色域" class="headerlink" title="色域"></a>色域</h6><ul>
<li><p>CIE XYZ 的色域</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/372.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>中间是白色，是混合而成的、最不纯的颜色。纯的颜色在边界</li>
</ul>
</li>
<li><p>RGB 的色域</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/373.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>可以看到，RGB 的色域是 XYZ 色域的子集。对于 sRGB，色域只是一个三角形，表示不了   CIE XYZ 这么大的色域</li>
</ul>
</li>
</ul>
<h5 id="其他的颜色空间，Perceptually-Organized-Color-Spaces"><a href="#其他的颜色空间，Perceptually-Organized-Color-Spaces" class="headerlink" title="其他的颜色空间，Perceptually Organized Color Spaces"></a>其他的颜色空间，Perceptually Organized Color Spaces</h5><h6 id="HSV颜色空间"><a href="#HSV颜色空间" class="headerlink" title="HSV颜色空间"></a>HSV颜色空间</h6><ul>
<li>组成<ul>
<li>Hue（色调）：不同类型的颜色</li>
<li>Saturation（饱和度）：更接近白色，还是更接近纯色</li>
<li>Brightness &#x2F; Lightness &#x2F; Value（亮度）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/374.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>类似的基于感知的颜色空间（Perceptually Organized），比 RGB 等更方便艺术家们选择颜色，在颜色拾取器中被广泛使用</li>
</ul>
<h6 id="CIELAB-Space-aka-L-a-b"><a href="#CIELAB-Space-aka-L-a-b" class="headerlink" title="CIELAB Space (aka L*a*b*)"></a>CIELAB Space (aka L*a*b*)</h6><ul>
<li>组成<ul>
<li>L*：亮度</li>
<li>a*、b*：两端表示互补色</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/375.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h6 id="Misc"><a href="#Misc" class="headerlink" title="Misc."></a>Misc.</h6><p>由于颜色是感知，有一些奇妙的现象</p>
<ul>
<li>互补色：人脑的视觉暂留现象就是在长时间看一个图片后，切换后会看到它的互补色。具体的互补色组合经过实验确定</li>
<li>一切都是相对而言<ul>
<li>A 和 B 的亮度相同<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/376.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>两个 X 的颜色相同<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/377.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="典型的减色系统：CMYK"><a href="#典型的减色系统：CMYK" class="headerlink" title="典型的减色系统：CMYK"></a>典型的减色系统：CMYK</h5><ul>
<li>减色系统在打印中常用</li>
<li>在减色系统中，把不同的颜色混到一起会越混越黑<ul>
<li><p>C：品蓝，M：品红，Y：黄色，K：黑色</p>
</li>
<li><p>CMY 就可以混合出各种颜色</p>
</li>
<li><p>加一个 K 是因为打印常用黑色，如果混合来用会增加很多成本</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/378.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>本课程未提及的部分：</p>
<ul>
<li>HDR（High Dynamic Range），当颜色亮过了白色，会不会还能更亮</li>
<li>伽马校正：之前提过，在 path tracing 最后算出了 pixel 的 radiance，radiance 最后要变成颜色，做简单的线性变换是得不到的。颜色在显示器上是非线性的，必须先校正过来，使得两部分非线性抵消后还是线性</li>
</ul>
<hr>
<h3 id="Lecture-21-Animation-x2F-Simulation"><a href="#Lecture-21-Animation-x2F-Simulation" class="headerlink" title="Lecture 21 - Animation &#x2F; Simulation"></a>Lecture 21 - Animation &#x2F; Simulation</h3><p>这节课：概念，下节课：解法</p>
<p>Introduction to Computer Animation</p>
<ul>
<li>History</li>
<li>Keyframe animation</li>
<li>Physical simulation</li>
<li>Kinematics</li>
<li>Rigging</li>
</ul>
<h4 id="1-计算机动画"><a href="#1-计算机动画" class="headerlink" title="1 - 计算机动画"></a>1 - 计算机动画</h4><h5 id="动画是什么"><a href="#动画是什么" class="headerlink" title="动画是什么"></a>动画是什么</h5><ul>
<li>刚开始，动画是“Bring things to life”，更多作为交流的工具，更关注美学</li>
<li>在图形学里，也可以把动画理解成：对于建模和几何的一种拓展，把3D模型延伸到时间的维度上</li>
<li>动画的形成：由于人眼的视觉暂留效应，把很多图片按顺序、按一定速度播放即可<ul>
<li>不同的应用对动画的帧数要求不同：电影 24fps，视频 30fps，游戏 60~144fps，VR 90fps</li>
</ul>
</li>
</ul>
<h5 id="一些里程碑"><a href="#一些里程碑" class="headerlink" title="一些里程碑"></a>一些里程碑</h5><ul>
<li>1878年出现了电影</li>
<li>1937年，迪士尼制作了第一步手绘、剧场版长度（40min）的动画《白雪公主和七个小矮人》</li>
<li>1963年，首个遥控笔控制、电脑生成的图形动画，由 Ivan Sutherland 演示</li>
<li>1972年，Ed Catmull &amp; Frederick Parke 的计算机动画人脸</li>
<li>1993年，电影《侏罗纪公园》把恐龙的模型做进电影里</li>
<li>1995年，第一步完全由电脑生成的动画电影《玩具总动员》（光栅化）</li>
<li>之后，动画电影进一步发展（光线追踪）</li>
<li>到2019年，《冰雪奇缘2》植物、头发的细致渲染</li>
</ul>
<h4 id="2-关键帧动画"><a href="#2-关键帧动画" class="headerlink" title="2 - 关键帧动画"></a>2 - 关键帧动画</h4><ul>
<li>关键帧定义整个动画的走向</li>
<li>中间补充过渡帧，可以自动生成</li>
<li>本质上是一种插值技术，在不同关键帧对应的点之间进行插值<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/379.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>可以用曲线、样条来控制插值的方式，在需要的地方分别应用线性插值或其他插值方法，这就是动画跟之前的几何有关联的地方<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/380.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="3-物理模拟动画"><a href="#3-物理模拟动画" class="headerlink" title="3 - 物理模拟动画"></a>3 - 物理模拟动画</h4><h5 id="物理模拟基本"><a href="#物理模拟基本" class="headerlink" title="物理模拟基本"></a>物理模拟基本</h5><ul>
<li>除了插值，更多情况下使用物理模拟的方式来制作动画</li>
<li>把牛顿第二定律 $F&#x3D;ma$ 应用到物体的质点，物体本身有一个质量、对其施加一个力，物体就会获得一个加速度</li>
<li>有了加速度，就能计算速度；有了速度就能计算位置。也就是知道了物体的力和初始条件，就能动态地更新物体下一个时刻的位置</li>
<li>对于简单或复杂的物体，<u>只要能够建立正确的物理模型</u>，包括进行正确的受力分析，并且物体本身的建模足够好，就能通过解一些数值方程来正确模拟各种动画</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/381.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>一些例子<ul>
<li>布料模拟 Cloth Simulation：衣服跟着人的运动而运动，涉及到各种摩擦力、压力的计算；为了衣服不会穿模，需要进行各种碰撞检测</li>
<li>流体模拟 Fluids：首先模拟各处水滴的形成，位置、形状、运动状况；然后进行渲染、得到外观上的样子</li>
</ul>
</li>
</ul>
<h5 id="Mass-Spring-System-质点弹簧系统"><a href="#Mass-Spring-System-质点弹簧系统" class="headerlink" title="Mass Spring System 质点弹簧系统"></a>Mass Spring System 质点弹簧系统</h5><p>需要建立物体之间的相互作用模型，模拟物理效果，来正确模拟物体的运动。质点弹簧系统是一个简单并且实用的系统。</p>
<p>质点弹簧系统的一些应用：HW8 绳子模拟；头发被风吹起的效果（考虑本身重力、之间的摩擦力、风力等）；布料动态效果的模拟等。</p>
<h6 id="基本单元"><a href="#基本单元" class="headerlink" title="基本单元"></a>基本单元</h6><p>质点弹簧系统是一系列相互连接的质点和弹簧，基础单元是<u>一个弹簧左右连接着两个质点</u> </p>
<ul>
<li>弹力<ul>
<li>理想的弹簧：没有长度，被拉开多长就产生相应多大的力（由胡克定律确定）<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/382.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>$f_{a\rightarrow b}$ 表示 $a$ 点被拉向 $b$ 点的力</li>
<li>它的问题：现实的弹簧不会没有长度</li>
</ul>
</li>
<li>对于一般弹簧，本身有长度，考虑拉伸它的情况<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/383.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>$(||b-a||-l)$ 是拉开的长度，$\frac{b-a}{||b-a||}$ 是受力的方向</li>
<li>它的问题：会永远震动下去</li>
</ul>
</li>
</ul>
</li>
<li>摩擦力<ul>
<li>需要添加摩擦力（damping），让弹簧能够停止<ul>
<li>先介绍一个记号：在仿真中，用 $x$ 表示（位置），那么用 $\dot x$ 表示一阶导数（速度），用 $\ddot x$ 表示二阶导数（加速度）</li>
<li>给弹簧施加一个跟速度相反的力 $f&#x3D;-k_d\dot b$ ，来让弹簧停下来</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/384.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>它的问题：会引起所有的运动都停下来<ul>
<li>弹簧向一个方向平移运动，即使不进行拉伸（内部没有力），也会因为有速度而停下来</li>
<li>只能描述弹簧外部的力，无法描述弹簧内部的力</li>
</ul>
</li>
</ul>
</li>
<li>用相对运动关系描述弹簧内部的力<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/385.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>$\dot b-\dot a$ 是相对速度，$\frac{b-a}{||b-a||}$ 是受力的方向（单位向量，沿着ab方向），$\frac{b-a}{||b-a||} \cdot (\dot b-\dot a)$ 就是相对速度在受力方向上做<strong>投影的长度</strong>，即相对速度在受力方向上的速度是多少（算出来的是<strong>一个数</strong>，是标量）<ul>
<li>有一些速度不影响弹簧的长度，比如 b 围绕 a 做圆周运动。这一步投影就是排除 ab 方向以外的速度，相对速度为 0 就没有摩擦力</li>
</ul>
</li>
<li>然后用这个数做跟之前一样的计算，乘上方向（变成矢量）、取反、乘以系数</li>
<li>再次理解：红框内做向量点乘、求出一个数，然后再乘以单位方向变成矢量</li>
<li>注意：摩擦力跟弹力是不同的，不需要考虑弹簧本身的长度，而考虑相对速度</li>
</ul>
</li>
</ul>
</li>
<li>最后，把弹力和摩擦力都考虑，就得到基本单元（一节弹簧）的物理模型</li>
</ul>
<h6 id="组合模型（布的物理模型）"><a href="#组合模型（布的物理模型）" class="headerlink" title="组合模型（布的物理模型）"></a>组合模型（布的物理模型）</h6><p>通过组合一节弹簧，就得到各种形状的物理模型</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/386.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>考虑组合成上图的上面的结构，来模拟布的物理模型<ul>
<li>问题一：切变会受影响，如果拉着两个角，形状会被拉长，而真实的布会抵抗切变力、不会形变<ul>
<li>改进：加入斜的对角线，如果沿着↙、↗拉动结构，↖、↘方向的弹簧会被压缩、产生对抗变化的力</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/387.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>此外，这个结构可以抵抗斜着的弯折，弹簧被折后会有复原的趋势</li>
<li>然而，这个结构不能抵抗竖直、水平的弯折</li>
</ul>
</li>
<li>问题二：现实的布也会有对抗 out-of-plane bending 的性质，也就是不能像纸一样被轻松折，在这个结构中也无法体现<ul>
<li>改进：任一个点跟隔一个点连接一条弹簧，这样不管怎样弯折都会引起弹簧的形变了</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/388.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>此外，红线的连接弱一些（对应现实中的布也会被弯折），只起辅助作用，而主要还是蓝线的连接。因此也不能只采用红线而不使用蓝线</li>
</ul>
</li>
</ul>
</li>
<li>这样就是描述布料的简单的物理模型了，可以模拟一些简单的效果<ul>
<li>如果想要布料跟人体动作的配合，还需要有摩擦力等其他模型</li>
<li>在前面讲过，真实的布料是纤维缠绕形成股、股缠绕形成线、线经过不同编织方法得来的，物理模拟通常是对真实世界的简化的表示方法</li>
</ul>
</li>
</ul>
<p>除了质点弹簧系统，也有其他的系统用来建立物理模型</p>
<ul>
<li>FEM（Finite Element Method）有限元方法，被广泛应用于汽车碰撞<ul>
<li>接触墙的地方受力，但力会传导到整个车子，就像热的传导一样</li>
<li>这种力和热的传导被称为 Diffusion 扩散，适合用有限元方法来做</li>
</ul>
</li>
</ul>
<h5 id="Particle-Systems-粒子系统"><a href="#Particle-Systems-粒子系统" class="headerlink" title="Particle Systems 粒子系统"></a>Particle Systems 粒子系统</h5><p>粒子系统基本</p>
<ul>
<li>粒子系统是什么<ul>
<li>并不是所有的东西都适合用质点弹簧系统来建模。对于<u>一堆很小的东西</u>，通常使用粒子系统来建模 <ul>
<li>把每一个粒子都建模出来</li>
<li>定义每一个粒子收到的力，分为内部的力（粒子之间的碰撞、引力等）、外部的力（重力、风力等）</li>
<li>进行模拟</li>
</ul>
</li>
</ul>
</li>
<li>粒子系统的应用<ul>
<li>适合于营造魔法的效果，模拟雾、灰尘等运动，在游戏中得到广泛的应用</li>
<li>也可以做一些其他的效果，比如流体模拟可以把水看作一个整体，也可以考虑每个小水滴的运动情况</li>
</ul>
</li>
</ul>
<p>粒子系统的实现</p>
<ul>
<li>粒子系统的挑战<ul>
<li>trade-off：粒子越多越精细，但计算也越慢</li>
<li>实现不容易。或许需要用到加速结构，比如计算粒子之间的引力时，需要查找最近的 n 个粒子</li>
</ul>
</li>
<li>粒子系统的实现方法（简单描述），对于动画的每一帧：<ul>
<li>（如果需要）创建新的粒子</li>
<li>计算每个粒子的受力（modeling，决定粒子性质的一步，到底模拟的是水滴还是雾）</li>
<li>更新每个粒子的位置、速度（如何解。是学术界更关注的部分）</li>
<li>（如果需要）删除废弃粒子</li>
<li>渲染所有粒子</li>
</ul>
</li>
<li>更多需要考虑的力<ul>
<li>重力，电磁力，弹簧力，斥力，吸引力（如图），…</li>
<li>摩擦力，空气阻力，粘滞力，…</li>
<li>跟墙、容器的碰撞（防止穿模），跟其他粒子的碰撞</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/389.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>概念延申的粒子系统</p>
<ul>
<li>粒子系统不一定只用来描述简单的点：<u>在大规模的范围内，有很多小的重复的东西，这些东西就可以被理解为粒子</u> <ul>
<li>群体中的个体，如模拟鸟群。考虑鸟会接近鸟群，但避免跟其他鸟过于接近，并且倾向于朝向平均方向</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/390.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li><u>粒子系统本质上就是定义个体与群体的关系</u>。也可以用来模拟鱼群、蜜蜂、分子运动等等，每个粒子的行为也可以复杂，比如用来模拟人群</li>
</ul>
<h4 id="4-Kinematics-运动学"><a href="#4-Kinematics-运动学" class="headerlink" title="4 - Kinematics 运动学"></a>4 - Kinematics 运动学</h4><h5 id="Forward-Kinematics"><a href="#Forward-Kinematics" class="headerlink" title="Forward Kinematics"></a>Forward Kinematics</h5><p>骨骼系统</p>
<ul>
<li><p>描述一个骨骼系统，表示跟人的骨骼连接类似的拓扑结构。通过定义、组合不同的简单关节，形成相连的复杂模型</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/391.png" style="zoom: 75%;" / loading="lazy"> </li>
<li>Pin 钉子关节，在钉住的平面内旋转</li>
<li>Ball 球关节，可以自由旋转</li>
<li>Prismatic joint，可以移动、拉长</li>
</ul>
</li>
<li><p>整个模型可以组织成一个树形结构</p>
</li>
</ul>
<p>正向运动学：Animator 提供连接方式、旋转的角度等信息，需要计算各个点运动到什么位置</p>
<ul>
<li>以图中的 Pin 关节为例，知道了两个关节的旋转情况和长度，计算机需要求出关节末端的坐标<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/392.png" style="zoom: 80%;" / loading="lazy"></li>
</ul>
</li>
<li>优点：实现容易，计算方便</li>
<li>缺点：正向运动学的定义都非常物理，但艺术家们不喜欢这样创作，需要更加直观的动画创作方式</li>
</ul>
<h5 id="Inverse-Kinematics"><a href="#Inverse-Kinematics" class="headerlink" title="Inverse Kinematics"></a>Inverse Kinematics</h5><p>逆向运动学：直接调节尖端的位置，关节自动调整位置和角度</p>
<ul>
<li>缺点：计算非常复杂，并且通常解不唯一，并且有时不存在解<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/393.png" style="zoom: 80%;" / loading="lazy"></li>
</ul>
</li>
<li>正常情况下，采用一些优化方法，来找到关节对应的位置<ul>
<li>定义优化问题：已有最后的结果坐标，如何找到几个角度和位置，满足整个结构的末端位于结果坐标</li>
<li>对于优化问题，通常用机器学习中的梯度下降法来解，或者用牛顿法等其他数值方法，而不是通过数学来求</li>
</ul>
</li>
</ul>
<h4 id="5-Rigging"><a href="#5-Rigging" class="headerlink" title="5 - Rigging"></a>5 - Rigging</h4><h5 id="控制点-Rigging"><a href="#控制点-Rigging" class="headerlink" title="控制点 Rigging"></a>控制点 Rigging</h5><p>Rigging 指的是对于一个形状的控制，类似于“木偶”的操作，一定程度上是逆运动学的一种应用</p>
<ul>
<li>对于一个角色，给它不同的控制点，通过拉拽这些点做出不同的运动。可以联想贝塞尔曲线的控制点</li>
<li>使用<ul>
<li>在电影界，会应用 Rigging，给模型添加动作、做各种造型，包括面部表情、肢体动作等</li>
<li>在美术工作中，具体需要给物体定义控制点、加上骨骼，包括软选取、蒙皮等步骤</li>
</ul>
</li>
<li>Blend Shape：可以把两个形状混合到一起，但实际上不是在混合 shape，而是在混合控制点和周围能影响到的区域，具体的 blend 过程需要各种曲线的定义<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/394.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="Motion-Capture"><a href="#Motion-Capture" class="headerlink" title="Motion Capture"></a>Motion Capture</h5><ul>
<li>可以给模型加上不同的控制点、做 Rigging 来生成动画，也可以给真人加上控制点，让控制点的位置直接反应到虚拟造型上去<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/395.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>优点：可以直接把人的动作转移到动画中去，迅速、真实，不需要美术去调，可以方便地生成大量数据</li>
<li>缺点：设备复杂，有时捕捉不到好的数据</li>
<li>除了直接通过白点，经过视觉的方法识别，也可以有电磁或机械的多种方法，完成人和模型的控制点之间的映射。其中应用最广泛的是光学的方法，贴一些 Markers、用很多复杂的摄像机，准确地捕捉动作<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/396.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="Misc-1"><a href="#Misc-1" class="headerlink" title="Misc."></a>Misc.</h5><p>其他一些问题</p>
<ul>
<li><p>感知上的问题：Uncanny valley（恐怖谷效应）</p>
</li>
<li><p>技术上的问题：面部的动作捕捉，更加细微</p>
</li>
</ul>
<p>动画生产的Pipeline</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/397.png"  / loading="lazy"> </li>
<li>第一行：设计阶段。对整体有基本把握，包括各个场景的样子、故事、模型的运动</li>
<li>第二行：进入产品线，进行实际操作。如布置场景、主人公建模、制作材质、使用 Rigging 制作造型、形成动画、Lighting、VFX（visual effects）、Rendering 等，把主人公和场景结合到一起</li>
<li>第三行：后处理。组合、2D 的滤镜效果、调色等</li>
</ul>
<hr>
<h3 id="Lecture-22-Animation-Cont"><a href="#Lecture-22-Animation-Cont" class="headerlink" title="Lecture 22 - Animation Cont."></a>Lecture 22 - Animation Cont.</h3><ul>
<li>Single particle simulation<ul>
<li>Explicit Euler method</li>
<li>Instability and improvements</li>
</ul>
</li>
<li>Rigid body simulation</li>
<li>Fluid simulation</li>
</ul>
<h4 id="1-Single-Particle-Simulation"><a href="#1-Single-Particle-Simulation" class="headerlink" title="1 - Single Particle Simulation"></a>1 - Single Particle Simulation</h4><h5 id="单个粒子模拟问题——求解常微分方程"><a href="#单个粒子模拟问题——求解常微分方程" class="headerlink" title="单个粒子模拟问题——求解常微分方程"></a>单个粒子模拟问题——求解常微分方程</h5><p>规定了物体任何一个时刻的速度，知道物体开始出现的位置，求解：在某个时间之后物体出现在哪里    </p>
<p>首先，考虑一个单个粒子的运动情况</p>
<ul>
<li>定义一个理想的情况：物体在速度场中。可以得到粒子在任一位置、任意时刻的速度 $v(x,t)$ <ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/398.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>Ordinary Differential Equation (ODE) 常微分方程<ul>
<li>$\frac{dx}{dt}&#x3D;\dot x&#x3D;v(x,t)$ </li>
<li>常微分方程：知道一个量的微分是多少，希望求出这个量是多少。常微分方程只有单个变量</li>
<li>在此处，知道速度、想求得位置。具体来说，给定起始位置 $x_0$，知道任意时间速度 $v(x,t)$，想要求得在任何时间 $t$ 的位置 $x$ </li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/399.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h5 id="Explicit-Euler-method"><a href="#Explicit-Euler-method" class="headerlink" title="Explicit Euler method"></a>Explicit Euler method</h5><p>Euler’s Method（Forward 前向欧拉 &#x2F; Explicit 显式欧拉）</p>
<ul>
<li>$x^{t+\Delta t} &#x3D; x^t + \Delta t \dot x^t$，$\dot x^{t+\Delta t} &#x3D; \dot x^t + \Delta t \ddot x^t$ </li>
<li>把时间分成很多小块（离散化），每一步不断加上步长的时间，用上一个时刻的量估计这一时刻的量</li>
<li>问题：<ul>
<li>不准，有误差。通过缩小 $\Delta t$，可以减小误差<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/400.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>稳定性不好，越模拟离现实情况越远。如下图上方，不管取什么样的步长，最后都会离开螺旋型的速度场；下方，随着模拟，不会走向水平，而变化会越来越大<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/401.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li>对于使用数值方法求解微分方程，都会面临两个问题<ul>
<li>Errors 误差：每一步计算都有误差，累积起来也会有误差。如果用较小的步长，就会降低误差。此外，<u>图形学关注视觉效果，而不是物理上的正确，一定范围内的误差可以被接受</u> </li>
<li>Instability 不稳定：“diverge”，有任何一个模拟方法，但模拟得到的结果都跟正确结果差得越来越远</li>
</ul>
</li>
</ul>
<h5 id="Instability-and-improvements"><a href="#Instability-and-improvements" class="headerlink" title="Instability and improvements"></a>Instability and improvements</h5><p>由于欧拉方法的不稳定性质，有一些<u>其他的求解微分方程的方法</u>，可以在一定程度上解决不稳定性</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/402.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h6 id="方法一-Midpoint-Method"><a href="#方法一-Midpoint-Method" class="headerlink" title="方法一 Midpoint Method"></a>方法一 Midpoint Method</h6><p>中点法</p>
<ul>
<li>第一，从起点应用欧拉方法，得到点 a；第二，取起点跟 a 点的中点，记录中点处的速度；第三，回到起点，使用中点的速度，应用欧拉方法<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/403.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
<li>把式子展开后会发现，中点法比欧拉法多了二次项。中点法（或叫做 Modified Euler 改进欧拉法）不再是线性的估计模型，而是局部的二次模型，更加准确<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/404.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<h6 id="方法二-Adaptive-Step-Size"><a href="#方法二-Adaptive-Step-Size" class="headerlink" title="方法二 Adaptive Step Size"></a>方法二 Adaptive Step Size</h6><ul>
<li>把欧拉方法和中点法结合起来：第一，在 $\Delta t$ 应用欧拉方法，得到一个点；第二，把 $\Delta t$ 分成两半，在两个 $\Delta t&#x2F;2$ 依次应用欧拉方法，得到第二个点；第三，如果这两个结果差不多，就不再继续进行，如果差得远，就把 $\Delta t&#x2F;2$ 继续往下分</li>
<li>借用了中点的思想。不是应用中点的速度、从起点出发，而是在中点再进行一次欧拉方法</li>
<li>最终，在不同的地方会选择不同的 $\Delta t$ 的大小，是一种 Adaptive 自适应的方法</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/405.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h6 id="方法三-Implicit-Euler-Method"><a href="#方法三-Implicit-Euler-Method" class="headerlink" title="方法三 Implicit Euler Method"></a>方法三 Implicit Euler Method</h6><p>隐式（Backward 后向）欧拉方法，使用下一个时刻的信息更新这个时刻的信息</p>
<ul>
<li>显式：$x^{t+\Delta t} &#x3D; x^t + \Delta t \dot x^t$，$\dot x^{t+\Delta t} &#x3D; \dot x^t + \Delta t \ddot x^t$ </li>
<li>隐式：$x^{t+\Delta t} &#x3D; x^t + \Delta t \dot x^{t+\Delta t}$，$\dot x^{t+\Delta t} &#x3D; \dot x^t + \Delta t \ddot x^{t+\Delta t}$ </li>
<li>不易求解，通常用优化方法（牛顿法，求根公式等）<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/406.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>如何定义、度量稳定性？</p>
<ul>
<li>每一步度量 local 误差，累计成 total 误差，用这两个量衡量稳定性</li>
<li>研究这两个数是没有意义的，应该研究它们的阶，也就是它们跟取的 $\Delta t$ 的关系<ul>
<li>误差会随着取更小的 $\Delta t$ 而减小。具体是如何减小的？它们之间是什么样的关系？这是要研究的问题</li>
</ul>
</li>
<li>结论：Implicit Euler Method 是一阶的<ul>
<li>Local truncation error: $O(h^2)$ </li>
<li>Global truncation error: $O(h)$ </li>
<li>$h$ 是步长，在这里就是 $\Delta t$</li>
</ul>
</li>
<li>理解 $O(h)$ ：<ul>
<li>如果把 减小一半，那么最后的误差也会减小一半</li>
<li>阶数越高越好（$\Delta t$ 减小一些，误差减小得更多）。用阶数来衡量一个数值方法的稳定性</li>
</ul>
</li>
</ul>
<h6 id="方法四-Runge-Kutta-Families"><a href="#方法四-Runge-Kutta-Families" class="headerlink" title="方法四 Runge-Kutta Families"></a>方法四 Runge-Kutta Families</h6><p>龙格库塔方法，是一类方法，非常适合于解常微分方程，特别是对于非线性的情况</p>
<ul>
<li>对于场扭曲得厉害的情况，中点法比前向欧拉法好（中点法是平方模型、前向欧拉是线性模型），而龙格库塔更加好</li>
<li>四阶的龙格库塔方法是最广泛使用的方法，aka RK4</li>
<li>可以看到，更新方法是在两个点之间取了几个中间点，进行平均</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/407.png" style="zoom: 75%;" / loading="lazy"></li>
</ul>
<h6 id="方法五-Position-Based-x2F-Verlet-Integration"><a href="#方法五-Position-Based-x2F-Verlet-Integration" class="headerlink" title="方法五 Position-Based &#x2F; Verlet Integration"></a>方法五 Position-Based &#x2F; Verlet Integration</h6><p>不是基于物理的方法，只是一些计算</p>
<ul>
<li>通过一些非物理的方式，直接改变位置。原理简单、实现快，但不满足一些性质（如能量守恒）</li>
<li>在之后的流体中，介绍一个具体例子，调整不同粒子的位置</li>
<li>这个方法不基于物理，但在某些模拟中好用。在 HW8 里实现</li>
</ul>
<h4 id="2-Rigid-Body-Simulation"><a href="#2-Rigid-Body-Simulation" class="headerlink" title="2 - Rigid Body Simulation"></a>2 - Rigid Body Simulation</h4><p>Rigidbody 是刚体的概念。刚体不会发生形变，它内部所有的点都按照同一种方式进行运动。整个刚体可以看作一个粒子运动。</p>
<p>在刚体模拟中，会更多考虑其他物理量</p>
<ul>
<li>在原本的位置基础上，又会考虑它的速度、角速度等物理量。因为刚体很大，需要一些物理量计算它每一个点的运动方式</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/408.png" style="zoom: 67%;" / loading="lazy"> </li>
<li>位置求导是速度，角度求导是角速度，速度求导是加速度，角速度求导是角加速度</li>
<li>可以看作是对粒子进行一些扩充，在这之后，就利用之前的各种数值方法，求解任何时间 $t$ ，刚体对应的位置和旋转方式</li>
</ul>
<h4 id="3-Fluid-Simulation"><a href="#3-Fluid-Simulation" class="headerlink" title="3 - Fluid Simulation"></a>3 - Fluid Simulation</h4><p>流体模拟有不基于物理的方法，也有基于物理的方法</p>
<h5 id="A-Simple-Position-Based-Method"><a href="#A-Simple-Position-Based-Method" class="headerlink" title="A Simple Position-Based Method"></a>A Simple Position-Based Method</h5><p>Key idea</p>
<ul>
<li><p>整个水体是由很多不可压缩的刚体小球组成的。计算出每个小球的位置，然后进行渲染即可</p>
</li>
<li><p>基本假设：<u>水在任何一个地方都是不可压缩的</u>，const density</p>
</li>
<li><p>给定任一时刻，对于如果有一个地方，水的密度发生改变（跟平静时的密度不一样），那么就需要把密度“修正”过来</p>
<ul>
<li>通过移动小球的位置，使得任意一个密度不正确的地方被修正</li>
<li>为了做修正，需要知道<u>任何一个点的密度对所有小球位置的梯度是多少</u>。比如，考虑一个点的密度跟它周围小球的位置的导数，如果小球的位置改变了，这个点的密度也会发生改变</li>
</ul>
</li>
<li><p>具体的修正过程中，对于任何一个位置，想要让不正确的密度回到正确的密度，并且知道如何调整各个小球的位置、使得密度向想要的方向变化，这就是机器学习中 gradient descent 的过程（想要目标跟某个结果相似，就让当前状态往目标状态变化）</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/409.png" style="zoom: 67%;" / loading="lazy"> </li>
<li><p>这<strong>不是基于物理的模拟</strong>，不需要用到欧拉法等方法，而是在任何一个时间，都知道任何一个小球应该如何进行位置变化</p>
</li>
<li><p>因此，也会无止境地修正、运动下去。可以再添加一些能量衰减的系统，使得水最后回归到静止状态</p>
</li>
</ul>
<h5 id="Eulerian-vs-Lagrangian"><a href="#Eulerian-vs-Lagrangian" class="headerlink" title="Eulerian vs. Lagrangian"></a>Eulerian vs. Lagrangian</h5><p>在<strong>物理模拟</strong>中，模拟大规模物质用到的两个基本思路</p>
<ul>
<li>Lagrangian Approach 拉格朗日方法 &#x2F; 质点法：<ul>
<li><u>考虑每个个体的运动情况</u> </li>
<li>盯着物体看</li>
</ul>
</li>
<li>Eulerian Approach 欧拉方法 &#x2F; 网格法：<ul>
<li><u>看待一系列大规模物体</u>，把整个空间分成网格，不考虑网格中个体的进出，而考虑网格随着时间如何变化</li>
<li>盯着某个固定空间看：对于中间的网格，t 时刻是黑色的鸟，t-1是橙色的鸟要飞走，t+1是蓝色的鸟飞过来（鸟向右飞）</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/410.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
</li>
</ul>
<p>也有工作将两种方法结合</p>
<ul>
<li>首先，认为不同粒子都具有某些材质属性；然后，融化的过程在格子里做、考虑不同的格子，将信息记录在格子上；最后，把格子记录的信息写回到每个粒子个体中</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/411.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<h4 id="4-展望"><a href="#4-展望" class="headerlink" title="4 - 展望"></a>4 - 展望</h4><p>Rasterization</p>
<ul>
<li>图形API</li>
<li>实时渲染：《Real-time Rendering》 + OpenGL</li>
<li>Real-time Ray Tracing，DXR，跨平台的 Vulkan 等</li>
<li>各种不同的着色器，顶点和片段</li>
</ul>
<p>Geometry</p>
<ul>
<li>数学基础，微分几何、离散微分几何，拓扑，流形</li>
</ul>
<p>Light Transport</p>
<p>Animation &#x2F; Simulation</p>
<ul>
<li>GAMES 201</li>
</ul>
<p>Real-Time High Quality Rendering（GAMES 202）</p>
<ul>
<li>Soft Shadows and Environment Lighting</li>
<li>Precomputed Radiance Transfer</li>
<li>Image-Based Rendering</li>
<li>Non-Photorealistic Rendering</li>
<li>Interactive Global Illumination</li>
<li>Real-time Ray Tracing &amp; DLSS, etc.</li>
</ul>
<p>Advanced Image Synthesis</p>
<ul>
<li>Part 1: Advanced Light Transport</li>
<li>Part 2: Advanced Appearance Modeling</li>
<li>Part 3: Emerging Technology for Rendering</li>
<li>Foundations for rendering research! 开启 rendering 的科研之路</li>
</ul>
<p>最终目标：做到真正的“以假乱真”：</p>
<ul>
<li><p>实时渲染，离线渲染：解决已知的问题，比如全局光照怎么做</p>
</li>
<li><p>外观建模：解决未知的问题，比如不知道动物的毛发应该如何渲染，就进行一些建模</p>
</li>
<li><p>成像：未来设备，不同成像方式</p>
</li>
<li><p>新的技术</p>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/cg-img/games101/412.png" style="zoom: 67%;" / loading="lazy"></li>
</ul>
<hr>
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>课程笔记</tag>
        <tag>游戏开发</tag>
        <tag>图形学</tag>
      </tags>
  </entry>
  <entry>
    <title>低浮上 - 2022年终总结</title>
    <url>/2023/02/19/summary-2022/</url>
    <content><![CDATA[<h3 id="2022"><a href="#2022" class="headerlink" title="2022"></a>2022</h3><p>2022年过得并不能说好，甚至有些时候体会到了“道心破碎”的感觉。但总体上还是向未来迈出了关键的一步，应该好好记录一下。</p>
<p>此外，思考一番后我决定还是不去纠结年终总结是按照自然年还是农历年来写了，怎么方便怎么来吧。所以本文的“今年”、“明年”这种习惯性表述都是以兔年春节来分的，但回顾一整年做了什么其实还是从1月数到12月的。总之方便理解就好。</p>
<h4 id="其一"><a href="#其一" class="headerlink" title="其一"></a>其一</h4><p>过去一年大概是这样过的：</p>
<ul>
<li>1月、2月：在家放假，其中1月在走实习生面试流程。过完年在看GAMES101。</li>
<li>3月、4月：去学校第二周就被封了，在寝室吃了一个来月的国家饭。把GAMES101看完了，然后趁着做作业，把笔记也整理了一下。<a href="https://acbgzm.github.io/2022/03/15/games101/">【笔记点击这里】。</a>此外看了一点LearnOpenGL，但看得不多。后面也没继续看了。</li>
<li>5月、6月：学校解封我就直接回家开躺了。6月2号线上入职实习了，但跟mentor交流不太方便，所以基本上也没干啥。此外今年也跟家人去爬山了，在端午节假期。</li>
<li>7月、8月：上海一解封，我就去上海线下实习了。回忆起来，这段时间是一整年的高光时刻了，努力了很多，也见识了很多。虽然也没能转正就是了。</li>
<li>9月：离职后学校又封了没法返校，就再次回家呆着了。除了躺，这个月也在投简历、做笔试。这个月基本上没什么面试机会。</li>
<li>10月、11月：10月初回学校了（又隔离），国庆节假期结束后经历了高强度面试，月底拿到几个offer就把剩下的流程全推了。11月除了走完了签约流程，剩下的都在玩了。此外也写了一篇文章记录了一下秋招经历，当作回馈牛客社区了。原文点击<a href="https://www.nowcoder.com/discuss/428252230693683200">【牛客链接】</a>，含博客特典的版本<a href="https://acbgzm.github.io/2023/03/28/jobhunting-2/">【点击这里】</a>。</li>
<li>12月：各种意义上的不太平。学校又放假了，就又回家躺着了。</li>
</ul>
<h4 id="其二"><a href="#其二" class="headerlink" title="其二"></a>其二</h4><p>今年只有3、4、10、11月在学校。其中，3、4月被封在宿舍小房间里，直接爽躺+爽玩游戏。10月忙秋招，11月觉得很累就又全职玩了一个月的游戏。因此2022说真的确实没有把一丁点心思放在毕业、科研上，一整年都没有新进展。</p>
<p>其实我也不知道为什么会对一件事情有毫无理由的、如此强烈的抗拒感。但不管怎么说，从今天开始算，到交初稿的时候需要完成所有的工作和写完大论文，也就1.5个月了。虽然现在还是一边愁一边抗拒的状态，但想到一切都要结束了，还是有点兴奋的。</p>
<p>另外，今年竟然有一大半的时间在蹲家，分别是1、2、5、6、9、12月。感觉也挺好的，2023年就要工作了，今年也算是最后跟父母相处的时间了。但其实房子很小，我每天都是关着门在小屋里、支个桌子烂在床上，并没怎么说上话。记得大约5年前读『青春野郎』的时候，看到一种说法是太独立、太像大人了也不太好，总之这段时间就心安理得地在家当巨婴了，享受着最后的饭来张口、被伺候着的生活。</p>
<p>最后，今年也在心浮气躁的路上越走越远了，动画、漫画看不进去，书从去年的2本进化为今年的1本。虽然也有与时俱进的“先审后播”伟大制度和优良传统的花钱看删减的不爽作为原因，但归根结底还是失去耐心了。</p>
<p>只要能躺着，我就捧着手机看来看去；只要能坐着，我就打开LOL大乱斗度过一段脑子放空的时光。还好今年足够闲或者足够摆烂，除此之外如果还有一点劲头，我就在努力地玩新游戏，并且有意地体验了各种各样的品类。</p>
<h4 id="其三"><a href="#其三" class="headerlink" title="其三"></a>其三</h4><p>2022年——或者说好多年了吧，总感觉自己不在状态，或者说这种迷迷糊糊的状态要成为正常状态了？但2022年确实感觉干啥都不顺利。在青岛、长春、上海都被各种封，也遇到了最烂的毕业行情之2022年的游戏行业，实习没卷到转正hc，秋招到处碰壁。</p>
<p>要说在20年、21年，甚至可以说是今年之前的每一年，我都觉得自己前途一片光明，虽然笨且懒，但早晚有一天能成为大牛，能在专业道路上走得很远。但现在我觉得其实在时代洪流下，我做的各种选择其实是没法说好坏的，我的努力也是不值一提的。跟很多人比，尤其是跟最近的一届比，我们确实是天上地下中的地下，但从另一个角度想，呃，也没想出什么。总之就这么回事吧。</p>
<p>但不管怎么说，在2022年我也向未来和梦想迈出了第一步了。虽然曾有一万个人向我说过，第一份工作非常重要，有机会一定要去大厂。但是不管怎么说，呃，也没什么好说的，就这样吧。目前唯一的目标就是能在行业里多苟几年吧。近一点的话，不被裁应届就算成功了。</p>
<p>就这样吧。反正该做的事情还是做，该学的东西也慢慢学，但不会很急切地去追求什么了。</p>
<h4 id="其四"><a href="#其四" class="headerlink" title="其四"></a>其四</h4><p>网易云音乐年度歌单：</p>

    <div id="aplayer-aOpttonU" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="7878377703" data-server="netease" data-type="playlist" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#555"
    ></div> 

<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/2.jpg"  / loading="lazy">

<p>steam年度游玩概况：</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/1.png"  / loading="lazy">

<p>此外，2022年的观赛生活非常精彩，我基本上全程看下来的有CSGO IEM科隆（恭喜karrigan）、LOL S赛（恭喜deft）、世界杯（恭喜梅西）。很多比赛前面好看，但到了决赛就会一边倒，相对沉闷。但这三个比赛可以说是从头精彩到尾，甚至决赛不约而同地上演了大比分、弱胜强、老将圆梦的剧情，看得我心潮澎湃，非常感动。其中，世界杯决赛是我看球以来见过的最精彩的足球比赛，看到穆阿尼单刀的时候我心跳都停了。一年看下来三场完美大戏，甚至有点不真实的感觉，但剧本其实也不敢写出这样的剧情不是吗。</p>
<p>后文大概就是按时间总结的生活经历和游戏经历了。</p>
<h3 id="上下铺神游——空洞骑士、海沙风云"><a href="#上下铺神游——空洞骑士、海沙风云" class="headerlink" title="上下铺神游——空洞骑士、海沙风云"></a>上下铺神游——空洞骑士、海沙风云</h3><p>2月底去的学校，大概过了一周吧，吉林疫情就开始了，学校连着核酸检测了几天，最后还是封了。由于全校一起做核酸的队实在是太长了，硬排要排两个多小时。我提出一个伟大定理：如果队变短了，那只会越来越短，然后在每次快没人了的时候再去——大概是晚上两点，这样就只需要排不到15分钟了。</p>
<p>这一整年排过的队多如牛毛，这个策略屡试不爽。如果是规定结束时间、超时不做的，就可以装教职工、在马上要下班时插队，好用，但我面子薄，因此只干过一次。这些经验不知道以后还有没有用武之地。</p>
<p>3、4两个月是全程被封在寝室的，洗漱都成困难，东西也没法买，每个人的生活空间其实就是上床下桌这么大。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/bgm_hollowknight.png"  / loading="lazy">

<p>其实我作为非硬核玩家，对“受苦”一直没什么兴趣，也一直不明白高难度的美学在何处。玩『空洞骑士』是因为这段时间加了一个做2D类银河战士恶魔城的同人游戏小组，其中领了一个“人物下砸”的功能，为了更好地实现它我就去玩世界上最有名的类银河战士恶魔城游戏了。</p>
<p>『空洞骑士』确实质量过硬。审美、设计水平在线，每个区域地图风格各异，但都又好看又好玩，NPC和剧情各有特色，共同打造了一个生机勃勃的地下虫子帝国，让我充分体验到了类银河战士恶魔城的乐趣。操作手感利索舒服，难度曲线也很合理。此外『空洞骑士』在保证质量的同时提供了相当大的体量，不管是喜欢平台跳跃还是喜欢打boss，都有不同难度的各种内容可以选择。我一边玩一边在想：这真是独立游戏吗？</p>
<p>『空洞骑士』确实是有难度的，但在我通关过程中也没感到折磨。当然了游戏也有专门给硬核玩家提供的一些内容，像痛苦之路、硬核二周目、英灵殿（？）啥的，但我就都没玩了。毕竟我不是硬核玩家，对每个游戏都是尽可能在一周目内体验更多的内容，通关后就不再碰了。</p>
<p>有一件小事：直到我实现完“下砸”功能，都没解锁『空洞骑士』的下砸😅。在解锁之后其实也基本不用它打架。但有没有是另一回事，游戏的技能+符文系统设计得也不错，可以形成几个手感完全不同的战斗流派，提供了另一部分的战斗乐趣。</p>
<p><strong>『空洞骑士』：82&#x2F;100</strong></p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/bgm_haisha.png"  / loading="lazy">

<p>通关『空洞骑士』后我就在玩『海沙风云』了。这是一个各方面水平都在线的AVG，没什么特别需要评价，但毕竟有“国产”这一buff加持，让我有了一些好感。总之我打完了全路线、全成就。我认为比较好或新奇的有这几点：警匪题材，剧情轻松，少女主角，全程粤配。我最喜欢的是游戏整体的体验，也就是通过各种不同的视角经历同一件事情，知道完整的真相。至于缺点，主要就是不能随时读档和剧本、人设上的小问题吧。</p>
<p><strong>『海沙风云』：70&#x2F;100</strong> </p>
<p>这段时间除了上床就是下桌，吃饭和洗澡都成问题，基本上一切停摆，过得比较迷糊。『空洞骑士』提供了强大的氛围感，『海沙风云』讲好了一个故事，都让我度过了一段不错的神游时间。</p>
<h3 id="集中——只狼、GNOSIA、吸血鬼幸存者"><a href="#集中——只狼、GNOSIA、吸血鬼幸存者" class="headerlink" title="集中——只狼、GNOSIA、吸血鬼幸存者"></a>集中——只狼、GNOSIA、吸血鬼幸存者</h3><p>5月学校基本上是放假了，我就又回家了。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/bgm_sekiro.png"  / loading="lazy">

<p>在学校的时候我就把『只狼』开了个头，回家更是爽完半个月。玩这个的起因是封寝时期恰逢『艾尔登法环』发售，我看到了铺天盖地的热烈讨论，因此产生了一些兴趣。但由于之前从未玩过魂游，也知道自己手残，就没有直接去打『艾尔登法环』。『只狼』作为2019的TGA年度最佳游戏，被称为“任何人都能乐在其中的硬核动作游戏”（忘了是谁说的，可能是我说的），叫好又叫座。而且『只狼』打折，『艾尔登法环』不打折。因此我就去玩『只狼』了。</p>
<p>在玩『只狼』的时候，我脑子里一直冒出来一个词“禅意”，可能这个词本意并不是我想的那样，但我其实想说的是一种极致的意境。</p>
<p>『只狼』的gameplay简化了等级、装备等系统，把“挡刀”这个机制发掘并突出了出来。『只狼』的难度可以说完全在boss战：难是因为不能靠刷等级涨数值，只能靠提升水平来硬打，但仔细想其实只用一只手拿着鼠标就可以过。因此操作倒是其次，更多需要的是心态上的稳定。屡战屡败，一次次复活，一点点变强。看着刀剑相交的火光，听着铛铛铛的打铁声，我感到自己的心都慢慢变得平静，在某种程度上也取得了武道上的极致。</p>
<p>另外就是比较外部的意境了，我非常喜欢只狼各个区域营造的氛围，尤其是源之宫（下图并不是）。不觉得很酷吗？我觉得源之宫太酷了，完美符合我对东方仙境的想象，美丽并带着凄凉。我想在内容创作的过程中，最有含金量的一步就是把心中某个模糊的概念具象化吧。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/sekiro_2.jpg"  / loading="lazy">

<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/sekiro_3.jpg"  / loading="lazy">

<p>『只狼』的沉浸感确实是我体验过的非常强的，我总结了几个原因：</p>
<ul>
<li>系统比较简单，需要考虑的杂事很少，跳戏的情况非常少</li>
<li>数值不重要，变强的是玩家而不是角色，打败强敌的时候成就感爆棚</li>
<li>难，游玩时需要刻意地保持专注，很容易一直处于心流</li>
<li>从设计到玩法的整体性好<ul>
<li>我能关注到的比如复活机制的配套剧情设计，不同阵营的打扮差异</li>
<li>举另一个例子的话，打完之后我看了<a href="https://www.bilibili.com/video/BV1N3411B7D6">这个视频</a>，讲的是每个角色的一招一式也体现了人物的个性和成长经历。这样的考虑在游玩时大概关注不到，但毫无疑问增加了沉浸感</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/sekiro_1.jpg"  / loading="lazy">

<p>『只狼』是我心目中文化色彩很浓厚的游戏，把死亡、忍者、武士等文化展示到了极致。不管是游戏里还是游戏外，from software不像局限于普遍价值上“快意恩仇”的烂武侠，而是回归本心，走极致之路。</p>
<p>要是硬说缺点，我觉得虽然『只狼』的场景做得很好，但人物模型总觉得缺少细节；然后除了拼刀，其它的操作手感（比如跳跃）不是很爽，知道就是这么设计的，但依然觉得不是很爽。</p>
<p><strong>『只狼』：91&#x2F;100</strong> </p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/bgm_gnosia.png"  / loading="lazy">

<p>5月还玩了『GNOSIA』。这是一个真能玩狼人杀的AVG，把狼人杀游戏规则的各种身份、玩法放到了太空时空旅行的故事背景中。在一次次循环里，主角回忆起同伴和往事，认清世界的真相。『GNOSIA』很有趣，很浪漫，不太好描述，只有自己玩了才知道。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/gnosia_1.jpg"  / loading="lazy">

<p><strong>『GNOSIA』：75&#x2F;100</strong> </p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/bgm_vampiresurvivors.png"  / loading="lazy">

<p>此外，『吸血鬼幸存者』也是我在5月玩的、值得一记的游戏。虽然画面不好看，但技能、道具设计得多样且好玩，便宜大碗，很有诚意。甚至引领了行业发展，催生出“类吸血鬼幸存者”（或许可以称为割草型肉鸽）游戏的井喷，算是开创了一个子类型吧。</p>
<p>什么时候我会去玩新游戏？一般需要心态比较轻松，再在打开前做一些思想建设。但刷两把『吸血鬼幸存者』不需要有任何负担。『吸血鬼幸存者』比较休闲，但也提供了一种比较纯粹的快乐，让我处于持续集中和彻底放空的中间态。</p>
<p>但我对一直刷钱涨数值的玩法不太感冒，所以还是“通关就扔”的打法，把地图都打了一遍就差不多弃了。</p>
<p><strong>『吸血鬼幸存者』：70&#x2F;100</strong> </p>
<h3 id="细糠——DISCO-ELYSIUM"><a href="#细糠——DISCO-ELYSIUM" class="headerlink" title="细糠——DISCO ELYSIUM"></a>细糠——DISCO ELYSIUM</h3><h4 id="其一-极乐迪斯科"><a href="#其一-极乐迪斯科" class="headerlink" title="其一 极乐迪斯科"></a>其一 极乐迪斯科</h4><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/bgm_discoelysium.png"  / loading="lazy">

<p>6月入职了但还是在家，相对比较清闲。7、8月在上海，其中7月相当忙，没怎么玩新游戏，8月任务完成得差不多了就又开始玩了。这三个月我主要就在攻略广受好评的『极乐迪斯科』。</p>
<p>『极乐迪斯科』作为独立游戏获得过TGA的最佳叙事、最佳独立游戏等重量级奖项，在各种社区也被吹得特别高，因此我就对它产生了一些兴趣。其实早在在2020年『极乐迪斯科』出了中文并且第一次打折的时候我就买了，也简单地玩了一下，不到1h就卸载了。因为这游戏确实是属于不太容易打开玩的类型，全是字，还相当抽象晦涩，配音更是一听就要睡着了。就算时隔两年重新玩，也花了我近三个月。</p>

    <div id="aplayer-MkHsqmwL" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="28064267" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>我对哲学、政治没有多大的兴趣，但在玩的过程中，我发现就算完全除去这些，『极乐迪斯科』也是一个不错的RPG。音乐、美术风格独特，搭配上故事，营造出一种混乱又颓废的气氛。此外我非常想感谢『极乐迪斯科』的汉化组。翻译比较有文学性的原著总是难的，但『极乐迪斯科』的中文翻译可以说非常出色，我光是看着24种中文技能名称都想起立鼓掌。信达雅都难以涵盖它所有的好。</p>
<p>特别地，我觉得难能可贵、也能促进行业发展的一点是，『极乐迪斯科』展示了电子游戏的内容包容性和强大的性能。电子游戏一方面可以给传统媒介增加交互性和沉浸感、拓宽艺术性，另一方面又降低了人们接受严肃内容的难度。比如我，可能很难静下来看一本艰深的大部头，但也能愉快地体验『极乐迪斯科』表达的故事和思想。一定也有某个人因为这个契机，成为了真正的文学的粉丝。电子游戏在向上和向下两个维度对内容进行了拓展。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/discoelysium_1.jpg"  / loading="lazy">

<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/discoelysium_2.jpg"  / loading="lazy">

<p>有这些感想是因为当时刚好赶上<a href="https://game.qq.com/2022/recall/">腾讯游戏发布会</a>。作为宇宙top 1的游戏公司，腾讯的发布会无微不至地照顾了各路领导的关注点，一方面展示了游戏“赋能”传统文化、航天、元宇宙，助力文物保护、环境保护（俗称整虚的），另一方面展示了通过游戏能进军多少领域、申请什么项目（俗称领导的生意）。发布会涵盖了从冠冕堂皇到口袋饱饱一条龙，鉴定为满分；而作为公司，腾讯在商业上登峰造极的同时，也开拓了游戏发布会的特色版新形式，真正做到了引领时代。</p>
<p>我完全知道这是为什么。但我和很多人也都知道即使抛开电子游戏承载的东西，其本身同样具有价值。虽然很唐突，但我想到岩田聪等人的真正伟大之处或许不在于创造过多么伟大的历史，而是他们成为了代表着文明社会、文明人类的符号，当想到他们，其他人得以保留对未来的希望。</p>
<p>在『极乐迪斯科』里的瑞瓦肖，大人物们心怀鬼胎，为了占领或者颠覆，实现某个概念。这种概念可能来源于光辉历史，也可能来源于外部。世界上总有一些人在追求更高级的艺术或者更纯粹的快乐，社会的文明程度从宏观上也一定是进步的。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/3.png"  / loading="lazy">

<p>因为拿offer的时间比较特殊，我可以加入2021、2022两年的官方实习生群。平时我除了问问题基本上不在群里水，也没有高强度地跟踪各种消息，只是偶尔看看两个群气氛的差异，找点节目效果。最大的节目效果产生于秋招时期。临近9月，2022群经常交流笔面经验，因此我看得更勤了一些。有些人胆大包天，竟敢直言工作不好找、寒气逼人；也有人对现状表达了不满，矛头直指一号位。果然，9月刚开始没几天，官方的2022年实习生群就给冲没了。我一方面感叹年轻人真是武德充沛，另一方面也觉得未来其实也是有盼头的。</p>
<p>或许是因为身处人人都知道怎么回事、人人又不明说的奇妙默契氛围中，我对『极乐迪斯科』的评价偏高。或许它本身并不传达这么别扭的内容，但不管怎么说它让我想了些有的没的。</p>
<p><strong>『极乐迪斯科』：83&#x2F;100</strong> </p>
<h4 id="其二-从理塘到了上海"><a href="#其二-从理塘到了上海" class="headerlink" title="其二 从理塘到了上海"></a>其二 从理塘到了上海</h4><p>实习的生活还是很有趣的。虽然也没特别地去什么地方玩，只是在过普通的上班生活，但比起在学校和在家，还是做了一些有趣的事情。</p>
<p>我合租的自如在静安新城二区，是老破小里的一个房间（因为是短租只能选到最大的一间，app上显示是11.1m²，但有个跟房间一样长的、大到意义不明的衣柜，所以实际体验并没有11.1m²）。一床、一空调、一桌、一衣柜，每月加上水电就是4000块钱房租（人家挣钱多方便），公共空间窄到只能通过一个人，洗澡的地方转身都费劲。这种房间哪些人会来租？当然就是大厂的独身年轻人了。毕竟这里是离漕河泾最近的、有一点生活气息的地方了，从这里骑共享单车到公司只需要12分钟。</p>
<p>我的作息大概是这样的：</p>
<ul>
<li>9点起床，排队洗个澡，找单车去公司（但经常走到一半才能找到车）。到了公司拿早饭，到工位上刚好是10点，上工。</li>
<li>12点关灯，我一般是等半个小时再去食堂，避一避饭点。</li>
<li>吃完了随便做点什么，一般是看看视频啥的。13:40闭眼休息一下。</li>
<li>14点上工。同样是为了避饭点，18:30去食堂。</li>
<li>21点走。正式员工应该是22点走。到家后洗个澡、收拾一下基本上是22点。</li>
<li>一边狂吃，一边跟同学打LOL大乱斗，或者自己玩点别的，到大约0点。</li>
<li>看手机到睡着。刚去的时候作息比较收敛，慢慢地习惯上班节奏后就又越来越阴间了，可能2点左右睡觉。</li>
</ul>
<p>但这段时间其实过得很舒服，甚至可以说是随心所欲了，不管是从生活上还是工作上。</p>
<p>生活上，哥们独居并且有钱拿，虽然还做不到“不费力就住进了高楼”，但比起以前，确实是“收获好多money”了。用劳动赚钱本身就是一件值得高兴的事情，而且大公司确实能让实习生都生活得比较体面，该管的饭都管、该发的礼物都发，就算除去4000的房租+没刻意节省的日常花销，平均每个月的工资和补贴也能剩下一小半。</p>
<p>我在2019年也独居过挺长时间的，但当时没有收入，还有各种压力，虽然感觉也挺爽，但总体上不如实习时爽。工作日还比较收敛，只是每天晚上下班了去便利店买零食。到了休息日，那就是想吃什么吃什么，想干什么干什么。后面几周的周末我经常中午起床后先吃一顿，晚上8点再出门（刚好是垃圾分类的最晚时间）去跑步+骑车，回来的路上买饭+冷饮在屋子里吃到半夜。有的时候并不锻炼，就单纯地骑着车到处逛逛，遇到什么想吃的就去买。工作后，吃饭、买零食基本上也不看价格了，可以说￥30以下花起来眼睛都不眨，非常潇洒。</p>
<p>（下图是骑得最远的一次，到了刚去上海第一周时住的公司中转酒店。）</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/8.jpg"  / loading="lazy">

<p>而工作上，我作为实习生的任务就是全身心地投入到一个问题中，不用操心其它任何的事情。难度和随之而来的压力当然不小，但比起做业务来说肯定自由很多。另外让我感到舒服的一点是，没人关心我要不要跟别人建立联系，没人在意别人的生活方式和性格。这样的环境让我感到平静和安心。阳角当然可以广结天下英豪，但阴角也不会被强迫做任何事情，或者其实没人在意阳角阴角这种概念。只要做好工作，其它的可以尽情保留本心。</p>
<p>我的意思是——我确实没跟别人去建立联系。我的工作内容是完全独立的，而且也是超级阴角，就没去花心思结识别人。但只从旁观的角度来看，我觉得同事们都是比较纯粹、比较努力的打工人。大家虽然性格、习惯各异，但都有游戏这个共同爱好在，休息时大部分人不是在玩游戏，就是在看各种游戏视频，根本看不出来谁25、谁35。只有吃饭时偶尔听到的育儿话题，才让我意识到有些人是肩负多方面责任的立派职场人。</p>
<p>而打工人的努力这方面，除了表现在所有人的工作强度上，还有就是体现在我观察比较多的两个人的生活方式上。mentor家在苏州，每周末在两地来回，工作日经常一天一块钱都不花：早上多拿点吃到中午，晚上再去吃免费晚餐（公司只有午饭收费），水当然是接公司的喝；上下班走路或者等晚上的免费打车，共享单车都很少骑。他也很用功，晚上经常是一个屏做需求、另一个屏放着网课在看。小组组长更是重量级，每天走的最晚、来的最早（据他说一直是6点半起、8点到，10点多走，来回还都比较远要坐地铁），甚至中午或者晚上吃饭时间也很少看到他在休息。</p>
<p>他们两个都是33岁左右、有小孩的人，我作为24岁确实各方面都自愧不如，心里很是敬佩。我根本想象不到自己做到这种程度的样子。</p>
<p>总结我的见闻到一句话：游戏行业确实是凝聚了很多人的热爱、梦想和聪明才智的地方。这句话我也写到我的面经文章里了。</p>
<p>还有一件事，可以说是短暂、平静的上班生活中唯一“跳出日常”的部分。刚去没两周吧，我当时强度很大，一边要上手工作、一边老师又在催小论文，连着干了好几天之后得了结膜炎，两眼通红。有一天中午12点下班后，我顶着40度大太阳骑车先去了最近的社区医院，不让进、又反方向骑到一个私立医院（叫古美医院）。拿了药后，被药师叫住听她说了半天她儿子的事情（后面竟然加上微信继续聊），骑车回公司，排队做核酸，然后去吃饭，草草吃完回到工位刚好14点，又接着上班。这种事情回想起来会觉得自己强度挺高的。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/sh_map2.png"  / loading="lazy">

<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/5.png"  / loading="lazy">



<h4 id="其三-世上无难事和减法难做"><a href="#其三-世上无难事和减法难做" class="headerlink" title="其三 世上无难事和减法难做"></a>其三 世上无难事和减法难做</h4><p>对于我的工作内容，其实就是项目组想使用UE4的一个功能，但发现会出现问题A，让我去把它解决掉。但由于UE4相当庞大，动其中一个地方会引出一系列其他问题，而且问题本身也在不断地扩展，其中比较严重的一个是我发现的问题B。</p>
<p>一上来我觉得这个问题挺难。现在复盘一下，我会觉得一件事难应该是有这些原因：</p>
<ul>
<li>问题太难了<ul>
<li>不知道具体成因，也不知道怎么找成因</li>
<li>完全查不到相关资料，只能自己来看</li>
<li>知道成因后也不知道怎么修改</li>
</ul>
</li>
<li>系统（UE4）太庞大了<ul>
<li>不知道从哪里开始了解</li>
<li>一个功能牵扯的东西太多了，很多关键数据在几个步骤中不互通</li>
<li>动一个地方必定引起其它地方出错</li>
</ul>
</li>
<li>硬件不行<ul>
<li>实习生电脑太他妈差了，第一天申请的固态硬盘离职了还没到。编译一次引擎要6h，我一天只能编译两次（上班一次，下班一次）。还有一次早上去了开不开机，要找人来修。</li>
</ul>
</li>
</ul>
<p>不过总之我把问题A和问题B都解决了，并且用了一种我自认为相当优雅的方式。不管是时间还是空间，还是“可解释性（笑）”，我都认为尽到我的全力了。</p>
<p>下面这张图我从7月2号画到8月10号（已模糊处理）。刚开始是因为发现牵扯到的东西有很多，只记录文字比较乱，就改为画图。但说实话我没有想到最终的问题有这么繁琐，因此现在我很庆幸当时这样做了。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/7.png"  / loading="lazy">

<p>对我来说，当面对一个比较大型的系统时，记录这样的文档是重要和必要的。不管是整理思路、回顾备忘还是方便与别人交流，这张图都帮到我很多。我今后应该会把这个做法延续下去。</p>
<p>此外还遇到了一个难题，就是做实习答辩的PPT。不知道是从什么时候开始，我发现自己其实很烦把工作总结成文档。而烦就是因为我习惯于长篇大论。以前不管是做个什么笔记还是讲个什么报告，我都会弄到非常庞大，但当真正面对受众时，展示繁多的细节其实非常不好。</p>
<p>我的第一版PPT除了代码截图就是文字，搞了将近80页。然后就开始一点点地修改、删减。在改了将近两周后，终于删减到50页上下，并删掉了所有的代码相关的内容——代替掉它们的是我制作的一大批图片和动画。这个过程并不比把问题解决掉更轻松。</p>
<p>这时我也体会到了『只狼』的究极美感——把一种玩法做到极致，其实是在其他方面的减法做到了极致。</p>
<p>此时又刚好赶上<a href="https://gameinstitute.qq.com/tgdc/2022/">腾讯游戏开发者大会</a>。在独立游戏那一part有『去月球』『寻找天堂』的作者高瞰，因此我就听了一下。他提到：对于游戏复杂的那一方面，中国作者将其展示出来，外国作者将其藏在表面之下（大意）。其实一直听到的一个说法是，不能给受众展示太多难点，而需要假定受众从未接触过相关概念、也不具备非常强的学习能力。但我现在认为，没必要把观众当弱智，难度其实是可以有的，但一定要降低细节上的复杂度。</p>
<p>不过我觉得长篇大论也不是完全没有好处，举个例子就是在个人博客的年终总结环节。就像人老了可以看着信回味过去，但面对着电话机或许大脑一片空白。</p>
<p>（也算是得益于问题的繁琐，实习期间我把UE4 Slate渲染这部分的代码看得比较细致，这也帮助我在秋招找到了工作。其中的很多东西我从来没有搜索到过，因此我想要把这些知识总结成一系列文章，但一直没下手。去年我也开了《我看UE4源码》这个大坑，2023年希望能把欠的这些文章都写出来吧。）</p>
<p>虽然我一直觉得会这样想只是还没遭到毒打罢了，但慢慢的我确实有了“世上无难事”的想法。可能是连着通关了『空洞骑士』、『只狼』，也可能是自认为把实习任务完成得比较完整。用更有底气来形容或许更好一些？希望今后我也能解决越来越难的问题。</p>
<h4 id="其四-热浪和一切的终结"><a href="#其四-热浪和一切的终结" class="headerlink" title="其四 热浪和一切的终结"></a>其四 热浪和一切的终结</h4><p>在我线下实习的两个月里，上海都奇热无比。见过热，但没见过一个月每天都稳定40℃，确实是晒并且闷并且热，只要在室外就无处可躲。刚去的第三天，我跑来跑去找了一天房，就已经被晒出非常明显的色差了。</p>
<p>实习期间我也正好非常喜欢听<em><strong>春泥棒</strong></em>，觉得还挺应景。同时也希望能有个小偷来把夏天赶快偷走。</p>

    <div id="aplayer-JGpqCAkm" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="1810759765" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>上班的时候，小区附近经常找不到共享单车，顶着太阳沿路走15分钟找车，感觉非常难受。就算骑车，到了公司也是一身的汗。在工作日里，从家里到公司永远是最折磨的一段时间。</p>
<p>一天最晒的时间是中午饭点。7月刚去的时候，我在周末中午亲自去附近的店吃了一次饭，走在路上感到被晒得发疼。后面周末中午我就只吃外卖了。</p>
<p>但其实在室内也很不好过，因为我特别讨厌吹空调，而房间又非常小、只能直吹，所以我坐着用电脑的时候就开一会、关一会，睡觉前就直接关掉。但不开嫌热、开了嫌冷，怎么都不舒服。总之每天都要洗两次澡、洗一次衣服。</p>
<p>去之前我还有3天一跑步的习惯，但去了上海因为工作累、天气热，就减到只在每周末找一天晚上跑一次了。跑得也不多，每次都是从住的地方开始，沿着合川路跑到平阳路路口旁边的一个小公园，休息一会，然后骑共享单车或者直接走回住的地方。公园里老头老太太的娱乐方式竟然是组团吹西洋乐器，非常上流。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/sh_map.png"  / loading="lazy">

<p>大概是8月第二周，我当时正忙着准备转正的PPT，跑步也成了难得的减压方式。但这次刚跑了1km，就觉得异常得费力，还有点喘不上气。在一边转圈跑、一边等红绿灯的时候思考了一下，决定停下来。然后一停下来竟然头晕没劲、眼前发黑、心脏狂跳、呼吸困难，在旁边的桥上坐了10分钟才慢慢缓过来。这次跑步虽然是在晚上，但气温好像比前几次都高不少，我猜可能有37℃。高温确实是非常可怕的。</p>
<p>8月23日是我转正答辩的日子。很巧，这一天也是上海“高温退场”的分界线。答辩完的那天晚上，我走在路上感到非常清凉，有种一切都结束了的感觉。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/4.jpg"  / loading="lazy">

<p>（关于运动，9月回家后天气很适宜，我就继续在坚持跑步。但10月刚去学校没几天竟然下雪了😓。直到今天就没再动一次了。）</p>
<h4 id="其五-之后"><a href="#其五-之后" class="headerlink" title="其五 之后"></a>其五 之后</h4><p>最后记录一下我去过的地方吧。除了偶尔在住处附近骑车，我并没有独自去任何地方。但因为表哥也在上海，并且经常邀请我出去，我得以感受一下大城市的生活。表哥非常有生活经验、非常靠谱，可以把行程规划得突出一个性价比高。成为立派的成年人确实还需要多方面的修行啊。我觉得他的狗运是不错的，找工作赶上21年就业形势非常好的时候，钱又多又没被裁应届，现在工作和生活都很舒服（至少从外人视角来看是这样）。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/6.png" style="zoom: 80%;" / loading="lazy">

<p>还有就是在吃饭逛商场的时候，头一次去主机游戏店里扭了个蛋，是魔法未来2020冬日祭系列。38块一个（而且因为这个机器正好坏了是让老板直接拿出来的，还没了扭的乐趣），反正不是我能心安理得地大玩特玩的价格。不过扭到的东西（是个软胶吊饰）我确实挺喜欢的，基本上从来都没舍得拿出来过。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/9.png" style="zoom: 100%;" / loading="lazy">



<p>临走的时候我在住处附近买了点点心（也不知道是不是地道老上海），也把公司发的端午礼盒带回家了。其实端午节的时候我还没入职来着。后面到了中秋节，虽然我离职了，但公司也把中秋月饼给我寄过来了。总之我也是头一次在这种意义上收到这些东西。以上的东西爸妈都挺高兴地消化了，其中端午礼盒我一直没打开，里面竟然还有一瓶什么果子的酒，家人觉得挺新鲜的。</p>
<p>总之，不管怎么说，我还是觉得能用劳动换到钱本身就是一件值得高兴的事情。</p>
<p>在有了工作经验之后，我觉得我还是有了一些心态上的改变的。除了前面说的对未知技术更自信了，还有可能就是在各方面都更理直气壮了一些。如果在本科毕业后，甚至在更早的时候去实习、积累一点社会经验，大概会对心理层面的成长更有益吧。不过既然我已经轻轻松松地过到现在了，也就无所谓了。</p>
<p>至于上海，我也没什么特别的想法。日子该咋过还是咋过，在这个层面上跟我之前呆过的武汉之类的地方也没什么两样。后面秋招我也投了一些上海的企业，但都没什么下文。下次再去就不知道是什么时候了~</p>
<h3 id="再一次——HADES"><a href="#再一次——HADES" class="headerlink" title="再一次——HADES"></a>再一次——HADES</h3><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/bgm_hades.png"  / loading="lazy">

<p>9月当然就是秋招了。今年腾讯的秋招基本上等于没开，转正当然也是惨不忍睹。因此刚在实习期间完成了一项完整的任务，9月又要从零开始准备秋招这件事情了。</p>
<p>关于秋招的经历、经验总结，我已经写了一篇文章发到牛客上了（<a href="https://www.nowcoder.com/discuss/428252230693683200">点击链接</a>），或许也会在博客里存一份并写一些个人博客特典。这篇文章被收藏的次数还挺多的，很多人找我私信聊聊或者问问，我也发现大部分人确实都不算顺利。</p>
<p>实习那阵子每天下班的路上，我和室友经常聊这一届有多倒霉，跟上一届完全没得比。但后面又觉得自己其实也还好了，毕竟还有空间可以向下兼容，大厂不招人那就去试试小厂。如果自己回到2020年的状态，成为了2023年毕业的双非本科生，那就真的没得兼容了。（不过从功利上，最好的还是2020年毕业就直接工作三年。但毕竟考了研可以多玩好几年，所以也没什么。）</p>
<p>现在选择游戏行业确实是选了既卷又不稳定的一条路，需要梦想、热情之类的东西加持。这些东西其实人人都看得清，就看愿不愿意了。</p>
<p>9月其实主要在笔试，也没什么紧张感，就在爽玩『HADES』。游戏本身也不用多评价什么，就是好看好听又好玩。好像在哪里看到说这游戏的3D画面其实是完全2D制作的，让我对工作量稍微地震惊了一下。确实怪不得玩起来这么流畅。</p>
<p>『吸血鬼幸存者』、『HADES』这种roguelike游戏的不断复活——“再一次”的尝试——其实就是玩数值。每一次的尝试都带来了数值上或者构筑上的一些改变，因此下一次就产生了新的体验和新的乐趣。再一次，是为了在一次次的成功（美好的体验）中寻找更爽的一次。</p>
<p>而『只狼』的再一次，其实就是单纯的一次次失败。在一次次的失败中，数值没有任何变化，或者说没有数值来度量玩家的水平，游戏角色只是游戏角色，真正变强的其实是玩家。最终，玩家会到达唯一一次的成功，获得无与伦比的成就感。</p>
<p>秋招也是一样的。一次次笔试一次次面试，挂了又来新的。再一次的机会或许永远都有吧。反正在一次次的尝试中，我也分不清楚这是前者还是后者了。应该两个都不是吧，“成就感”这种东西在秋招的任何一个阶段都是完全不会出现的。没找到时，觉得累并且难受；找到之后，觉得累并且提心吊胆。</p>
<p><strong>HADES：80&#x2F;100</strong> </p>
<h3 id="真正强大的内心——Persona-5-the-Royal"><a href="#真正强大的内心——Persona-5-the-Royal" class="headerlink" title="真正强大的内心——Persona 5 the Royal"></a>真正强大的内心——Persona 5 the Royal</h3><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/bgm_persona5royal.png"  / loading="lazy">

<p>2022年『P5R』登录了PC和XBOX，正式成为了全平台游戏。10月下旬秋招结束后，我就马不停蹄地开始玩了。2021年我玩『P4G』用了50h左右，所以理所应当地以为『P5R』也能用一个来月打完。结果没想到前后两作体量竟然差距如此之大，我花了139h，到12月底才把一周目打完。（其实是打了全成就的，但不知道为啥steam关于印象空间的成就都没给我解锁，有点难受。）</p>

    <div id="aplayer-PzaVUAPB" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="454224836" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div> 

<p>『P4G』的色调是金黄色，主人公从大城市来到小镇，度过了一段追求真理的金色时光。而『P5R』的主色调是鲜红色，主题也是更现实、更沉重的“反抗”。主人公从小城市来到了国际大都市东京，开启了一段轰轰烈烈的改造社会之旅。</p>
<p>我其实是非常喜欢这种主题的，因为我总觉得设定一个幻想世界、再把人物和故事放进去，虽然也不错，但写起来肯定更简单一些，被大家接受的可能性也更高一些。我认为立足于现实的作品才是更有难度、更有意义的好作品。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/p5r_3.jpg"  / loading="lazy">

<p>跟『P4G』相比，『P5R』玩起来肯定舒服很多。除了技术升级带来的3D画质进步，游戏里日常能做的事情和能去的地方多了很多，战斗时可以用换手等乱七八糟的技能、体验流畅爽快，迷宫也都经过了精心设计，人格面具练级、刷社群等级也都有相应的方便的技能，没有一样会给玩家添堵。</p>
<p>『P5R』也是真正意义上的视听盛宴。10多年前的『P4G』的设计水平在我看来已经很时尚了，『P5R』则更加灵动扎眼，高饱和、大色块、酷炫动画比比皆是，潮到不行。听觉上，虽然不懂细节，但爵士、摇滚的使用相当普遍，贝斯如雷贯耳，很有优雅又帅气的怪盗团气质，也凸显了反抗的主题。一系列音乐延续了一贯的高质量，并且也有<em><strong>Life Will Change</strong></em>这种知名斩杀神曲，伴随着剧情，在游戏中听到真的要high到不行了。跟邪恶的大人做个了断吧！（我觉得单独听倒是一般。而且这首歌本来就非常著名，就不在博客里加了。）</p>
<p>与伙伴的羁绊是『P4G』最关键的要素之一，在『P5R』中这个要素依然存在并且升华。主角团的人设也是一大亮点。我曾经评价过年代久远的『逆转裁判』的人设有点样板化，太俗气；『弹丸论破』堆砌要素，太刻意。但这都是难得的能做明白风格的作品了。即使在今天，把萌点组合一下而诞生的角色也多如牛毛。『P4G』虽然不错，但也不免有像Tatsumi Kanji这种略显刻意的反差不良人设，和用男同🤺一掩而过的不自然感。</p>
<p>我认为『P5R』的角色就做得非常好。虽然也得益于强大剧本的支撑，但即使脱离主线故事，角色的设定也相当出彩。分析其原因，我认为一方面是跟塔罗牌的结合非常紧密、不容易出错，另一方面则是把更多平凡甚至朴实的要素加入了角色中，让角色产生了既现实又幻想、既朴素又华丽的观感。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/p5r_2.jpg"  / loading="lazy">

<p>对我来说『P5R』并不是没有缺点，那就是可笑到可怜的侦探这么一个角色。但随着慢慢玩下去，我认为这或许也是『P5R』给我的一种体验。『P5R』像每个人的第二人生，拥有无数可以关注的正面因素，拥有各色的可能性，也拥有一定想去实现的愿望。从下定决心、向着目标心无旁骛的那一刻开始，连叫声都发不出的蝼蚁就不可能再进入主角的视野了。如同外星人看待人类一样，人会在意草履虫的行为吗？会关注微生物权吗？我对『P5R』的体验和回忆只有一个个美好的瞬间。</p>
<p>因为『P5R』立足于现实，好玩，并且长，在将近3个月的时间里确实成为了我的第二人生。或者说至今也没完全回过神来，今后大概也会回味很久。</p>

    <div id="aplayer-GPKdYCyN" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="454224842" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div> 

<p>说到人生，就想多写几句。不管是<em><strong>Last Surprise</strong></em>还是<em><strong>Life Will Change</strong></em>还是<em><strong>I Believe</strong></em>，我认为它们是最能代表主角团的印象曲，从歌名开始，就不遗余力地展示自信、潇洒、华丽的精神面貌，也代表了跟阴暗面斗争到底的反抗精神。那么，<em><strong>Beneath the Mask</strong></em>作为华丽外表的另一面，就代表了每个人既重要、又不可避免的与自己独处的时间，也是游戏本体比起宣传片来说多出来的一部分体验。前文曾提到『P4G』的核心表达之一：与伙伴们的羁绊可以斩断一切虚伪，凝聚成打开真相大门的钥匙。『P5R』虽然也将伙伴、羁绊等要素延续并升华，但整个打完后，我却更多地感受到了主角（或者自己）的存在。</p>
<p>可能是因为简单体验过大城市的迷失生活，也可能是对游戏中展现的一些社会阴暗面多少感同身受，我总觉得『P5R』有一种不能言说的独特的气质。想了很久或许有了一点眉目：玩家能够选择大部分时间的行动方式，这让Joker的行动具有了强大的自驱性或者说合理性。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/p5r_6.jpg"  / loading="lazy">

<p>在具有主角成长要素的其他大部分作品中，甚至在『P4G』中，我觉得主角其实在很大程度上是被推着走。跟人建立联系，拓宽边界，要么太过巧合、要么镇子就这么大，总之都是相对被动的。观众带入不了鸣人的惊天大后台和孙悟空的赛亚人血统，也没有校园动画主角的受欢迎程度。大部分人不是那些太过独特的主角。</p>
<p>但『P5R』不一样。主角虽然有操控面具的特殊能力，但在日常生活中，好像也没那么特别（？）。投身于大城市东京的茫茫人海中，只有强大的内心可以依靠，只有从自身开始才能向外拓展。</p>
<p>确实如此。『P5R』一直在讲扩展自己的生活，但一切的扩展都要以自己为中心。只有真正面对自己的内心，才能将真情实感延续给他人。就算是叱诧风云的怪盗团团长，在一切结束时，也要回到咖啡店的二楼，跟自己独处，想想自己走到了哪一步。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/p5r_5.jpg"  / loading="lazy">

<p>为了方便今后回忆和整理，今年我在bangumi上补写了对一众动画和游戏的评价。在过程中我发现，有一些作品确实塑造了我的一部分人格、改变了我的人生（这么说挺萌二的但确实如此）。每个人都是这样的，如果喜欢一个角色、一部作品，那么多多少少的就会向他们靠拢。</p>
<p>『P4G』是我2021年的“年度最喜欢”，如果硬说它让我明白了什么，那就是人要无止境地向内聚合，求真、求美，寻找金色的真理，开启金色的羁绊，度过金色的时光。『P5R』则让刚有了一点社会经历的我见识到了人在另一个层面的能力，那就是不断向外延申自己的生活。</p>
<p>不管是出于趋利避害的考量，还是胆小怕事的性格所致，我从小到大都是游离于边缘的沉默人。分到我头上的事情会尽力做好，但也仅限于此，一直如此。虽然怡然自得，但也在很多时候感到力不从心。『P5R』让我看到了有主见的人生。</p>
<p>在教室里耍帅，跟精明的长辈或面相不好的大叔建立联系，解决朋友生活中的难题，准备一场跟新岛真的完美白色情人节……或者总结为以自己的意志度过一生，这些都依赖于游戏中的五维属性：知识、胆量、灵巧、温柔、魅力。在此之上，我认为它们有一个共同的本源——勇气。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/p5r_4.jpg"  / loading="lazy">

<p>又到了我不知道该如何形容的时候了。也许不应该叫它勇气——它不是勇敢地站出来，主动承担责任、主动建立联系、主动干更多活的这种肤浅的事情，而是一种内心上的强大。或许是对自己的足够自信，或许是不为外物所动、击穿一切的坚定信念。它并不在性格上的一端，跟内向或者安静也不冲突，但大概是我非常缺乏的一种能力。</p>

    <div id="aplayer-poaqLWvr" class="aplayer aplayer-tag-marker meting-tag-marker"
         data-id="1403087325" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"
    ></div>

<p>回到游戏上吧。『P5R』的体验、体量，讲的故事，用的设定，视听的质量，我也都觉得已经做到极限了。红黄蓝三原色也已经用光了。那么P6到底要讲什么？想到这里，我其实有一些担心，但更多的还是期待。期待玩到新游戏，也期待自己的审美或者认知下一次被突破，期待自己承担越来越大的责任。</p>
<p>我非常高兴能在一段不忙工作、不忙找工作、不忙毕业的时间里，完整、投入地体验了『P5R』。以我目前的能力和眼界，我依然把成为高质量的螺丝钉当作主要人生目标。但玩完『P5R』后，我可能在心中种下了一个种子。高中时期看动画时埋下的种子已经慢慢发芽了，有朝一日我或许也能像Joker一样，可以真正“随心所欲”地主宰生活。</p>
<p>至于游戏最后的经典告别，虽然笔墨更重、劲很大，但不知为何，我并不像玩『P4G』那样感到伤感。大概我在我眼里，鸣上悠才是真正属于小镇子的，而雨宫莲肯定会走向更广阔的世界、不断延伸自己的生活吧。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/p5r_1.png" / loading="lazy">

<blockquote>
<p>“应当这么说，这首曲子是最能体现p5r中“皇家”这一风格的。从容而大气、优雅且华丽，这是这首曲子最直观的听感。仿佛夜幕下，大衣尾摆随风飘动的Joker被聚光灯环绕着走向舞台中央，而肮脏的大人才是小巷里鬼鬼祟祟的盗贼。”——节选自<a href="https://music.163.com/#/song?id=1403087325">网易云音乐<em><strong>Take Over</strong></em>评论区</a></p>
</blockquote>
<p><strong>P5R：94&#x2F;100，年度最喜欢，年度最佳</strong> </p>
<h3 id="2022年的音游、手游、网游、漫画"><a href="#2022年的音游、手游、网游、漫画" class="headerlink" title="2022年的音游、手游、网游、漫画"></a>2022年的音游、手游、网游、漫画</h3><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/bgm_rhythmdoctor.png"  / loading="lazy">

<p>每年一个音游，今年是『节奏医生』。这应该也是我玩过的最喜欢的音游了，人情味很足。我最喜欢的是关卡2-3的bgm<em><strong>缭绕</strong></em>，音乐软件上没版权，就放一下<a href="https://www.bilibili.com/video/BV1PZ4y1P7yE?p=12">视频链接</a>吧。</p>
<p>今年的手游，『原神』基本上是弃坑了，剧情又臭又长，新地图的探索欲望也没了，最无语的是角色越出越怪，连着几个版本，池子里不是男的就是大妈和萝莉，总之养成上也没什么玩的动力了，就弃坑了。</p>
<p>后面在玩『无期迷途』，起码到目前感觉还是不错的，审美在线，非常休闲，抽卡友好、基本上不用花钱，最重要的是以我的造纸竟然能看明白文案在表达什么，比起那些“你文化程度再高你也听不懂的”的手游确实很让我有好感。游戏玩法上是有缺陷的，虽然看上去比『明日方舟』多了移动的操作，但为了限制难度，在局外拉高养成难度、在局内又搞成数值游戏了。但好在非常之休闲，缺点都懒得去关注了，而且也没在扫荡之类的地方故意恶心玩家，总之一句话就是不错。</p>
<p>今年在网游上把那些FPS都断了，专心『LOL』了。玩起来没什么乐趣，但算一下都玩了8年了，已经形成习惯了。希望有朝一日能退坑。</p>
<p>2022在我随缘看的一些漫画里，『再见绘梨』是我最喜欢的。去年就写博文说要单独写一些关于藤本树作品的感想，但可能是我的水平还评价不了他的作品吧，每次看都觉得如梦如幻，感受难以言说。想以存在和虚无、时代情绪、生命和爱的价值等等角度来写一下我对这些作品的看法，或者用篇幅、自由的另一面是负担、名场面等等角度分析一下藤本树的创作特点，甚至备忘录里记的关键词都写了改、改了删，但还是觉得词不达意，总之就继续放一放吧。</p>
<h3 id="四叠半时光机布鲁斯"><a href="#四叠半时光机布鲁斯" class="headerlink" title="四叠半时光机布鲁斯"></a>四叠半时光机布鲁斯</h3><p>在从青岛去上海的高铁上我读完了『四叠半时光机布鲁斯』。这也是今年完整看完的唯一一本书。</p>
<p>书本身是很轻松愉快的，但让我回想了不少跟这个系列有关的回忆。高中的时候，因为室友的安利我第一次看了『四叠半神话大系』的动画；在大学期间又看了一遍动画；2020年底我高价买了一本绝版的中文译版然后把原著看了一遍（图左）；2022年『四叠半神话大系』和新出的『四叠半时光机布鲁斯』又一起新印了一版中文版（图右），我第一时间买了并且在火车上把新作看完了。（实际上买了一版阅读一版传教，不过一直没送出去就是了。）</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/blogimg/s-2022/4.png" style="zoom: 80%;" / loading="lazy">

<p>第一次看时，可能更多感到了对未来生活的憧憬。但在马上要去上班的时间点去回想，就会觉得自己一方面抓住了一些“可能性”和“良机”，另一方面随着年龄增大、走向社会，身上潜在的“可能性”已经越来越少了。虽然这并不是坏事，但还是有点惆怅。</p>
<p>不知道是因为回忆，还是因为是在通往未知生活的火车上，感觉真的像坐了一把时光机一样。</p>
<h3 id="新年"><a href="#新年" class="headerlink" title="新年"></a>新年</h3><p>新年总要有一些新的尝试。我希望能：</p>
<ul>
<li>多输入。更多地玩好游戏，早日达到追着玩的高度；好的动画和书能看就看，提高审美水平</li>
<li>多输出。比这几年的低迷有进步就好，并不难做到</li>
<li>顺利毕业。开启新生活</li>
<li>努力工作。以成为立派职场人为目标而努力，这次争取转正成功</li>
<li>心直口快。忠于自己，少一些和稀泥，多一些表达鲜明</li>
<li>多运动。这是必要的</li>
</ul>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活小记</tag>
        <tag>年终总结</tag>
      </tags>
  </entry>
  <entry>
    <title>【我看UE源码】（1）从UE4的一处下行Cast，到初步理解C++类型转换</title>
    <url>/2022/09/07/ue-1/</url>
    <content><![CDATA[<blockquote>
<p>叠甲：仅为记录个人学习过程；内容很多是从别处看来的，在参考中列出；缺乏实践经验，有些地方纸上谈兵；难免有错误，敬请指出。</p>
<p>系列目录：<a href="https://acbgzm.github.io/2022/06/18/ue-0/">【我看UE源码】（0）系列开篇 &amp; 目录</a></p>
</blockquote>
<h4 id="UE4-dynamic-cast一则"><a href="#UE4-dynamic-cast一则" class="headerlink" title="UE4 dynamic_cast一则"></a>UE4 dynamic_cast一则</h4><p>GEngine是一个UEngine类型的指针，它是所有引擎类的抽象基类，用于管理一些对编辑器或游戏必要的系统：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">&#x2F;**
* Global engine pointer. Can be 0 so don&#39;t use without checking.
*&#x2F;
ENGINE_API UEngine*	GEngine &#x3D; NULL;</code></pre>



<p>它有一个派生类UGameEngine，用来管理支持游戏的核心系统，包含一些指针比如GameViewportWindow等：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">&#x2F;**
 * Engine that manages core systems that enable a game.
 *&#x2F;
UCLASS(config&#x3D;Engine, transient)
class ENGINE_API UGameEngine
	: public UEngine
&#123;
    &#x2F;&#x2F; ...
public:      
	&#x2F;** The game viewport window *&#x2F;
	TWeakPtr&lt;class SWindow&gt; GameViewportWindow;
	&#x2F;** The primary scene viewport *&#x2F;
	TSharedPtr&lt;class FSceneViewport&gt; SceneViewport;
	&#x2F;** The game viewport widget *&#x2F;
	TSharedPtr&lt;class SViewport&gt; GameViewportWidget;
&#125;;</code></pre>



<p>在使用的时候，新建一个子类指针、把父类指针GEngine转成子类UGameEngine类型的指针，这种情况下为什么能访问到子类单独定义的数据呢？</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">UGameEngine* GameEngine &#x3D; Cast&lt;UGameEngine&gt;(GEngine);
OutSceneViewport &#x3D; GameEngine-&gt;SceneViewport;
OutInputWindow &#x3D; GameEngine-&gt;GameViewportWindow;</code></pre>



<p>这其实就是dynamic_cast，把基类指针或引用转换成派生类指针或引用，目的是使用派生类的非虚函数、访问派生类的成员变量。</p>
<p>C++ 11有四种类型转换，在此结合使用场景简单总结一下。</p>
<h4 id="①const-cast"><a href="#①const-cast" class="headerlink" title="①const_cast"></a>①const_cast</h4><h5 id="修改属性和编译器优化"><a href="#修改属性和编译器优化" class="headerlink" title="修改属性和编译器优化"></a>修改属性和编译器优化</h5><p>const_cast用于修改const或volatile属性。一般是用来去掉const属性：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">const int a &#x3D; 1;

&#x2F;&#x2F; int* pa &#x3D; &amp;a;	&#x2F;&#x2F; 错误，const int*类型的值不能用于初始化int*类型的实体
int* pa &#x3D; const_cast&lt;int*&gt;(&amp;a);	&#x2F;&#x2F; 正确，去掉了const常量a的const属性，可以赋值给pa

&#x2F;&#x2F; int&amp; ra &#x3D; a;		&#x2F;&#x2F; 错误
int&amp; ra &#x3D; const_cast&lt;int&amp;&gt;(a);	&#x2F;&#x2F; 正确</code></pre>



<p>通过以下代码，尝试给指向“解除const属性的值”的指针和引用赋值：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">const int a &#x3D; 1;

int* pa &#x3D; const_cast&lt;int*&gt;(&amp;a);
*pa &#x3D; 10;
std::cout &lt;&lt; &quot;a:&quot; &lt;&lt; &amp;a &lt;&lt; &#39; &#39; &lt;&lt; a &lt;&lt; std::endl;
std::cout &lt;&lt; &quot;pa:&quot; &lt;&lt; pa &lt;&lt; &#39; &#39; &lt;&lt; *pa &lt;&lt; std::endl;

std::cout &lt;&lt; std::endl;

int&amp; ra &#x3D; const_cast&lt;int&amp;&gt;(a);
ra &#x3D; 5;
std::cout &lt;&lt; &quot;a:&quot; &lt;&lt; &amp;a &lt;&lt; &#39; &#39; &lt;&lt; a &lt;&lt; std::endl;
std::cout &lt;&lt; &quot;pa:&quot; &lt;&lt; pa &lt;&lt; &#39; &#39; &lt;&lt; *pa &lt;&lt; std::endl;
std::cout &lt;&lt; &quot;ra:&quot; &lt;&lt; &amp;ra &lt;&lt; &#39; &#39; &lt;&lt; ra &lt;&lt; std::endl;</code></pre>

<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ue-img/1.png"/ loading="lazy">

<p>首先，const_cast说是“去掉const属性”，作用的对象不是a，而是指向a的指针或引用。<u>不会修改a本身的const属性，而是把指向常量的指针和引用去掉了const属性、提供了修改a的一个新接口</u>。</p>
<p>然后，输出结果有些奇怪：</p>
<ul>
<li>第一，指针、引用转换const属性前后，指向的内存是一样的。把常量指针&#x2F;引用转换成非常量的指针&#x2F;引用，仍然指向原来的对象，理解为只是看待该内存的方式变了</li>
<li>第二，通过指针和引用修改原值，发现原值依然没变（好像是保留了const属性），但去掉const属性的指针和引用之间就会相互影响</li>
</ul>
<p>既然地址相同，值也应该是相同的，可能是编译器优化的原因。这里的优化可能是通过符号表，详情见下面专门讲const内存的小节。</p>
<p>加上volatile，告诉编译器a的值随时可能发生变化（虽然它是const），不让编译器优化，每次都从内存中取值，就发现相同的地址的值都一样了：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">volatile const int a &#x3D; 1;
int* pa &#x3D; const_cast&lt;int*&gt;(&amp;a);

*pa &#x3D; 10;
std::cout &lt;&lt; &quot;a:&quot; &lt;&lt; &amp;a &lt;&lt; &#39; &#39; &lt;&lt; a &lt;&lt; std::endl;
std::cout &lt;&lt; &quot;pa:&quot; &lt;&lt; pa &lt;&lt; &#39; &#39; &lt;&lt; *pa &lt;&lt; std::endl;

std::cout &lt;&lt; std::endl;


int&amp; ra &#x3D; const_cast&lt;int&amp;&gt;(a);
ra &#x3D; 5;
std::cout &lt;&lt; &quot;a:&quot; &lt;&lt; &amp;a &lt;&lt; &#39; &#39; &lt;&lt; a &lt;&lt; std::endl;
std::cout &lt;&lt; &quot;pa:&quot; &lt;&lt; pa &lt;&lt; &#39; &#39; &lt;&lt; *pa &lt;&lt; std::endl;
std::cout &lt;&lt; &quot;ra:&quot; &lt;&lt; &amp;ra &lt;&lt; &#39; &#39; &lt;&lt; ra &lt;&lt; std::endl;</code></pre>

<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ue-img/5.png"/ loading="lazy">



<h5 id="const和volatile"><a href="#const和volatile" class="headerlink" title="const和volatile"></a>const和volatile</h5><p>上面的例子，既然已经给变量加上了const属性，还能给它加上volatile属性吗？有意义吗？</p>
<p>首先，它们各自的含义并不是绝对的：</p>
<ul>
<li>const含义是“请作为常量使用”，对于局部const变量，可以理解为加了一个注释，表示一般不希望在程序中修改它的值。并非“这肯定是一个常量”，也并不存在常量区</li>
<li>volatile的含义是“编译器不要自以为是地优化，这个值随时可能改变”。并非“你可以修改这个值”</li>
</ul>
<p>然后，它们的作用和起作用的阶段有所不同：</p>
<ul>
<li>const只在编译期有用，在运行期没用。在编译时，const保证其修饰的变量在范围内没有被直接从值上修改；在运行期，变量的值被改变就不受const限制了</li>
<li>volatile在编译器和运行期都有用。在编译期，告诉编译器不要优化这个变量；在运行期，每次用到这个变量，都从内存中取该变量的值</li>
</ul>
<p>因此，const和volatile同时修饰一个变量：</p>
<ul>
<li>可以，volatile并不是non-const的含义，它们之间不冲突</li>
<li>有意义，表示变量在程序编译期不能被修改、不能被优化，运行期可能修改，每次用到时都去内存读数据，避免出错</li>
</ul>
<blockquote>
<p>编译器一般不为const变量分配内存，而是将它保存在符号表中，这使得它成为一个编译期间的值，没有了存储和读内存的操作。</p>
<p>volatile的作用是告诉编译器，i是随时可能发生变化的，每次使用它的时候必须从内存中取出i的值。</p>
<p>——《C语言深度剖析》</p>
</blockquote>
<h5 id="打印volatile变量的地址"><a href="#打印volatile变量的地址" class="headerlink" title="打印volatile变量的地址"></a>打印volatile变量的地址</h5><p>上图中，为啥打印volatile变量的地址，结果是1？可以看看<a href="https://stackoverflow.com/questions/8239262/why-is-the-address-of-this-volatile-variable-always-at-1">这个帖子（最高赞好像还答错了）</a>。简单来说，ostream重载了<code>const void*</code>的<code>&lt;&lt;</code>运算符，对于一般的指针<code>T*</code>会隐式转换成<code>const void*</code>来输出。但volatile变量的指针不会隐式转换成指针，而被认作了bool变量输出1。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ue-img/2.png"/ loading="lazy">

<p>那么为了用<code>&lt;&lt;</code>输出volatile变量的地址，就需要显式转换一下。使用static_cast是不行的，会报错：static_cast无法丢掉常量或其他类型限定符。</p>
<p>因此又回过头来，使用本小节讲述的对象：const_cast，去掉变量的volatile属性，转换成普通指针<code>T*</code>、再被隐式转换成<code>const void*</code>来输出地址。</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">volatile int b &#x3D; 2;
std::cout &lt;&lt; const_cast&lt;int*&gt;(&amp;b) &lt;&lt; std::endl;

std::cout &lt;&lt; std::endl;

volatile const int a &#x3D; 1;
int* pa &#x3D; const_cast&lt;int*&gt;(&amp;a);
*pa &#x3D; 10;
std::cout &lt;&lt; &quot;a:&quot; &lt;&lt; const_cast&lt;int*&gt;(&amp;a) &lt;&lt; &#39; &#39; &lt;&lt; a &lt;&lt; std::endl;
std::cout &lt;&lt; &quot;pa:&quot; &lt;&lt; pa &lt;&lt; &#39; &#39; &lt;&lt; *pa &lt;&lt; std::endl;

std::cout &lt;&lt; std::endl;

int&amp; ra &#x3D; const_cast&lt;int&amp;&gt;(a);
ra &#x3D; 5;
std::cout &lt;&lt; &quot;a:&quot; &lt;&lt; const_cast&lt;int*&gt;(&amp;a) &lt;&lt; &#39; &#39; &lt;&lt; a &lt;&lt; std::endl;
std::cout &lt;&lt; &quot;pa:&quot; &lt;&lt; pa &lt;&lt; &#39; &#39; &lt;&lt; *pa &lt;&lt; std::endl;
std::cout &lt;&lt; &quot;ra:&quot; &lt;&lt; &amp;ra &lt;&lt; &#39; &#39; &lt;&lt; ra &lt;&lt; std::endl;</code></pre>

<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ue-img/4.png"/ loading="lazy">



<p>或者，<code>&lt;&lt;</code>运算符用不了可以用<code>printf</code>：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">volatile const int a &#x3D; 1;

std::cout &lt;&lt; &amp;a &lt;&lt; std::endl;		&#x2F;&#x2F; 输出1
printf(&quot;%p\n&quot;, &amp;a);		&#x2F;&#x2F; 输出地址
std::cout &lt;&lt; const_cast&lt;int*&gt;(&amp;b) &lt;&lt; std::endl;		&#x2F;&#x2F; 输出地址</code></pre>



<p>顺带一提，普通类型转换成const类型具体是什么规则我也不是很懂。在UE4里，我新建了一个<code>TMap&lt;const SWidget*, int32&gt;</code>，那就必须给它加入<code>const</code>类型的<code>SWidget*</code>，否则就会有模板报错。当然也必须就是这种原生指针，其他封装后的智能指针也不行。</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">TMap&lt;const SWidget*, int32&gt; WidgetIndexMap;

&#x2F;&#x2F; ...
void function(const SWidget* Widget, ...)
&#123;
    int32 Index &#x3D; XXXLists.Add(NewList);
	WidgetIndexMap.Emplace(Widget, Index);
&#125;

&#x2F;&#x2F; ...
const SWidget* WidgetPtr &#x3D; ...;
if(WidgetIndexMap.Contains(WidgetPtr))
&#123;
	int32 RnederBatchIdnex &#x3D; WidgetIndexMap[WidgetPtr];
&#125;</code></pre>



<h5 id="补充：const内存和其他"><a href="#补充：const内存和其他" class="headerlink" title="补充：const内存和其他"></a>补充：const内存和其他</h5><p>对于上述通过指针、引用（也就是地址）来修改const变量的值，只对局部变量有效。</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">const int ga &#x3D; 1;

int main()
&#123;
    int* pga &#x3D; const_cast&lt;int*&gt;(&amp;ga);
	*pga &#x3D; 10;		&#x2F;&#x2F; 运行时报错
	std::cout &lt;&lt; &quot;a:&quot; &lt;&lt; const_cast&lt;int*&gt;(&amp;a) &lt;&lt; &#39; &#39; &lt;&lt; a &lt;&lt; std::endl;
    
	int&amp; rga &#x3D; const_cast&lt;int&amp;&gt;(ga);
	rga &#x3D; 20;		&#x2F;&#x2F; 运行时报错
	std::cout &lt;&lt; &quot;a:&quot; &lt;&lt; const_cast&lt;int*&gt;(&amp;a) &lt;&lt; &#39; &#39; &lt;&lt; a &lt;&lt; std::endl;
&#125;</code></pre>

<p>如果进行上面代码中的操作，试图修改全局const变量的值，编译器不报错，但运行时在指针、引用赋值的地方报错：“引发了异常：写入访问权限冲突”。</p>
<p>这是由C++ const的内存分配方式导致的。关于内存、const的更多信息、const引用传参，见<a href="https://acbgzm.github.io/2022/09/07/ue-2/">【我看UE源码】（2）源码阅读支线之如何理解C++的const关键字？</a>。</p>
<h5 id="const-cast的使用"><a href="#const-cast的使用" class="headerlink" title="const_cast的使用"></a>const_cast的使用</h5><p>被const_cast转换的类型<strong>必须是指针、引用</strong>或指向对象类型成员的指针。也就是，<u>不能把基本类型变量和对象转换成非常量</u>：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">const Student stu1;
Student stu2 &#x3D; const_cast&lt;Student&gt;(stu1);	&#x2F;&#x2F; 错误

const int x &#x3D; 10;
int y &#x3D; const_cast&lt;int&gt;(i);	&#x2F;&#x2F; 错误</code></pre>

<p>使用const_cast去掉const属性，并不是真的想改变原类型的const属性，只是又提供了一个接口（指针或引用）。因为前文（补充的文章）提到，对全局const，使用地址也无法修改其值；对于局部const，可以看作是一个注释，可以用接口修改其值。</p>
<h4 id="②reinterpret-cast"><a href="#②reinterpret-cast" class="headerlink" title="②reinterpret_cast"></a>②reinterpret_cast</h4><h5 id="灵活但不安全的转换"><a href="#灵活但不安全的转换" class="headerlink" title="灵活但不安全的转换"></a>灵活但不安全的转换</h5><p>从单词意思上看，它的作用是把相同内存中的数据“重新解释”，概念上像union。比如进行<u>不同类型的指针之间、不同类型的引用之间、指针和能容纳指针的整数类型之间的转换</u>。转换时执行逐个bit复制的操作。</p>
<p>这种转换类型提供了很强的灵活性，但安全性需要程序员来保证。</p>
<p>下面的案例复述自<a href="https://zhuanlan.zhihu.com/p/33040213">这篇文章</a>。</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">int num &#x3D; 0x00636261;
int* pnum &#x3D; &amp;num;
char* pstr &#x3D; reinterpret_cast&lt;char*&gt;(pnum);	&#x2F;&#x2F; 把int*重新看成char*
std::cout &lt;&lt; &quot;pnum: &quot; &lt;&lt; pnum &lt;&lt; &quot;, &quot; &lt;&lt; std::hex &lt;&lt; *pnum &lt;&lt; std::endl;
std::cout &lt;&lt; &quot;pstr: &quot; &lt;&lt; static_cast&lt;void*&gt;(pstr) &lt;&lt; &quot;, &quot; &lt;&lt; pstr &lt;&lt; std::endl;	&#x2F;&#x2F; 对于char*指针，直接输出会输出字符串，要转成void*输出它指向的地址</code></pre>

<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ue-img/8.png"/ loading="lazy">

<p>在上面的程序片段中，首先定义int变量num，用十六进制值0x00636261初始化；然后定义int* pnum指向num；然后使用reinterpret_cast，把int*类型的指针pnum转换成了char*类型的指针pstr。</p>
<p>对比输出两个指针，发现它们的地址是相同的，也就是reinterpret_cast不改变操作对象的值，而是改变了看待同一块内存区域的方式。</p>
<blockquote>
<p>一个指向字符串的指针是如何地与一个指向整数的指针或一个指向其他自定义类型对象的指针有所不同呢？从内存需求的观点来说，没有什么不同！它们三个都需要足够的内存（并且是相同大小的内存）来放置一个机器地址。指向不同类型之各指针间的差异，既不在其指针表示法不同，也不在其内容（代表一个地址）不同，而是在其所寻址出来的对象类型不同。也就是说，指针类型会教导编译器如何解释某个特定地址中的内存内容及其大小。</p>
<p>——《深度探索C++对象模型》</p>
</blockquote>
<p>如图：</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ue-img/7.png" style="zoom:50%;" / loading="lazy"></li>
</ul>
<p>使用reinterpret_cast把pnum从int*转变成char*类型的指针，并初始化pstr后，pstr也指向num的内存区域，并且按照指针的类型（char*）对应的规则来解读这个内存区域。一个char占用一个Byte，对pstr解引用，得到的将是一个字符，也就是a（0x61是a的ASCII码）。</p>
<p>为什么输出了三个字符？这是由于在输出char*指针时，ostream会把它当做输出一个字符串来处理，直至遇到<code>&#39;\0&#39;</code>才表示字符串结束。如果将num的值改为0x63006261，输出的字符串就变为”ab”。</p>
<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ue-img/9.png"/ loading="lazy">

<p>再改为0x64636261后，作者输出字符串除了abcd后面还有6个字符，但我试着就只输出abcd了。</p>
<h5 id="补充：endian"><a href="#补充：endian" class="headerlink" title="补充：endian"></a>补充：endian</h5><p>上面的例子里，为什么输出16进制数字是0x00636261，输出字符串却成了abc（61 62 63）？</p>
<p>这是因为在reinterpret_cast中，存在字节序（endian）问题。查看<a href="https://www.zhihu.com/question/29452981/answer/44418335">这个问题下milo的回答</a>。</p>
<p>整数类型static_cast，指针++&#x2F;–都不涉及casting和字节序问题。把指针类型reinterpret_cast，或者使用union转换了类型，就有字节序问题。</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">int a &#x3D; 0x12345678;
char* c &#x3D; reinterpret_cast&lt;char*&gt;(&amp;a);
printf(&quot;%x %x %x %x\n&quot;, c[0], c[1], c[2], c[3]);

union
&#123;
	int a;
	char c[4];
&#125;u;
u.a &#x3D; 0x12345678;
printf(&quot;%x %x %x %x\n&quot;, u.c[0], u.c[1], u.c[2], u.c[3]);</code></pre>

<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ue-img/10.png"/ loading="lazy">

<p>从这个答案和我的输出可以确定我的电脑是little-endian。接下来说一下什么是endian：</p>
<ul>
<li>对于多字节的数据，计算机一定从低地址向高地址存储</li>
<li>big-endian的计算机，把数据的高字节（比如数的高位）存到低地址、低字节存到高地址，也就是按人类读数的顺序来存。对于int32 的 0x12345678，从低地址到高地址的四个字节分别存 0x12、0x34、0x56、0x78</li>
<li>little-endian的计算机，数据的低字节存入低地址、高字节存入高地址。同一个例子，字节存放顺序从低地址到高地址为：0x78、0x56、0x34、0x12</li>
</ul>
<p>人们阅读内存的顺序总是大端序的。而且小端序的每个字节内部还是保留了大端序的形式，而不是全部反过来，这让小端序在人类的脑子里更容易混淆。</p>
<p>那么为什么还被使用呢？因为至少在1970s，当CPU只有几千个逻辑门的时候，小端序更加有效率。因此，很多内部处理都是小端序的，并且也渗透到一些外部格式中。</p>
<p>另一方面，大端序对人类更容易理解，因此对于格式规范，如网络协议、文件格式，大部分都是大端序的。</p>
<h5 id="reinterpret-cast的使用"><a href="#reinterpret-cast的使用" class="headerlink" title="reinterpret_cast的使用"></a>reinterpret_cast的使用</h5><p>这个“重新解释”可以实现两个作用：</p>
<ul>
<li>指针和整数之间的转换</li>
<li>不同类型的指针&#x2F;成员指针&#x2F;引用之间的转换</li>
</ul>
<p>对于第一个作用，可以用来在指针中存储额外信息。例如在特定平台上，如果指针指向的类型是4字节对齐的，那么指针转成的整数的最低两bit位一定都是0（因为都指向0x00，0x04，0x08，0x0c这样的地址），就可以用这两位存储其他数据。</p>
<p>第二个作用，可以用来通过成员访问到完整的结构体对象，或者从完整的结构体对象访问间接成员。</p>
<p>除此之外，还可以利用以上功能，把类型不符合的数据硬塞到容器中。虽然需要使用者额外注意才能保证类型安全，但有时候能降低数据结构的复杂程度。</p>
<p>要是只从使用的角度上来说，C++的这些转换是为了代替C语言用括号执行的强制转换。如果换成<code>const_cast</code>或者<code>static_cast</code>合法，就具有相似的转换行为；如果不合法，就可以要考虑使用<code>reinterpret_cast</code>。</p>
<p>相当于告诉编译器：我用其他的转换你会报错，那么我就用<code>interpret_cast</code>，对于类型我心里有数，你不要报错。</p>
<p>总之，reinterpret_cast较危险，使用较少。</p>
<h4 id="③static-cast"><a href="#③static-cast" class="headerlink" title="③static_cast"></a>③static_cast</h4><p>题外话：</p>
<p>虽然const_cast是用来去除const指针或引用的const属性的，但static_cast却不是用来去掉变量static属性的。因为static实际上决定的是一个变量的作用域和生命周期，改变static属性会造成范围性的影响，而const只是限定一个变量自己。</p>
<p>但无论是哪一个属性，都是在变量一出生（完成编译的时候）就决定了的变量的特性，所以实际上都是不容许改变的。这点在前文已经看到了，const_cast也就是给const常量的指针或引用去除const属性，给原常量提供一个新的接口，并没有真正修改对象本身的const属性（还是不能直接给它赋值）。</p>
<p>回到static_cast，一般用它来代替C语言的类型转换。它跟reinterpret_cast一样，都不能去除指针或引用的const限定，跟const_cast的分工有所区别。其他一些区分：</p>
<ul>
<li>static_cast不仅可以用在指针和引用上，还可以用在基础数据类型和对象上</li>
<li>reinterpret_cast可以用于“完全没有关系”的类型之间，static_cast需要转换的双方“有一点关系”</li>
<li>static_cast会进行编译期类型检查，但不会进行运行时类型检查</li>
</ul>
<h5 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h5><p>static_cast的作用：</p>
<ul>
<li>基本数据类型之间的转换</li>
<li>类层级结构中，基类和派生类之间的指针或引用的转换<ul>
<li>进行上行转换（把派生类的指针或引用转换成基类）是安全的</li>
<li>进行下行转换（基类指针或引用转换成派生类），由于没有运行时类型检查，是不安全的，因为派生类中可能定义了基类中没有的成员变量</li>
</ul>
</li>
<li>把空指针转换成目标类型的空指针</li>
<li>把任何类型的表达式转换成void类型</li>
</ul>
<p>static_cast 不能用于在不同类型的指针之间互相转换，也不能用于整型和指针之间的互相转换，当然也不能用于不同类型的引用之间的转换。因为这些属于风险比较高的转换。</p>
<p>在指针和引用方面，似乎也只有继承关系是可以被static_cast接受的，其他情况的指针和引用转换都会被static_cast直接扔出编译错误，而这层关系上的转换又几乎都可以被dynamic_cast所代替。这样看起来static_cast的作用就比较小了。</p>
<p>static_cast真正用处并不在指针和引用上，而在基础类型和对象的转换上。而基于基础类型和对象的转换都是其他三个转换运算符所办不到的。</p>
<p>以下是一些例子。</p>
<p>基本数据类型：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">char a &#x3D; &#39;a&#39;;
int b &#x3D; static_cast&lt;int&gt;(a);	&#x2F;&#x2F; 正确

double* c &#x3D; new double;
void* d &#x3D; static_cast&lt;void*&gt;(c);	&#x2F;&#x2F; 正确

int e &#x3D; 10;
const int f &#x3D; static_cast&lt;const int&gt;(e);	&#x2F;&#x2F; 正确，给变量加上const属性

const int g &#x3D; 20;
int* h &#x3D; static_cast&lt;int*&gt;(&amp;g);		&#x2F;&#x2F; 编译错误，static_cast不能去掉常量对象的引用或指针的const属性</code></pre>



<p>类层级结构：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">class Parents
&#123;
public:
    virtual ~Parents()&#123;&#125;
    &#x2F;&#x2F; ...
&#125;;

class Children : public Parents
&#123;
    &#x2F;&#x2F; ...
&#125;;

int main()
&#123;
    Child* daughter &#x3D; new Children();
    Parents* mother &#x3D; static_cast&lt;Parents*&gt; (daughter);	&#x2F;&#x2F; right, cast with polymorphism
    
    Parents* father &#x3D; new Parents();
    Children* son &#x3D; static_cast&lt;Children*&gt; (father);	&#x2F;&#x2F; no error, but not safe	
&#125;</code></pre>

<p>从基类到子类的转换，使用static_cast是不安全的，在后面的dynamic_cast里讲。</p>
<p>指针和引用：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">class A
&#123;
public:
	operator int() &#123; return 1; &#125;
	operator char* () &#123; return nullptr; &#125;
&#125;;

int main()
&#123;
	A a;
	int n;
	char* p &#x3D; const_cast&lt;char*&gt;(&quot;abcd&quot;);
	
	n &#x3D; static_cast&lt;int&gt;(3.14159);	&#x2F;&#x2F; n变为3
    
	n &#x3D; static_cast&lt;int&gt;(a);	&#x2F;&#x2F; 调用a.operator int()，返回1
	p &#x3D; static_cast&lt;char*&gt;(a);	&#x2F;&#x2F; 调用a.operator char*()，返回空指针
    
    n &#x3D; static_cast&lt;int&gt;(p);	&#x2F;&#x2F; 编译错误，static_cast不能进行整形和指针之间的转换
	p &#x3D; static_cast&lt;char*&gt;(n);	&#x2F;&#x2F; 编译错误，static_cast不能进行整形和指针之间的转换
    
	return 0;
&#125;</code></pre>





<h4 id="④dynamic-cast"><a href="#④dynamic-cast" class="headerlink" title="④dynamic_cast"></a>④dynamic_cast</h4><h5 id="安全的下行转换"><a href="#安全的下行转换" class="headerlink" title="安全的下行转换"></a>安全的下行转换</h5><p>多态基类指含有虚函数的基类。用reinterpret_cast可以将多态基类指针强制转换为派生类的指针，但这种转换不检查安全性，也就是不检查转换后的指针是否确实指向一个派生类的对象。</p>
<p>dynamic_cast专门用来<u>将多态基类的指针或引用转换成派生类的指针或引用</u>，并且进行运行时类型检查（RTTI，Run-Time Type Identification）。对于不安全的指针转换返回nullptr。如果是引用，由于不存在空引用，直接抛出异常，名为std::bad_cast。</p>
<p>即，dynamic_cast“只能用于安全的类型转换”，而不是“保证变换的安全”，因此不能用来将非多态基类的指针或引用转换成派生类的指针或引用，因为这个转换不安全。只能用reinterpret_cast来完成。</p>
<p><u>dynamic_cast使用的前提是基类指针本身就指向一个派生类的对象</u>。这一部分在下一节再提，先看一下如何判断转换成功和失败：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">class Base
&#123;
public:
	virtual ~Base() &#123;&#125;;
&#125;;

class Derived :public Base &#123;&#125;;

int main()
&#123;
	Base b;
	Derived d;
	Derived* pd;
	pd &#x3D; reinterpret_cast&lt;Derived*&gt;(&amp;b);
	if (pd &#x3D;&#x3D; nullptr) std::cout &lt;&lt; &quot;unsafe reinterpret_cast from base to derived&quot; &lt;&lt; std::endl;

	pd &#x3D; dynamic_cast&lt;Derived*&gt;(&amp;b);
	if (pd &#x3D;&#x3D; nullptr) std::cout &lt;&lt; &quot;unsafe dynamic_cast from base to derived&quot; &lt;&lt; std::endl;

	pd &#x3D; dynamic_cast&lt;Derived*&gt;(&amp;d);
	if (pd &#x3D;&#x3D; nullptr) std::cout &lt;&lt; &quot;unsafe dynamic_cast from derived to derived&quot; &lt;&lt; std::endl;

	Base* pb &#x3D; dynamic_cast&lt;Base*&gt;(&amp;d);
	if (pb &#x3D;&#x3D; nullptr) std::cout &lt;&lt; &quot;unsafe dynamic_cast from derived to base&quot; &lt;&lt; std::endl;

	return 0;
&#125;</code></pre>

<p>输出：unsafe dynamic_cast from base to derived。在转换错误的例子中，是从一个多态基类（含有虚函数的基类）转换到子类，但<code>b</code>本身不是一个子类<code>d</code>的对象，因此也不能转换。</p>
<p>除了检查指针到指针的转换返回nullptr，可以用以下方式捕获引用转换的异常：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">try&#123;
    const Derived&amp; d &#x3D; dynamic_cast&lt;Derived&amp;&gt;(b);
&#125;catch(bad_cast)&#123;
    &#x2F;&#x2F; ...
&#125;</code></pre>



<p>如果把指针转换到void*，RTTI决定表达式的实际类型。结果是指向被转换对象的指针。</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">void* pb &#x3D; dynamic_cast&lt;void*&gt;(b);	&#x2F;&#x2F; pb指向b类型的对象
void* pd &#x3D; dynamic_cast&lt;void*&gt;(d);	&#x2F;&#x2F; pd指向d类型的对象</code></pre>



<h5 id="dynamic-cast的使用"><a href="#dynamic-cast的使用" class="headerlink" title="dynamic_cast的使用"></a>dynamic_cast的使用</h5><h6 id="基类指针本来就指向派生类对象"><a href="#基类指针本来就指向派生类对象" class="headerlink" title="基类指针本来就指向派生类对象"></a>基类指针本来就指向派生类对象</h6><p>接着上一小节，<strong>dynamic_cast使用的前提之一是基类指针本身就指向一个派生类的对象。</strong>dynamic_cast的目的是把基类指针转换程派生类指针，<u>调用派生类中的非虚函数、使用派生类的成员变量</u>。因为基类指针本来就指向派生类对象，可以直接调用派生类的虚函数（里氏替换），无需转换。</p>
<p>以下是一个完整的例子：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;

class Base
&#123;
public:
	Base(int _x) :x(_x) &#123;&#125;
	int getx() &#123; return x; &#125;
	virtual ~Base() &#123;&#125;
private:
	int x;
&#125;;

class DerivedA : public Base
&#123;
public:
	DerivedA(int _x, int _y) :Base(_x), y(_y) &#123;&#125;
	int gety() &#123; return y; &#125;
private:
	int y;
&#125;;

class DerivedB : public Base
&#123;
public:
	DerivedB(int _x, int _z) :Base(_x), z(_z) &#123;&#125;
	int getz() &#123; return z; &#125;
private:
	int z;
&#125;;

enum TYPE
&#123;
	BASE &#x3D; 0, 
	DERIVEDA, 
	DERIVEDB
&#125;;

&#x2F;&#x2F; 根据设定的type，创建相应的对象，统一返回基类指针
Base* CreateObject(int x, int y, TYPE type)
&#123;
	if (type &#x3D;&#x3D; BASE)
	&#123;
		return new Base(x);
	&#125;
	else if (type &#x3D;&#x3D; DERIVEDA)
	&#123;
		return new DerivedA(x, y);
	&#125;
	else if (type &#x3D;&#x3D; DERIVEDB)
	&#123;
		int z &#x3D; y;
		return new DerivedB(x, z);
	&#125;
	return nullptr;
&#125;

int main()
&#123;
	Base* p &#x3D; CreateObject(10, 20, DERIVEDA);	&#x2F;&#x2F; 基类指针p，指向一个派生A类对象
	std::cout &lt;&lt; &quot;访问基类成员变量x：&quot; &lt;&lt; p-&gt;getx() &lt;&lt; std::endl;

	&#x2F;&#x2F; 希望使用派生A类的成员变量b，需要进行dynamic_cast
	DerivedA* pA &#x3D; dynamic_cast&lt;DerivedA*&gt;(p);	&#x2F;&#x2F; p本来就指向派生A类对象，因此可以安全转换
	if (pA)
	&#123;
		std::cout &lt;&lt; &quot;访问派生A类成员变量y：&quot; &lt;&lt; pA-&gt;gety() &lt;&lt; std::endl;
	&#125;
	else
	&#123;
		std::cout &lt;&lt; &quot;转换到派生A类失败&quot; &lt;&lt; std::endl;
	&#125;

	DerivedB* pB &#x3D; dynamic_cast&lt;DerivedB*&gt;(p);	&#x2F;&#x2F; p本来不是指向派生B类，因此转换失败
	if (pB)
	&#123;
		std::cout &lt;&lt; &quot;访问派生B类成员变量y：&quot; &lt;&lt; pB-&gt;getz() &lt;&lt; std::endl;
	&#125;
	else
	&#123;
		std::cout &lt;&lt; &quot;转换到派生B类失败&quot; &lt;&lt; std::endl;
	&#125;

	return 0;
&#125;</code></pre>

<p>创建一个派生A类的对象，用一个基类指针<code>p</code>指向它。dynamic_cast只能把<code>p</code>转换成它指向的派生A类的指针，不能转换成另一个派生B类对象的指针。</p>
<p>比较容易理解，因为不能从无到有、凭空访问子类特有的数据。</p>
<h6 id="多态基类"><a href="#多态基类" class="headerlink" title="多态基类"></a>多态基类</h6><p><strong>使用dynamic_cast的另一个前提：基类是多态基类，即至少有一个虚函数。</strong>一般来说，日常使用的基类的析构函数都是虚函数，这样用基类指针调用时，可以从子类开始析构，并一直析构到基类。也就是这个前提一般是满足的。</p>
<h6 id="使用总结"><a href="#使用总结" class="headerlink" title="使用总结"></a>使用总结</h6><p>运行时多态也可以通过在基类里添加虚函数来实现，但在一些应用场景里，一些方法是派生类特有的，对基类无意义，如果添加为基类的虚函数就进行了污染，不符合设计思想。</p>
<p>而在合适的时候，通常应该优先使用虚函数。也有几个情况下dynamic_cast是更好的选择：</p>
<ul>
<li>不能改变基类来添加虚函数时（比如基类是标准库中的类）</li>
<li><strong>想访问派生类特有的成员函数时（目的是使用非虚函数）</strong> </li>
<li>添加虚函数到基类中没有意义时（不合适的时候，比如基类里没有合适的返回值）</li>
</ul>
<h5 id="回到开头的UE4-Cast"><a href="#回到开头的UE4-Cast" class="headerlink" title="回到开头的UE4 Cast"></a>回到开头的UE4 Cast</h5><p>讲完了四种类型转换，回过头来，看一下文章一开始提到的UE4中的一个转换场景：</p>
<p>首先，GEngine是一个UEngine类型的指针，UEngine类是所有引擎类的抽象基类，用于管理一些对编辑器或游戏必要的系统：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">&#x2F;**
* Global engine pointer. Can be 0 so don&#39;t use without checking.
*&#x2F;
ENGINE_API UEngine*	GEngine &#x3D; NULL;</code></pre>



<p>然后，它有一个派生类UGameEngine，用来管理支持游戏的核心系统，包含一些指针比如GameViewportWindow等：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">&#x2F;**
 * Engine that manages core systems that enable a game.
 *&#x2F;
UCLASS(config&#x3D;Engine, transient)
class ENGINE_API UGameEngine
	: public UEngine
&#123;
    &#x2F;&#x2F; ...
public:      
	&#x2F;** The game viewport window *&#x2F;
	TWeakPtr&lt;class SWindow&gt; GameViewportWindow;
	&#x2F;** The primary scene viewport *&#x2F;
	TSharedPtr&lt;class FSceneViewport&gt; SceneViewport;
	&#x2F;** The game viewport widget *&#x2F;
	TSharedPtr&lt;class SViewport&gt; GameViewportWidget;
&#125;;</code></pre>



<p>在使用的时候，是新建一个子类指针、把父类指针GEngine转成子类UGameEngine类型的指针，然后去访问子类单独定义的成员变量。这就是使用dynamic_cast的目的。</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">UGameEngine* GameEngine &#x3D; Cast&lt;UGameEngine&gt;(GEngine);
OutSceneViewport &#x3D; GameEngine-&gt;SceneViewport;
OutInputWindow &#x3D; GameEngine-&gt;GameViewportWindow;</code></pre>



<p>这个转换满足了dynamic_cast的两个前提，含有虚函数不必多说。另一个前提：基类指针本来就指向一个派生类的对象。我们查看GEngine的创建过程：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">int32 FEngineLoop::Init()
&#123;
	&#x2F;&#x2F; Figure out which UEngine variant to use.
	UClass* EngineClass &#x3D; nullptr;
	if( !GIsEditor )
	&#123;
		SCOPED_BOOT_TIMING(&quot;Create GEngine&quot;);
		&#x2F;&#x2F; We&#39;re the game.
		FString GameEngineClassName;
		GConfig-&gt;GetString(TEXT(&quot;&#x2F;Script&#x2F;Engine.Engine&quot;), TEXT(&quot;GameEngine&quot;), GameEngineClassName, GEngineIni);
		EngineClass &#x3D; StaticLoadClass( UGameEngine::StaticClass(), nullptr, *GameEngineClassName);
		if (EngineClass &#x3D;&#x3D; nullptr)
		&#123;
			UE_LOG(LogInit, Fatal, TEXT(&quot;Failed to load UnrealEd Engine class &#39;%s&#39;.&quot;), *GameEngineClassName);
		&#125;
		GEngine &#x3D; NewObject&lt;UEngine&gt;(GetTransientPackage(), EngineClass);
	&#125;
&#125;</code></pre>

<p>可以看到，<code>GEngine</code>在创建时，传入了一个<code>EngineClass</code>；<code>EngineClass</code>使用<code>UGameEngine::StaticClass()</code>创建。可以理解为，最终创建的基类GEngine指针指向了派生类UGameEngine的对象。</p>
<p>也就是，可以用dynamic_cast进行安全的下行转换，使用派生类的成员变量。</p>
<p>这样，我们从UE4的一处Cast，讲解了C++的四种类型转换，并在最后发现，UE4的这一处类型转换实际上是安全的下行转换。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul>
<li><a href="https://blog.csdn.net/kenjianqi1647/article/details/121511906">(CSDN) C++11类型转换总结</a></li>
<li><a href="https://stackoverflow.com/questions/8239262/why-is-the-address-of-this-volatile-variable-always-at-1">(StackOverflow) Why is the address of this volatile variable always at 1?</a></li>
<li><a href="https://blog.csdn.net/diesa5743/article/details/101275260">(CSDN) const_cast去除const限制，同一片内存</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/344264694">(知乎) const与volatile</a></li>
<li><a href="https://blog.erratasec.com/2016/11/how-to-teach-endian.html#.YuIvX3ZBxPZ">How to teach endian</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/33040213">(知乎) C++类型转换之reinterpret_cast</a></li>
<li><a href="https://www.zhihu.com/question/302752247">(知乎) 如何理解C++中的 reinterpret_cast？</a></li>
<li><a href="http://c.biancheng.net/view/410.html">C++强制类型转换运算符</a></li>
<li><a href="https://www.zhihu.com/question/29452981/answer/44418335">(知乎) 大端小端与指针的关系？</a></li>
<li><a href="https://blog.csdn.net/xiao_Ray/article/details/104977065">(CSDN) 大端和小端</a></li>
<li><a href="https://blog.csdn.net/feifei_csdn/article/details/101520218">(CSDN)C++标准转换运算符static_cast</a></li>
<li><a href="https://blog.csdn.net/m0_37834993/article/details/106310012">(CSDN)dynamic_cast用法总结</a></li>
</ul>
]]></content>
      <categories>
        <category>UE源码</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>UnrealEngine</tag>
        <tag>游戏开发</tag>
      </tags>
  </entry>
  <entry>
    <title>【我看UE源码】（2）源码阅读支线之如何理解C++的const关键字？</title>
    <url>/2022/09/07/ue-2/</url>
    <content><![CDATA[<blockquote>
<p>叠甲：仅为记录个人学习过程；内容很多是从别处看来的，在参考中列出；缺乏实践经验，有些地方纸上谈兵；难免有错误，敬请指出。</p>
<p>系列目录：<a href="https://acbgzm.github.io/2022/06/18/ue-0/">【我看UE源码】（0）系列开篇 &amp; 目录</a></p>
</blockquote>
<h5 id="Const的内存分配"><a href="#Const的内存分配" class="headerlink" title="Const的内存分配"></a>Const的内存分配</h5><p>（接上文）</p>
<p>对于上述通过指针、引用（也就是地址）来修改const变量的值，只对局部变量有效。</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">const int ga &#x3D; 1;

int main()
&#123;
    int* pga &#x3D; const_cast&lt;int*&gt;(&amp;ga);
	*pga &#x3D; 10;		&#x2F;&#x2F; 运行时报错
	std::cout &lt;&lt; &quot;a:&quot; &lt;&lt; const_cast&lt;int*&gt;(&amp;a) &lt;&lt; &#39; &#39; &lt;&lt; a &lt;&lt; std::endl;
    
	int&amp; rga &#x3D; const_cast&lt;int&amp;&gt;(ga);
	rga &#x3D; 20;		&#x2F;&#x2F; 运行时报错
	std::cout &lt;&lt; &quot;a:&quot; &lt;&lt; const_cast&lt;int*&gt;(&amp;a) &lt;&lt; &#39; &#39; &lt;&lt; a &lt;&lt; std::endl;
&#125;</code></pre>

<p>如果进行上面代码中的操作，试图修改全局const变量的值，编译器不报错，但运行时在指针、引用赋值的地方报错：“引发了异常：写入访问权限冲突”。</p>
<p>这是由C++ const的内存分配方式导致的。</p>
<h6 id="全局const变量的内存"><a href="#全局const变量的内存" class="headerlink" title="全局const变量的内存"></a>全局const变量的内存</h6><p>const全局变量存放在.rodata段的Read Only Data也就是常量存储区（只读数据区），无法通过取地址方式修改，所以报错</p>
<p>关键字修饰变量的内存存储方式如下：</p>
<ul>
<li>static：<ul>
<li>static表示是静态变量，内存中只存在一份。<u>无论是全局变量还是局部变量都存储在全局&#x2F;静态区域，在编译期就为其分配内存，在程序结束时释放</u> </li>
<li>类的static成员变量必须在类声明的外部进行初始化。初始化时不赋值就默认为0<ul>
<li>全局数据区的变量都有默认初始值0，而动态数据区（堆区、栈区）变量的默认值不确定，一般是垃圾值</li>
</ul>
</li>
<li>static成员变量属于类而不属于对象，在全局区分配内存，不占用对象的内存</li>
<li>static成员变量不随着对象创建而分配内存，不随着对象销毁而释放内存</li>
</ul>
</li>
<li>extern：在当前源文件声明全局变量，告知编译器去别的地方（其他文件，或本文件其他行）找定义</li>
<li>const<ul>
<li>const全局变量存储在只读数据段，编译期最初将其保存在<strong>符号表</strong>中，第一次使用时为其分配内存，在程序结束时释放</li>
<li>const局部变量存储在栈中，代码块结束时释放（见下一节）</li>
<li>const修饰的全局变量默认是内部链接，即<u>只在当前源文件有效</u>，不能直接用于其他源文件。<u>如果必须在其他源文件中使用只读的全局变量，需要加extern将变量转换成外部链接</u></li>
</ul>
</li>
</ul>
<h6 id="局部const变量的内存"><a href="#局部const变量的内存" class="headerlink" title="局部const变量的内存"></a>局部const变量的内存</h6><p>局部const变量的内存分配有一些争议，可以看看<a href="https://blog.csdn.net/u014157109/article/details/115350923">这篇文章</a>。据我尝试，跟这篇文章一样，VS2019对于基础类型赋值的const变量，release模式下不分配内存，debug模式下分配内存。</p>
<p>一般来说，总结为：</p>
<ul>
<li>用基础类型赋值的const变量，放入符号表中，不会分配内存</li>
<li>对const变量取地址的时候（&amp;a），给这个变量开辟空间</li>
<li>当用变量给const初始化赋值时，直接给它开辟空间，不会放入符号表中</li>
<li>自定义数据类型（结构体、类）的const对象，const数组等会分配内存空间</li>
</ul>
<p>举例：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;

struct Student
&#123;
	int age;
	char name[10];
&#125;;

int main()
&#123;
	const int a &#x3D; 10;	&#x2F;&#x2F; 用基础类型值初始化，a不分配内存，放到符号表里
	int* pa &#x3D; const_cast&lt;int*&gt;(&amp;a);	&#x2F;&#x2F; 对a取地址，分配内存
	*pa &#x3D; 20;
	std::cout &lt;&lt; *pa &lt;&lt; &#39; &#39; &lt;&lt; a &lt;&lt; std::endl;	&#x2F;&#x2F; 由于编译器优化（见上篇），*pa&#x3D;20，a&#x3D;10

	int b &#x3D; 30;
	const int c &#x3D; b;	&#x2F;&#x2F; 用变量初始化，直接给c分配内存空间，不放入符号表
	int* pc &#x3D; const_cast&lt;int*&gt;(&amp;c);
	*pc &#x3D; 40;
	std::cout &lt;&lt; *pc &lt;&lt; &#39; &#39; &lt;&lt; c &lt;&lt; std::endl;	&#x2F;&#x2F; 40 40

	const Student stu &#x3D; &#123; 20, &quot;tom&quot; &#125;;	&#x2F;&#x2F; 自定义类型的const对象，分配内存
	Student* pstu &#x3D; const_cast&lt;Student*&gt;(&amp;stu);
	pstu-&gt;age &#x3D; 21;
	std::cout &lt;&lt; pstu-&gt;age &lt;&lt; &#39; &#39; &lt;&lt; stu.age &lt;&lt; std::endl;	&#x2F;&#x2F; 21 21

	return 0;
&#125;</code></pre>

<img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master/ue-img/6.png" style="zoom: 80%;" / loading="lazy">



<h6 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h6><pre class="language-c++" data-language="c++"><code class="language-c++">const int a &#x3D; 3;	&#x2F;&#x2F; 全局const变量，符号表，使用时分配在常量区（只读数据段）
int b &#x3D; 2;			&#x2F;&#x2F; 全局变量，分配在静态&#x2F;全局区
static int c &#x3D; 1;	&#x2F;&#x2F; 静态全局变量，分配在静态&#x2F;全局区

static int d;		&#x2F;&#x2F; 未初始化的静态全局变量，有初始值0
int e;				&#x2F;&#x2F; 未初始化的全局变量，有初始值0

int main()&#123;
    static int f &#x3D; 4;	&#x2F;&#x2F; 初始化的静态局部变量，分配在静态&#x2F;全局区
    static int g;		&#x2F;&#x2F; 未初始化的静态局部变量，分配在静态&#x2F;全局区，有初始值0
    int h &#x3D; 5;			&#x2F;&#x2F; 初始化的局部变量，分配在栈区
    int i;				&#x2F;&#x2F; 未初始化局部变量，分配在栈区，初始值是无效值
    
    const int j &#x3D; 6;	&#x2F;&#x2F; 局部const变量，放入符号表，取地址时分配在栈区
    const int k &#x3D; h;	&#x2F;&#x2F; 局部const变量，直接分配在栈区
&#125;</code></pre>





<h5 id="const和define"><a href="#const和define" class="headerlink" title="const和define"></a>const和define</h5><p>const和define都可以用来定义常量，它们的区别是：</p>
<ul>
<li>const常量是编译期的概念<ul>
<li>const常量有数据类型，宏没有。编译器可以对const进行类型安全检查</li>
</ul>
</li>
<li>define宏定义作用在预处理期<ul>
<li>程序在预处理阶段，把define定义的内容进行字符串替换。需要注意运算符优先级之类的边缘效应问题</li>
<li>编译期不会进行数据类型检查</li>
<li>程序运行时，系统不为define定义的常量分配内存</li>
</ul>
</li>
</ul>
<p>在C++里，如果想定义一个常量，最好用const而不是define。关于尽量使用const，可以看《Effective C++》的条款三。</p>
<h5 id="const，引用传参，常量引用"><a href="#const，引用传参，常量引用" class="headerlink" title="const，引用传参，常量引用"></a>const，引用传参，常量引用</h5><p>在前文说过，可以把const看作是一种注释，对于局部const变量，不一定分配了内存，即使分配了或许也不在常量区。它只是让编译器注意不要直接修改其值，或者让程序员不要写直接修改值的代码。</p>
<p>const还有一大作用，就是在函数传参时，传递常量的引用类型。这是C++机制上的一个问题，可以看看<a href="https://www.zhihu.com/question/473730909/answer/2026149928">这个答案下的回答</a>。</p>
<h6 id="引用传参"><a href="#引用传参" class="headerlink" title="引用传参"></a>引用传参</h6><p>在函数传参时，如果传值，会新建值的对象并且进行拷贝，在函数中对新对象进行操作，不会影响原来的对象；而引用相当于给一块内存起一个别名，如果传递引用，就可以直接对实参进行操作。</p>
<p>指针和引用基本上是一样的，区别在于：</p>
<ul>
<li>指针会分配内存，用来存储指向的内存地址。可以找到自身开辟的内存空间，可以通过<code>*</code>运算符解引用找到指向的内存单元。</li>
<li>对于引用，虽然在C++底层处理时和指针处理方式相同，不过在用到引用变量的地方，会自动对其进行解引用，这一步骤系统默认进行，所以我们找不到引用自身开辟的内存单元，从这里看，引用好像没有开辟自身内存，只是给引用对象起了一个别名。</li>
</ul>
<p>关于指针、引用的区别和用它们传参，可以看<a href="https://blog.csdn.net/IT_Quanwudi/article/details/84549968">这篇文章</a>。其中提到了使用引用的几个条件：</p>
<ul>
<li>引用必须在声明时就进行初始化</li>
<li>引用初始化的变量一定要能取地址</li>
<li>引用关系是不可变的</li>
</ul>
<p>以及引用和指针的区别，不是本篇重点，但八股会问。稍微记录一下</p>
<ul>
<li>从内存分配上看，指针是对象实体，程序为其分配内存空间。而引用从表面上看，只是变量的别名，程序不需要为其分配空间（实际上引用跟指针是一样存的，但在使用时会自动解引用，用起来像别名）</li>
<li>引用使用时无需解引用，系统会为其自动进行解引用，而指针使用时需要手动解引用（<code>*</code>运算符）</li>
<li>指向关系上，引用只能在定义时被初始化一次，之后则不可变。指针则可以改变</li>
<li>引用不能为空（必须初始化），指针可以为空。</li>
<li>“sizeof 引用”得到的是所指向的变量（对象）的大小，而“sizeof 指针”得到的是指针本身（所指向的变量或对象的地址）的大小（为了完整寻址，32位4字节，64位8字节）</li>
<li>指针和引用的自增(++)运算意义不一样。指针自增是让其指向下一段内存单元，而引用自增是使其对应的变量自增</li>
</ul>
<h6 id="常量引用"><a href="#常量引用" class="headerlink" title="常量引用"></a>常量引用</h6><p>对于第二条，类似<code>int&amp; a = 10;</code>是不行的。也就是引用只能作用于一个变量，不能赋值到字面常量，比如<code>1, 2, 3</code>等字面量，<code>&quot;Hello World&quot;</code>等常量字符串。它们是不可以被修改的，不能使用非const类型的引用传递。</p>
<p>C++引入了常量引用的概念来处理这种情况。常量引用是“对const的引用”的简称，把它指向的对象看作是常量（不一定真是），不可以通过该引用修改它指向的对象的值。（严格来说不存在常量引用，因为引用不是一个对象，无法让引用本身恒定不变；由第三条，C++不允许随意改变引用绑定的对象，从这种意义上理解，所有引用本身都是常量。）</p>
<p>在使用上：</p>
<ul>
<li><p>指向常量对象时，必须用常量引用</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">const int a &#x3D; 10;
const int&amp; ra &#x3D; a;	&#x2F;&#x2F; 正确，指向常量对象
ra &#x3D; 20;	&#x2F;&#x2F; 错误
int&amp; rb &#x3D; a;	&#x2F;&#x2F; 错误</code></pre>
</li>
<li><p>常量引用可以指向非常量对象，但不能修改它的值</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">int a &#x3D; 10;
int&amp; ra &#x3D; a;
const int&amp; rb &#x3D; a;	&#x2F;&#x2F; 正确
ra &#x3D; 20;	&#x2F;&#x2F; 正确
rb &#x3D; 20;	&#x2F;&#x2F; 错误</code></pre>
</li>
<li><p>常量引用可以指向字面值、常量字符串、一般表达式等非左值、不能赋值的对象</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">int i &#x3D; 10;
const int&amp; ra &#x3D; i;	&#x2F;&#x2F; 正确
const int&amp; rb &#x3D; 10;	&#x2F;&#x2F; 正确，普通引用不行
const int&amp; rc &#x3D; ra * 2;	&#x2F;&#x2F; 正确，普通引用不行</code></pre>
</li>
<li><p>实际上，只要被引用的类型能够转换为常量引用的类型即可</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">double dval &#x3D; 3.14;
const int&amp; pi &#x3D; dval;	&#x2F;&#x2F; 可用</code></pre></li>
</ul>
<p>在上面一句引用语句中，编译器实际上相当于执行了以下两条语句：</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">const int temp &#x3D; dval;	&#x2F;&#x2F; 生成一个临时整形常量
const int&amp; pi &#x3D; temp;	&#x2F;&#x2F; 给常量引用初始化</code></pre>

<p>在这种情况下，常量引用实际上是绑定了一个临时量（temporary）对象。也就是说，允许常量引用指向一个临时量对象。</p>
<p>假设pi不是常量引用，就允许对它赋值，改变pi所引用的对象的值。但此时绑定的对象是一个临时量而非 dval，不能改变dval的值。在<code>const int&amp; pi = dval</code>中，我们既然想让pi引用dval，就肯定想通过pi改变 dval的值。如此看来，基本上在使用时不会想着把引用绑定到临时量上，C++ 语言也就把这种行为归为非法。</p>
<p>也就是说，不允许一个普通引用与字面值或者某个表达式的计算结果，或类型不匹配的对象绑定在一起，其实就是不允许一个普通引用指向一个临时变量，只允许将常量引用指向临时对象。</p>
<h6 id="常量引用传参"><a href="#常量引用传参" class="headerlink" title="常量引用传参"></a>常量引用传参</h6><p>在函数参数中，使用常量引用很重要。因为函数有可能接收临时对象，并且同时要禁止对引用的对象进行修改。</p>
<pre class="language-c++" data-language="c++"><code class="language-c++">int test()&#123; return 1; &#125;

void func1(int&amp; x)&#123; std::cout &lt;&lt; x &lt;&lt; std::endl; &#125;

void func2(const int&amp; x)&#123; std::cout &lt;&lt; x &lt;&lt; std::endl; &#125;

int main()
&#123;
	int m &#x3D; 1;
    
    func1(m);	&#x2F;&#x2F; 正确
    func1(1);	&#x2F;&#x2F; 错误
    func1(test());	&#x2F;&#x2F; 错误
    
    func2(m);	&#x2F;&#x2F; 正确
    func2(1);	&#x2F;&#x2F; 正确
    func2(test());	&#x2F;&#x2F; 正确
&#125;</code></pre>

<p>日常使用是在写模板的时候，不知道会传入什么类型的模板参数，就必须要让模板适配所有的情况。一个 const T &amp; 就是一种能够承载所有输入类型的办法。但一个 T&amp; 就做不到接收字面量作为参数，字面量无法直接转化为非常量引用。当我们需要constT&amp;或者T&amp;作为返回值的时候，问题会更多。</p>
<p>本质上也就是const T&amp; 能够承载右值（字面量），而单纯的T&amp;只能承载一个变量的引用，不能存放一个字面量。接下来应该还有一篇：C++ 11的右值引用。</p>
<h5 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h5><ul>
<li><a href="https://blog.csdn.net/MacKendy/article/details/123100342">（CSDN）C++之extern</a></li>
<li><a href="http://c.biancheng.net/view/2227.html">C++ static静态成员变量详解</a></li>
<li><a href="https://blog.csdn.net/u014157109/article/details/115350923">（CSDN）C和C++中const变量内存分配问题详解</a></li>
<li><a href="https://blog.csdn.net/shenju_dl_shenghuo/article/details/48241217">（CSDN）const，volatile同时修饰一个变量</a></li>
<li><a href="https://www.zhihu.com/question/473730909/answer/2026149928">（知乎）在c++代码中使用const关键字是不是自找麻烦？</a></li>
<li><a href="https://blog.csdn.net/IT_Quanwudi/article/details/84549968">（CSDN）C++引用传递基础</a></li>
<li><a href="https://blog.csdn.net/qq_39583450/article/details/110006989">（CSDN）C++ 常量引用用法详解</a></li>
<li><a href="https://blog.csdn.net/anda0109/article/details/79877899">（CSDN）const、static变量在内存中的位置</a></li>
</ul>
]]></content>
      <categories>
        <category>UE源码</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>UnrealEngine</tag>
        <tag>游戏开发</tag>
      </tags>
  </entry>
  <entry>
    <title>【我看UE源码】（-1）一切之前的事情</title>
    <url>/2022/06/18/ue-sourcecode--1/</url>
    <content><![CDATA[<h4 id="1-从源码编译UE4"><a href="#1-从源码编译UE4" class="headerlink" title="1 从源码编译UE4"></a>1 从源码编译UE4</h4><p>Unreal Engine是源码开放软件而不是开源软件。在获取引擎源码之前，需要加入Epic的Github组织，然后才能下载。从下载到跑起来，要经历下载源码、下第三方库、生成工程sln文件、编译引擎等步骤，可以看看这些资料：</p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1rU4y1T73E">（视频）如何调试UE4引擎源码</a> </li>
<li><a href="https://docs.unrealengine.com/4.27/zh-CN/ProductionPipelines/DevelopmentSetup/BuildingUnrealEngine/">（官方文档）从源代码构建虚幻引擎</a></li>
</ul>
<p><a href="https://www.unrealengine.com/en-US/eula/unreal">协议</a>里提到，分享源码有一个30行的限制。这个尽量遵守。</p>
<p>我的小笔记本和长城宽带做这些事情都是比较慢的。在编译引擎代码的时候，遇到了 <code>fatal error C1060: 编译器的堆空间不足</code>，<code>fatal error C1002: 在第 2 遍中编译器的堆空间不足</code>，<code>error C3859: 超过了 PCH 的虚拟内存范围</code> 等报错。原因和解决方法可以参考以下博文：</p>
<ul>
<li><a href="https://www.cnblogs.com/LynnVon/p/11287337.html">ue4 源码编译 虚拟内存不足 堆限制</a> </li>
<li><a href="https://blog.csdn.net/king4875/article/details/120142678">【ue4】fatal error C1060: “编译器的堆空间不足“</a> </li>
<li><a href="https://zhuanlan.zhihu.com/p/109178480">编译UE4时error C3859: 超过了 PCH 的虚拟内存范围问题解决</a></li>
</ul>
<p>如果只改factor的值，我还是编译不成。最终用了跟最后一篇完全一样的方法，修改到100、设置分页大小 64M-2927M。</p>
<h4 id="2-Rider-for-UE"><a href="#2-Rider-for-UE" class="headerlink" title="2 Rider for UE"></a>2 Rider for UE</h4><p>之前没用VS写过UE的C++（只用了蓝图），但看过一些Rider跟VS相比的文章，据说Rider全局搜索和跳转的速度快很多。因此我就直接下了个Rider来用。</p>
<p>JetBrains 的 IDE，我至今应该用过 WebStorm、IntelliJ IDEA、PyCharm 了，现在又在用 Rider。因此就打算把一些使用上的小习惯整理一下，也把在VS里用得顺手的部分迁移过来，延续下去。这里以 Rider + Unreal 为例，记录一些 IDEA 风格的 IDE 常用的快捷键，以及调试方法。（持续更新）</p>
<h5 id="白嫖学生免费的方法"><a href="#白嫖学生免费的方法" class="headerlink" title="白嫖学生免费的方法"></a>白嫖学生免费的方法</h5><p>可以看看这两篇文章：</p>
<ul>
<li><a href="https://blog.csdn.net/weixin_64568746/article/details/122421150">JetBrains学生认证全家桶申请学生账户方式-保姆级</a> </li>
<li><a href="https://zhuanlan.zhihu.com/p/378185042">Jetbrains学生授权获取指南</a></li>
</ul>
<p>我的edu邮箱不能直接申请，也是用了学信网。但也要填一个edu邮箱。过几天后，edu邮箱会收到邮件告知通过，并有一个链接可以把学生资历绑定到账号上。</p>
<p>不知为何，我没能将我的学生免费绑定在已有的账号上（绑定已有账号后，显示成功，但库里没有免费license），并且经过这一步后，收到的邮件也点不进去了。一番尝试后，我发现再用学生邮箱重新注册一个新号，注册完后就可以直接在库里找到免费的license，并且用户名直接给我实名上网了。大概是必须用之前填写的edu邮箱账号享受学生优惠吧。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master\blogimg\2022\51.png" style="zoom:67%;" / loading="lazy"></li>
</ul>
<p>之后要把UE跟Rider关联起来，看这篇文章：<a href="https://zhuanlan.zhihu.com/p/379911259">【UE4】Rider For Unreal体验报告</a>。</p>
<h5 id="Rider-快捷键"><a href="#Rider-快捷键" class="headerlink" title="Rider 快捷键"></a>Rider 快捷键</h5><p>打开File-&gt;Settings-&gt;Keymap，就可以设置快捷键了。我使用IntelliJ Keymap，不再使用VS</p>
<p>（个人习惯）首先，修改在VS里用得很顺的快捷键：</p>
<ul>
<li>Split Line 改为 <code>Shift + Enter</code>，Start New Line 改为 <code>Ctrl + Enter</code>（互换）</li>
<li>Switch Header&#x2F;Source 添加快捷键 <code>Alt + O</code></li>
</ul>
<p>查找快捷键：</p>
<ul>
<li><code>Ctrl + Shift + F</code> 全局查找</li>
<li><code>Ctrl + F</code> 文件内查找</li>
</ul>
<p>用到方向键的快捷键：</p>
<ul>
<li><code>Alt + 左右</code> 切换打开的文件</li>
<li><code>Alt + 上下</code> 文件内移动到相邻方法</li>
<li><code>Ctrl + 左右</code> 移动光标到相邻单词</li>
<li><code>Ctrl + 上下</code> 滚屏</li>
<li>（个人习惯）上面是原始的快捷键，我把<code>Alt + 上下</code>跟<code>Ctrl + 上下</code>交换了一下。这样，按 <code>Ctrl</code> 就是在文件内跳转光标位置，按 <code>Alt</code> 就是在切屏（正好我的 <code>Alt + O</code> 也是在切文件），我个人觉得更好记一些</li>
<li><code>Shift + 方向键</code> 选中</li>
</ul>
<p>在Main Menu-&gt;Navigate里可以看一些导航常用的快捷键</p>
<ul>
<li>通常用 <code>Ctrl + Click</code> 速览定义&#x2F;使用。这是可以使用<code>鼠标4/5键</code>，在光标位置之间移动（用于上一个操作的返回）。大拇指往下抠是向前，往上抠是向后。（公司电脑没有侧键，点击左上角两个方向图标）</li>
<li><code>Ctrl + N</code> 查找类</li>
<li><code>Ctrl + Shift + Backspace</code> 到上个编辑点</li>
</ul>
<h4 id="3-调试"><a href="#3-调试" class="headerlink" title="3 调试"></a>3 调试</h4><p>Unreal + VS 的调试方法，参考这个视频：<a href="https://www.bilibili.com/video/BV1rU4y1T73E">如何调试UE4引擎源码</a>。</p>
<p>Rider中的调试方法，参考这篇文章：<a href="https://blog.csdn.net/qq_34599132/article/details/90178393">Intellij IDEA中使用好Debug调试</a>。</p>
<p>这里整理一下UE + Rider的调试方法。事先，要从源码编译过引擎后，在引擎里新建一个项目，这一步就不多说了，只提有了项目之后的调试。</p>
<p>首先在Rider编译项目的Development Editor版本，然后在UE中把项目打一个debug包（只有用源码版引擎的时候才能打debug包）。此时，项目&#x2F;Binaries&#x2F;Win64&#x2F;下就有一个可执行的Debug.exe文件了。</p>
<p>然后，在Rider切换到项目的Debug版本，点Debug，就可以对引擎源码进行调试了。游戏会自动打开。</p>
<p>一个小技巧：Rider点Debug肯定就是打开最后打包好的Debug版本的游戏了（好像是在Binaries目录下面），但在Editor打包可以多搞几个文件夹、把好几个版本的包都保存下来。虽然不能调试引擎源码，但能看一下游戏里的表现，WidgetReflector之类的各种命令行也能用。对于做一些对比工作比较方便。</p>
<h5 id="基本"><a href="#基本" class="headerlink" title="基本"></a>基本</h5><p>这里详细说一下Rider+Unreal调试的过程。</p>
<p>当游戏开始后点击Rider里的暂停，这时可以看到游戏的各个线程</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master\blogimg\2022\52.png" style="zoom:67%;" / loading="lazy"></li>
</ul>
<p>点开一个线程可以看到函数调用栈，比如这里点开主线程，可以看到在做Slate相关的一些事情。游戏时时刻刻在做各种各样的事情，多暂停几次会发现函数调用栈都很不相同</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master\blogimg\2022\54.png" style="zoom:67%;" / loading="lazy"></li>
</ul>
<p>可以对照上面的VS视频，在VS里也是一样，跑Debug模式，然后按全部中断，可以选线程然后看函数调用栈</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master\blogimg\2022\53.png" style="zoom:67%;" / loading="lazy"></li>
</ul>
<h5 id="断点"><a href="#断点" class="headerlink" title="断点"></a>断点</h5><p>回到Rider，接下来展示一下打断点调试的过程。首先，可以在上面暂停游戏后，在代码中打断点，然后继续运行。</p>
<p>也可以事先打断点，比如我想看引擎在初始化阶段都做了什么（相关的过程可以看另一篇文章），可以在PreInit里随便打个断点，点击Debug后程序就会跑到这里停下，下方可以看相应的函数调用栈。</p>
<p>插播一条，如图，debug模式下也可以看到已执行函数的参数值，函数的返回值，等。写在这一行的后面。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master\blogimg\2022\56.png" style="zoom:67%;" / loading="lazy"></li>
</ul>
<p>然后，比较常用的就是框出的那几个按钮。其中我基本上只用中间的3个。</p>
<p>绿色“继续”按钮就是运行到下一个断点执行<strong>之前</strong>。蓝色的三个，第一个是跳到当前方法执行完的地方，第二个是一行行地执行，如果遇到方法就进入方法内部。</p>
<p>比如从这里，我想进入PreInitPreStartupScreen()看看，就点蓝色的第二个。在方法其中，可以打一些断点，那么我就点绿色的继续按钮，程序会执行到下一个断点处。通过类似的方法，在<code>FPreLoadScreenManager::Get()-&gt;Initialize(SlateRendererSharedRef.Get());</code>打了个断点后发现，这一步执行之后，游戏的窗口就被创建出来了。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master\blogimg\2022\55.png" style="zoom:67%;" / loading="lazy"></li>
</ul>
<p>一步步到PreInitPostStartupScreen中的<code>ProcessNewlyLoadedUObjects();</code>执行完后，游戏窗口变黑了。</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ACBGZM/MyPostImage@master\blogimg\2022\57.png" style="zoom:67%;" / loading="lazy"></li>
</ul>
<p>但我不是很关心这个窗口具体发生了什么，这时我可以点击蓝色的第三个按钮Step Out，跳转到PreInitPostStartupScreen执行结束的地方。</p>
<p>然后，如果发现之前有些断点没啥用，就点击倒数第二个叠起来的断点按钮，其中可以看到打过的所有断点，也可以在下方的代码预览框里把这个断点删掉。</p>
<p>最终，在Tick()方法中的<code>FSlateApplication::Get().Tick(ESlateTickType::TimeAndWidgets);</code>执行完后，窗口就画出了游戏画面了。然后就可以进入这个方法，看看到底是如何把游戏世界画出来的。</p>
<p>这就是我用这几个按钮的方法。</p>
<h5 id="条件断点"><a href="#条件断点" class="headerlink" title="条件断点"></a>条件断点</h5><p>在普通断点处点右键，condition里写语句就可以了。拿Slate举例，比如从WidgetReflector里查到一个控件的地址，可以打 <code>this==0x1f19e44a190</code> 让程序停到该控件的Paint()里；或者是 <code>ThisIndex&gt;=3</code> 这种判断某一帧有更多控件加入了循环，类似的语句都可以。</p>
<h5 id="数据断点"><a href="#数据断点" class="headerlink" title="数据断点"></a>数据断点</h5><p>我也不知道怎么在Rider里打数据断点。在<a href="https://www.jetbrains.com/help/rider/Using_breakpoints.html#data-breakpoints">官方文档有个图片</a>，看的应该是先把数据加入watch列表，然后右键可以设置数据断点，但是我右键没有这个选项。</p>
<p>文档还提了一句不支持2020.2的结构体的字段和属性，可能是这个原因吧。总之不是很清楚怎么在rider用数据断点。</p>
]]></content>
      <categories>
        <category>UE源码</category>
      </categories>
      <tags>
        <tag>UnrealEngine</tag>
        <tag>游戏开发</tag>
      </tags>
  </entry>
</search>
